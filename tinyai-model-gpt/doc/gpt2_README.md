# GPT-2 æ¨¡å‹å®ç°

åŸºäºTinyAIæ¡†æ¶å®ç°çš„GPT-2è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨è§£ç å™¨-only Transformeræ¶æ„ï¼Œæ”¯æŒå¤šç§è§„æ¨¡é…ç½®å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/
â”œâ”€â”€ GPT2Config.java              # GPT-2é…ç½®ç±»
â”œâ”€â”€ GPT2Model.java               # GPT-2æ¨¡å‹ç±»ï¼ˆç»§æ‰¿Modelï¼‰
â”œâ”€â”€ GPT2Block.java               # GPT-2æ ¸å¿ƒå—ï¼ˆç»§æ‰¿Blockï¼‰
â”œâ”€â”€ GPT2TransformerBlock.java    # Transformerè§£ç å™¨å—
â”œâ”€â”€ GPT2TokenEmbedding.java      # Tokenå’Œä½ç½®åµŒå…¥å±‚
â”œâ”€â”€ GPT2OutputHead.java          # è¯­è¨€æ¨¡å‹è¾“å‡ºå¤´
â””â”€â”€ test/                        # æµ‹è¯•å¥—ä»¶
```

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### 1. å¤šè§„æ¨¡æ¨¡å‹æ”¯æŒ
- **å°å‹æ¨¡å‹**: 256ç»´, 6å±‚, 8å¤´ (é€‚ç”¨äºæµ‹è¯•å’Œå­¦ä¹ )
- **æ ‡å‡†æ¨¡å‹**: 768ç»´, 12å±‚, 12å¤´ (GPT-2 117Må‚æ•°)
- **ä¸­å‹æ¨¡å‹**: 1024ç»´, 24å±‚, 16å¤´ (GPT-2 345Må‚æ•°)
- **å¤§å‹æ¨¡å‹**: 1280ç»´, 36å±‚, 20å¤´ (GPT-2 762Må‚æ•°)

### 2. æ¶æ„è®¾è®¡
- **è§£ç å™¨-only Transformer**: ä¸“ä¸ºè‡ªå›å½’è¯­è¨€å»ºæ¨¡è®¾è®¡
- **Pre-LayerNormç»“æ„**: å±‚å½’ä¸€åŒ–åœ¨æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œä¹‹å‰
- **å› æœæ©ç **: ä¿è¯ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è‡ªå›å½’ç‰¹æ€§
- **æ®‹å·®è¿æ¥**: æ”¯æŒæ·±å±‚ç½‘ç»œçš„æ¢¯åº¦ä¼ æ’­

### 3. æ ¸å¿ƒç»„ä»¶
- **TokenåµŒå…¥**: å¯å­¦ä¹ çš„è¯æ±‡è¡¨åµŒå…¥å’Œä½ç½®åµŒå…¥
- **å¤šå¤´è‡ªæ³¨æ„åŠ›**: å¸¦å› æœæ©ç çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
- **å‰é¦ˆç½‘ç»œ**: ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°çš„ä¸¤å±‚MLP
- **å±‚å½’ä¸€åŒ–**: ç¨³å®šè®­ç»ƒçš„å½’ä¸€åŒ–æŠ€æœ¯

## ğŸ—ï¸ ç½‘ç»œæ¶æ„å›¾

### æ•´ä½“æ¶æ„
```mermaid
graph TB
    Input["Token IDs<br/>(batch_size, seq_len)"] --> TokenEmbed["TokenåµŒå…¥å±‚<br/>GPT2TokenEmbedding"]
    TokenEmbed --> TransBlock1["Transformerå— 1<br/>GPT2TransformerBlock"]
    TransBlock1 --> TransBlock2["Transformerå— 2<br/>GPT2TransformerBlock"]
    TransBlock2 --> TransBlockN["...<br/>Transformerå— N"]
    TransBlockN --> FinalLN["æœ€ç»ˆå±‚å½’ä¸€åŒ–<br/>LayerNorm"]
    FinalLN --> OutputHead["è¾“å‡ºå¤´<br/>GPT2OutputHead"]
    OutputHead --> Output["Logits<br/>(batch_size, seq_len, vocab_size)"]
    
    style TokenEmbed fill:#e1f5fe
    style TransBlock1 fill:#f3e5f5
    style TransBlock2 fill:#f3e5f5
    style TransBlockN fill:#f3e5f5
    style FinalLN fill:#fff3e0
    style OutputHead fill:#e8f5e8
```

### GPT2TransformerBlockå†…éƒ¨ç»“æ„
```mermaid
graph TD
    BlockInput["è¾“å…¥<br/>(batch_size, seq_len, n_embd)"] --> LN1["LayerNorm 1"]
    LN1 --> MHA["å¤šå¤´è‡ªæ³¨æ„åŠ›<br/>å¸¦å› æœæ©ç "]
    MHA --> Res1["æ®‹å·®è¿æ¥ 1"]
    BlockInput --> Res1
    Res1 --> LN2["LayerNorm 2"]
    LN2 --> FFN["å‰é¦ˆç½‘ç»œ<br/>Linearâ†’GELUâ†’Linear"]
    FFN --> Res2["æ®‹å·®è¿æ¥ 2"]
    Res1 --> Res2
    Res2 --> BlockOutput["è¾“å‡º<br/>(batch_size, seq_len, n_embd)"]
    
    style MHA fill:#e1f5fe
    style FFN fill:#f3e5f5
    style Res1 fill:#fff3e0
    style Res2 fill:#fff3e0
```

### TokenåµŒå…¥å±‚ç»“æ„
```mermaid
graph LR
    TokenIDs["Token IDs"] --> TokenLookup["TokenåµŒå…¥æŸ¥æ‰¾<br/>(vocab_size, n_embd)"]
    Position["ä½ç½®ç´¢å¼•"] --> PosLookup["ä½ç½®åµŒå…¥æŸ¥æ‰¾<br/>(n_positions, n_embd)"]
    TokenLookup --> Add["åµŒå…¥ç›¸åŠ "]
    PosLookup --> Add
    Add --> Dropout["Dropout"]
    Dropout --> EmbedOutput["åµŒå…¥è¾“å‡º<br/>(batch_size, seq_len, n_embd)"]
    
    style TokenLookup fill:#e1f5fe
    style PosLookup fill:#f3e5f5
    style Add fill:#fff3e0
```

### ç±»å›¾å…³ç³»
```mermaid
classDiagram
    class GPT2Model {
        -GPT2Config config
        -GPT2Block gpt2Block
        +GPT2Model(String, GPT2Config)
        +createSmallModel(String) GPT2Model
        +createMediumModel(String) GPT2Model
        +createLargeModel(String) GPT2Model
        +predict(NdArray) Variable
        +predictNextToken(NdArray) int
        +generateSequence(NdArray, int) NdArray
    }
    
    class GPT2Block {
        -GPT2Config config
        -GPT2TokenEmbedding tokenEmbedding
        -List~GPT2TransformerBlock~ transformerBlocks
        -LayerNorm finalLayerNorm
        -GPT2OutputHead outputHead
        +layerForward(Variable...) Variable
        +getParameterCount() long
    }
    
    class GPT2TransformerBlock {
        -LayerNorm layerNorm1
        -MultiHeadAttention attention
        -LayerNorm layerNorm2
        -FeedForward feedForward
        +layerForward(Variable...) Variable
        +addResidualConnection(Variable, Variable) Variable
    }
    
    class GPT2TokenEmbedding {
        -Parameter tokenEmbedding
        -Parameter positionEmbedding
        +layerForward(Variable...) Variable
        +getTokenEmbeddings(NdArray, int, int) Variable
        +getPositionEmbeddings(int, int) Variable
    }
    
    class GPT2OutputHead {
        -Parameter outputWeight
        -Parameter outputBias
        +layerForward(Variable...) Variable
        +predictNextToken(Variable) Variable
    }
    
    GPT2Model --> GPT2Block : "åŒ…å«"
    GPT2Block --> GPT2TokenEmbedding : "ä½¿ç”¨"
    GPT2Block --> GPT2TransformerBlock : "åŒ…å«å¤šä¸ª"
    GPT2Block --> GPT2OutputHead : "ä½¿ç”¨"
    GPT2TransformerBlock --> LayerNorm : "ä½¿ç”¨"
    GPT2TransformerBlock --> MultiHeadAttention : "ä½¿ç”¨"
    GPT2TransformerBlock --> FeedForward : "ä½¿ç”¨"
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```java
// åˆ›å»ºæ ‡å‡†GPT-2æ¨¡å‹
GPT2Model model = new GPT2Model("my-gpt2", new GPT2Config());

// ä½¿ç”¨é¢„è®¾é…ç½®
GPT2Model smallModel = GPT2Model.createSmallModel("gpt2-small");
GPT2Model mediumModel = GPT2Model.createMediumModel("gpt2-medium");
GPT2Model largeModel = GPT2Model.createLargeModel("gpt2-large");

// å‰å‘ä¼ æ’­
NdArray tokenIds = NdArray.of(Shape.of(1, 10)); // è¾“å…¥tokenåºåˆ—
Variable output = model.forward(new Variable(tokenIds));

// é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
int nextToken = model.predictNextToken(tokenIds);

// ç”Ÿæˆæ–‡æœ¬åºåˆ—
NdArray generated = model.generateSequence(tokenIds, 20);
```

### è‡ªå®šä¹‰é…ç½®

```java
// åˆ›å»ºè‡ªå®šä¹‰é…ç½®
GPT2Config config = new GPT2Config(
    30000,  // vocabSize - è¯æ±‡è¡¨å¤§å°
    512,    // nPositions - æœ€å¤§åºåˆ—é•¿åº¦
    512,    // nEmbd - åµŒå…¥ç»´åº¦
    8,      // nLayer - Transformerå±‚æ•°
    8,      // nHead - æ³¨æ„åŠ›å¤´æ•°
    2048,   // nInner - å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦
    "gelu", // activationFunction - æ¿€æ´»å‡½æ•°
    0.1,    // residPdrop - æ®‹å·®dropout
    0.1,    // embdPdrop - åµŒå…¥dropout
    0.1,    // attnPdrop - æ³¨æ„åŠ›dropout
    1e-5,   // layerNormEpsilon - å±‚å½’ä¸€åŒ–epsilon
    0.02    // initializerRange - æƒé‡åˆå§‹åŒ–èŒƒå›´
);

// éªŒè¯é…ç½®
config.validate();

// åˆ›å»ºæ¨¡å‹
GPT2Model model = new GPT2Model("custom-gpt2", config);
```

## ğŸ“Š æ¨¡å‹é…ç½®å¯¹æ¯”

| é…ç½®ç±»å‹ | åµŒå…¥ç»´åº¦ | å±‚æ•° | æ³¨æ„åŠ›å¤´ | å‰é¦ˆç»´åº¦ | å‚æ•°é‡ä¼°ç®— | é€‚ç”¨åœºæ™¯ |
|----------|----------|------|----------|----------|------------|----------|
| å°å‹ | 256 | 6 | 8 | 1024 | ~10M | æµ‹è¯•ã€å­¦ä¹  |
| æ ‡å‡† | 768 | 12 | 12 | 3072 | ~117M | ä¸­ç­‰ä»»åŠ¡ |
| ä¸­å‹ | 1024 | 24 | 16 | 4096 | ~345M | å¤æ‚ä»»åŠ¡ |
| å¤§å‹ | 1280 | 36 | 20 | 5120 | ~762M | é«˜è´¨é‡ç”Ÿæˆ |

## ğŸ§ª è¿è¡Œæ¼”ç¤º

### 1. æ¨¡å‹ä¿¡æ¯å±•ç¤º
```java
GPT2Model model = GPT2Model.createMediumModel("gpt2-medium");
model.printModelInfo();
// è¾“å‡ºï¼š
// === GPT-2 æ¨¡å‹è¯¦ç»†ä¿¡æ¯ ===
// GPT-2æ¨¡å‹é…ç½®æ‘˜è¦:
// - è¯æ±‡è¡¨å¤§å°: 50,257
// - åµŒå…¥ç»´åº¦: 1024
// - Transformerå±‚æ•°: 24
// - æ³¨æ„åŠ›å¤´æ•°: 16
// - æ€»å‚æ•°æ•°é‡: XXX,XXX
```

### 2. æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
```java
// å‡†å¤‡è¾“å…¥
NdArray startTokens = NdArray.of(Shape.of(1, 5));
// å‡è®¾token: [1, 15, 23, 8, 42]
startTokens.set(1, 0, 0);
startTokens.set(15, 0, 1);
startTokens.set(23, 0, 2);
startTokens.set(8, 0, 3);
startTokens.set(42, 0, 4);

// ç”Ÿæˆ20ä¸ªæ–°token
NdArray generated = model.generateSequence(startTokens, 20);
System.out.println("ç”Ÿæˆçš„åºåˆ—é•¿åº¦: " + generated.getShape().getDimension(1));
```

### 3. é€æ­¥é¢„æµ‹
```java
NdArray currentSequence = startTokens;
for (int i = 0; i < 10; i++) {
    int nextToken = model.predictNextToken(currentSequence);
    System.out.println("é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken: " + nextToken);
    
    // å°†æ–°tokenæ·»åŠ åˆ°åºåˆ—ä¸­ç»§ç»­é¢„æµ‹
    currentSequence = appendToken(currentSequence, nextToken);
}
```

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### 1. ç»§æ‰¿ä½“ç³»
- `GPT2Model` extends `Model` (TinyAIæ¡†æ¶æ ¸å¿ƒæ¨¡å‹ç±»)
- `GPT2Block` extends `Block` (TinyAIæ¡†æ¶æ ¸å¿ƒå—ç±»)
- `GPT2TransformerBlock` extends `Block`
- `GPT2TokenEmbedding` extends `Layer`
- `GPT2OutputHead` extends `Layer`

### 2. æ ¸å¿ƒç®—æ³•
- **å› æœè‡ªæ³¨æ„åŠ›**: ä½¿ç”¨ä¸‹ä¸‰è§’æ©ç ç¡®ä¿åªèƒ½çœ‹åˆ°å½“å‰å’Œä¹‹å‰çš„ä½ç½®
- **ä½ç½®ç¼–ç **: å¯å­¦ä¹ çš„ç»å¯¹ä½ç½®åµŒå…¥
- **æ®‹å·®è¿æ¥**: æ¯ä¸ªå­å±‚éƒ½ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
- **æƒé‡åˆå§‹åŒ–**: ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œæ ‡å‡†å·®ä¸º0.02

### 3. å‰å‘ä¼ æ’­æµç¨‹
1. **è¾“å…¥åµŒå…¥**: Token ID â†’ TokenåµŒå…¥ + ä½ç½®åµŒå…¥
2. **Transformerå±‚**: Nä¸ªGPT2TransformerBlockçš„ä¸²è”
3. **æœ€ç»ˆå½’ä¸€åŒ–**: åœ¨è¾“å‡ºå‰åº”ç”¨å±‚å½’ä¸€åŒ–
4. **è¾“å‡ºæ˜ å°„**: çº¿æ€§å˜æ¢åˆ°è¯æ±‡è¡¨ç»´åº¦

## ğŸ“ˆ æ€§èƒ½ç‰¹ç‚¹

### ä¼˜åŠ¿
- âœ… æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•
- âœ… æ”¯æŒå¤šç§æ¨¡å‹è§„æ¨¡é…ç½®
- âœ… å®Œæ•´çš„TinyAIæ¡†æ¶é›†æˆ
- âœ… è§„èŒƒçš„å‚æ•°åˆå§‹åŒ–
- âœ… è¯¦ç»†çš„è¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†

### é€‚ç”¨åœºæ™¯
- ğŸ¯ æ–‡æœ¬ç”Ÿæˆå’Œç»­å†™
- ğŸ¯ è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ
- ğŸ¯ å¯¹è¯ç³»ç»Ÿæ„å»º
- ğŸ¯ ä»£ç ç”Ÿæˆä»»åŠ¡
- ğŸ¯ æ•™å­¦æ¼”ç¤ºå’Œç ”ç©¶

### å†…å­˜å’Œè®¡ç®—è€ƒè™‘
- **å†…å­˜ä½¿ç”¨**: ä¸æ¨¡å‹å¤§å°å’Œåºåˆ—é•¿åº¦æˆæ­£æ¯”
- **è®¡ç®—å¤æ‚åº¦**: O(nÂ²d) å…¶ä¸­nä¸ºåºåˆ—é•¿åº¦ï¼Œdä¸ºæ¨¡å‹ç»´åº¦
- **æ¨ç†é€Ÿåº¦**: æ”¯æŒæ‰¹å¤„ç†ï¼Œå¯å¹¶è¡ŒåŒ–å¤„ç†

## ğŸ” ä»£ç ç¤ºä¾‹

### è·å–æ¨¡å‹ç»„ä»¶
```java
GPT2Model model = GPT2Model.createSmallModel("gpt2");

// è·å–é…ç½®ä¿¡æ¯
GPT2Config config = model.getConfig();
System.out.println("è¯æ±‡è¡¨å¤§å°: " + config.getVocabSize());

// è·å–æ ¸å¿ƒç»„ä»¶
GPT2Block gpt2Block = model.getGPT2Block();
GPT2TokenEmbedding tokenEmbedding = model.getTokenEmbedding();
GPT2TransformerBlock firstTransformerBlock = model.getTransformerBlock(0);
GPT2OutputHead outputHead = model.getOutputHead();
```

### è¾“å…¥éªŒè¯
```java
try {
    NdArray tokenIds = NdArray.of(Shape.of(2, 1025)); // è¶…è¿‡æœ€å¤§é•¿åº¦
    model.validateInput(tokenIds);
} catch (IllegalArgumentException e) {
    System.out.println("è¾“å…¥éªŒè¯å¤±è´¥: " + e.getMessage());
    // è¾“å‡º: åºåˆ—é•¿åº¦(1025)è¶…è¿‡æœ€å¤§æ”¯æŒé•¿åº¦(1024)
}
```

### å‚æ•°ç»Ÿè®¡
```java
GPT2Model model = GPT2Model.createMediumModel("gpt2-medium");
long totalParams = model.getGPT2Block().getParameterCount();
System.out.println("æ¨¡å‹æ€»å‚æ•°æ•°é‡: " + String.format("%,d", totalParams));

// è·å–é…ç½®æ‘˜è¦
String summary = model.getConfigSummary();
System.out.println(summary);
```

## ğŸ“š ç›¸å…³æŠ€æœ¯

### GPT-2ç‰¹è‰²æŠ€æœ¯
- **Byte Pair Encoding (BPE)**: å­è¯æ ‡è®°åŒ–æŠ€æœ¯
- **Pre-LayerNorm**: å±‚å½’ä¸€åŒ–åœ¨æ³¨æ„åŠ›è®¡ç®—ä¹‹å‰
- **GELUæ¿€æ´»å‡½æ•°**: æ¯”ReLUæ›´å¹³æ»‘çš„æ¿€æ´»å‡½æ•°
- **æƒé‡ç»‘å®š**: è¾“å…¥åµŒå…¥å’Œè¾“å‡ºæŠ•å½±å…±äº«æƒé‡

### ä¸å…¶ä»–æ¨¡å‹çš„å¯¹æ¯”
- **vs GPT-1**: å¢åŠ äº†å±‚æ•°å’Œå‚æ•°é‡ï¼Œä½¿ç”¨äº†æ›´å¤§çš„æ•°æ®é›†
- **vs GPT-3**: GPT-2æ˜¯GPT-3çš„å‰èº«ï¼Œæ¶æ„ç›¸ä¼¼ä½†è§„æ¨¡æ›´å°
- **vs BERT**: GPT-2æ˜¯è§£ç å™¨-onlyï¼ŒBERTæ˜¯ç¼–ç å™¨-only

## ğŸ“ å­¦ä¹ èµ„æº

### æ ¸å¿ƒè®ºæ–‡
- "Language Models are Unsupervised Multitask Learners" (GPT-2)
- "Attention Is All You Need" (Transformer)
- "Improving Language Understanding by Generative Pre-Training" (GPT-1)

### æŠ€æœ¯åšå®¢
- GPT-2æ¶æ„æ·±åº¦è§£æ
- Transformeræ³¨æ„åŠ›æœºåˆ¶è¯¦è§£
- è‡ªå›å½’è¯­è¨€æ¨¡å‹åŸç†

## ğŸ¤ æ‰©å±•å»ºè®®

### å¯èƒ½çš„æ”¹è¿›æ–¹å‘
- [ ] æ·»åŠ Rotary Position Embedding (RoPE)æ”¯æŒ
- [ ] å®ç°KVç¼“å­˜ä¼˜åŒ–æ¨ç†é€Ÿåº¦
- [ ] æ”¯æŒæ¨¡å‹å¹¶è¡Œå’Œæ¢¯åº¦ç´¯ç§¯
- [ ] æ·»åŠ æ›´å¤šæ¿€æ´»å‡½æ•°é€‰é¡¹
- [ ] å®ç°æ¸è¿›å¼åºåˆ—é•¿åº¦è®­ç»ƒ

### é«˜çº§ç‰¹æ€§
- [ ] Top-kå’ŒTop-pé‡‡æ ·ç­–ç•¥
- [ ] æ¸©åº¦æ§åˆ¶çš„æ¦‚ç‡é‡‡æ ·
- [ ] æŸæœç´¢ (Beam Search) è§£ç 
- [ ] é‡å¤æƒ©ç½šæœºåˆ¶

---

*åŸºäºTinyAIæ¡†æ¶å®ç°ï¼Œéµå¾ªæ ‡å‡†GPT-2æ¶æ„è®¾è®¡ï¼Œæä¾›æ¸…æ™°çš„æ¨¡å—åŒ–å®ç°å’Œå®Œæ•´çš„åŠŸèƒ½æ”¯æŒã€‚*