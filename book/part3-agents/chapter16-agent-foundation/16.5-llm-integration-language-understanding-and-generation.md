# 16.5 LLM集成：语言理解与生成

> **设计思想**：构建灵活的LLM集成框架，实现智能体与大语言模型的无缝对接

## 本节概述

大语言模型（LLM）是现代智能体系统的核心引擎，它提供了强大的语言理解和生成能力。本节将设计并实现一个灵活的LLM集成框架，支持多种大语言模型的接入，提供统一的接口和高效的调用机制，同时确保系统的可扩展性和可维护性。

## 学习目标

完成本节学习后，你将：

- ✅ **理解LLM集成的核心概念**：掌握LLM接口设计和封装原则
- ✅ **掌握提示工程和上下文管理**：学会构建有效的提示和管理对话上下文
- ✅ **实现响应解析和后处理**：掌握LLM输出的解析和格式化技术
- ✅ **掌握性能优化和缓存机制**：学会提升LLM调用效率的方法
- ✅ **具备多模型支持能力**：能够集成不同厂商和类型的LLM
- ✅ **掌握错误处理和重试机制**：实现健壮的LLM调用处理

## LLM集成的核心概念

### LLM集成架构与设计原则

大语言模型集成是智能体系统的**大脑**,负责语言理解、推理决策和内容生成。我们采用**适配器模式**实现多模型统一接入,通过**分层架构**解耦业务逻辑与模型实现。

```mermaid
graph TB
    subgraph "应用层"
        A[智能体核心]
        B[对话管理]
        C[任务规划]
    end
    
    subgraph "抽象层-统一接口"
        D[LanguageModel<br/>抽象基类]
        E[提示管理器<br/>PromptManager]
        F[响应解析器<br/>ResponseParser]
        G[缓存层<br/>LRU Cache]
    end
    
    subgraph "实现层-具体适配器"
        H[OpenAIAdapter<br/>GPT-4/3.5]
        I[ClaudeAdapter<br/>Anthropic]
        J[本地模型Adapter<br/>Ollama/vLLM]
    end
    
    A --> D
    B --> E
    C --> E
    D --> F
    D --> G
    
    D --> H
    D --> I
    D --> J
    
    H --> K[(OpenAI API)]
    I --> L[(Anthropic API)]
    J --> M[(本地服务)]
```

### 关键设计决策

| 设计点 | 方案选择 | 理由 |
|--------|----------|------|
| **接口抽象** | 抽象基类+多态 | 支持多模型切换,易于扩展 |
| **缓存策略** | LRU Cache | 减少API调用,降低成本 |
| **错误处理** | 指数退避重试 | 应对网络抖动和限流 |
| **流式输出** | Server-Sent Events | 提升用户体验,实时反馈 |
| **上下文管理** | 滑动窗口 | 控制token消耗 |
| **工具调用** | Function Calling | 赋予模型执行能力 |

## LLM接口设计和封装

### 统一接口设计原则

LLM接口设计遵循**适配器模式**,通过抽象基类定义统一的调用接口,不同厂商的模型实现各自的适配器。这种设计带来**可插拔性**、**可测试性**和**可扩展性**。

**核心接口方法**:

```mermaid
graph LR
    A[LanguageModel<br/>抽象基类] --> B[generate<br/>单轮生成]
    A --> C[generateWithHistory<br/>多轮对话]
    A --> D[generateWithTools<br/>工具调用]
    A --> E[generateStream<br/>流式输出]
    A --> F[generateEmbedding<br/>向量生成]
    
    B --> G[LLMResponse]
    C --> G
    D --> G
    E --> H[Stream<Chunk>]
    F --> I[float向量]
```

### 抽象基类实现

```java
public abstract class LanguageModel {
    protected String modelId;              // 模型标识
    protected LLMConfig config;            // 配置参数
    protected Cache<String, LLMResponse> cache; // 响应缓存
    
    // 核心生成方法(子类必须实现)
    public abstract LLMResponse generate(String prompt) throws LLMException;
    public abstract LLMResponse generate(ChatHistory history) throws LLMException;
    public abstract LLMResponse generateWithTools(
        String prompt, List<Tool> tools) throws LLMException;
    
    // 流式生成(可选实现)
    public Stream<LLMResponseChunk> generateStream(String prompt) throws LLMException {
        throw new UnsupportedOperationException("Streaming not supported");
    }
    
    // 嵌入向量生成(可选实现)
    public float[] generateEmbedding(String text) throws LLMException {
        throw new UnsupportedOperationException("Embedding not supported");
    }
    
    // 缓存管理
    protected LLMResponse getCachedResponse(String cacheKey) {
        return config.isCachingEnabled() ? cache.get(cacheKey) : null;
    }
    
    protected void cacheResponse(String cacheKey, LLMResponse response) {
        if (config.isCachingEnabled()) {
            cache.put(cacheKey, response);
        }
    }
    
    /* ... 其他辅助方法 ... */
}

// 配置类:管理模型参数
public class LLMConfig {
    private double temperature = 0.7;      // 温度(0-1):控制随机性
    private int maxTokens = 2048;          // 最大输出长度
    private double topP = 1.0;             // 核采样参数
    private int cacheSize = 1000;          // 缓存大小
    private boolean cachingEnabled = true; // 是否启用缓存
    private long timeoutMs = 30000;        // 超时时间(毫秒)
    private int maxRetries = 3;            // 最大重试次数
    
    /* ... Getter/Setter方法 ... */
}

// 响应类:统一的返回格式
public class LLMResponse {
    private String content;                // 生成的文本内容
    private List<ToolCall> toolCalls;      // 工具调用列表
    private String finishReason;           // 结束原因(stop/length/tool_calls)
    private int inputTokens;               // 输入token数
    private int outputTokens;              // 输出token数
    private long generationTime;           // 生成耗时(毫秒)
    
    public boolean hasToolCalls() {
        return toolCalls != null && !toolCalls.isEmpty();
    }
    
    /* ... Getter/Setter方法 ... */
}
```

### OpenAI GPT集成示例

OpenAI适配器展示了如何实现具体的LLM接入,包括**重试机制**、**缓存策略**和**错误处理**。

```java
public class OpenAILanguageModel extends LanguageModel {
    private final OpenAIClient client;
    private final String modelName;  // e.g., "gpt-4", "gpt-3.5-turbo"
    
    public OpenAILanguageModel(String modelName, LLMConfig config, String apiKey) {
        super("openai-" + modelName, config);
        this.modelName = modelName;
        this.client = OpenAIClient.builder()
            .apiKey(apiKey)
            .timeout(Duration.ofMillis(config.getTimeoutMs()))
            .build();
    }
    
    @Override
    public LLMResponse generate(String prompt) throws LLMException {
        // 1. 检查缓存
        String cacheKey = buildCacheKey(prompt);
        LLMResponse cached = getCachedResponse(cacheKey);
        if (cached != null) return cached;
        
        try {
            long startTime = System.currentTimeMillis();
            
            // 2. 构建API请求
            CreateChatCompletionRequest request = CreateChatCompletionRequest.builder()
                .model(modelName)
                .messages(Arrays.asList(ChatMessage.of("user", prompt)))
                .temperature(config.getTemperature())
                .maxTokens(config.getMaxTokens())
                .build();
            
            // 3. 调用API(带重试)
            CreateChatCompletionResponse response = withRetry(() -> 
                client.createChatCompletion(request));
            
            // 4. 解析响应
            LLMResponse llmResponse = parseResponse(response);
            llmResponse.setGenerationTime(System.currentTimeMillis() - startTime);
            
            // 5. 缓存结果
            cacheResponse(cacheKey, llmResponse);
            
            return llmResponse;
        } catch (Exception e) {
            throw new LLMException("OpenAI API call failed: " + e.getMessage(), e);
        }
    }
    
    @Override
    public LLMResponse generateWithTools(String prompt, List<Tool> tools) 
            throws LLMException {
        // 转换工具为OpenAI Function格式
        List<ChatFunction> functions = tools.stream()
            .map(this::convertToolToFunction)
            .collect(Collectors.toList());
        
        CreateChatCompletionRequest request = CreateChatCompletionRequest.builder()
            .model(modelName)
            .messages(Arrays.asList(ChatMessage.of("user", prompt)))
            .functions(functions)
            .functionCall("auto")  // 自动决策是否调用工具
            .build();
        
        CreateChatCompletionResponse response = withRetry(() -> 
            client.createChatCompletion(request));
        
        return parseResponseWithTools(response);
    }
    
    // 指数退避重试机制
    private <T> T withRetry(Supplier<T> supplier) throws Exception {
        int maxRetries = config.getMaxRetries();
        Exception lastException = null;
        
        for (int i = 0; i < maxRetries; i++) {
            try {
                return supplier.get();
            } catch (Exception e) {
                lastException = e;
                if (i < maxRetries - 1) {
                    // 指数退避: 2^i * 1000ms
                    Thread.sleep((long) Math.pow(2, i) * 1000);
                }
            }
        }
        throw lastException;
    }
    
    /* ... 其他辅助方法 ... */
}
            llmResponse.setGenerationTime(endTime - startTime);
            
            // 缓存响应
            cacheResponse(cacheKey, llmResponse);
            logger.logGeneration(modelId, history.toString(), llmResponse);
            
            return llmResponse;
        } catch (Exception e) {
            logger.logError(modelId, history.toString(), e);
            throw new LLMException("Failed to generate response from OpenAI", e);
        }
    }
    
    @Override
    public LLMResponse generateWithTools(String prompt, List<Tool> tools) throws LLMException {
        if (!supportsTools()) {
            return generate(prompt);
        }
        
        try {
            long startTime = System.currentTimeMillis();
            
            List<ChatMessage> messages = Arrays.asList(ChatMessage.of("user", prompt));
            List<ChatCompletionTool> chatTools = convertToolsToChatTools(tools);
            
            CreateChatCompletionRequest request = CreateChatCompletionRequest.builder()
                .model(modelName)
                .messages(messages)
                .tools(chatTools)
                .temperature(config.getTemperature())
                .maxTokens(config.getMaxTokens())
                .topP(config.getTopP())
                .build();
            
            CreateChatCompletionResponse response = withRetry(() -> 
                client.createChatCompletion(request));
            
            long endTime = System.currentTimeMillis();
            
            LLMResponse llmResponse = buildResponseWithTools(response);
            llmResponse.setGenerationTime(endTime - startTime);
            
            logger.logGenerationWithTools(modelId, prompt, tools, llmResponse);
            
            return llmResponse;
        } catch (Exception e) {
            logger.logError(modelId, prompt, e);
            throw new LLMException("Failed to generate response with tools from OpenAI", e);
        }
    }
    
    @Override
    public Stream<LLMResponseChunk> generateStream(String prompt) throws LLMException {
        if (!config.isStreamingEnabled()) {
            throw new LLMException("Streaming is not enabled for this model");
        }
        
        try {
            CreateChatCompletionRequest request = CreateChatCompletionRequest.builder()
                .model(modelName)
                .messages(Arrays.asList(ChatMessage.of("user", prompt)))
                .temperature(config.getTemperature())
                .maxTokens(config.getMaxTokens())
                .topP(config.getTopP())
                .stream(true)
                .build();
            
            Stream<CreateChatCompletionResponse> responseStream = client.streamChatCompletion(request);
            
            return responseStream.map(this::buildResponseChunk);
        } catch (Exception e) {
            logger.logError(modelId, prompt, e);
            throw new LLMException("Failed to generate streaming response from OpenAI", e);
        }
    }
    
    @Override
    public float[] generateEmbedding(String text) throws LLMException {
        try {
            CreateEmbeddingRequest request = CreateEmbeddingRequest.builder()
                .model("text-embedding-ada-002")
                .input(text)
                .build();
            
            CreateEmbeddingResponse response = withRetry(() -> 
                client.createEmbedding(request));
            
            if (response.getData() != null && !response.getData().isEmpty()) {
                return response.getData().get(0).getEmbedding().stream()
                    .mapToDouble(Double::doubleValue)
                    .toArray();
            }
            
            throw new LLMException("No embedding data returned");
        } catch (Exception e) {
            logger.logError(modelId, "Embedding generation failed for: " + text, e);
            throw new LLMException("Failed to generate embedding from OpenAI", e);
        }
    }
    
    private <T> T withRetry(Supplier<T> operation) throws Exception {
        Exception lastException = null;
        
        for (int attempt = 0; attempt <= config.getMaxRetries(); attempt++) {
            try {
                return operation.get();
            } catch (Exception e) {
                lastException = e;
                if (attempt < config.getMaxRetries()) {
                    long delay = (long) Math.pow(2, attempt) * 1000; // 指数退避
                    Thread.sleep(delay);
                    logger.logRetry(modelId, attempt + 1, e);
                }
            }
        }
        
        throw lastException;
    }
    
    private LLMResponse buildResponse(CreateChatCompletionResponse response) {
        if (response.getChoices() != null && !response.getChoices().isEmpty()) {
            ChatCompletionChoice choice = response.getChoices().get(0);
            ChatMessage message = choice.getMessage();
            
            LLMResponse llmResponse = new LLMResponse(message.getContent());
            llmResponse.setFinishReason(choice.getFinishReason());
            llmResponse.setInputTokens(response.getUsage().getPromptTokens());
            llmResponse.setOutputTokens(response.getUsage().getCompletionTokens());
            
            return llmResponse;
        }
        
        return new LLMResponse("");
    }
    
    private LLMResponse buildResponseWithTools(CreateChatCompletionResponse response) {
        if (response.getChoices() != null && !response.getChoices().isEmpty()) {
            ChatCompletionChoice choice = response.getChoices().get(0);
            ChatMessage message = choice.getMessage();
            
            List<ToolCall> toolCalls = new ArrayList<>();
            if (message.getToolCalls() != null) {
                for (ChatCompletionMessageToolCall toolCall : message.getToolCalls()) {
                    ToolCall call = new ToolCall(
                        toolCall.getFunction().getName(),
                        parseJson(toolCall.getFunction().getArguments())
                    );
                    call.setId(toolCall.getId());
                    toolCalls.add(call);
                }
            }
            
            LLMResponse llmResponse = LLMResponse.withToolCalls(
                message.getContent(), toolCalls);
            llmResponse.setFinishReason(choice.getFinishReason());
            llmResponse.setInputTokens(response.getUsage().getPromptTokens());
            llmResponse.setOutputTokens(response.getUsage().getCompletionTokens());
            
            return llmResponse;
        }
        
        return new LLMResponse("");
    }
    
    private LLMResponseChunk buildResponseChunk(CreateChatCompletionResponse response) {
        if (response.getChoices() != null && !response.getChoices().isEmpty()) {
            ChatCompletionChoice choice = response.getChoices().get(0);
            ChatMessage delta = choice.getDelta();
            
            boolean isFinal = choice.getFinishReason() != null;
            LLMResponseChunk chunk = new LLMResponseChunk(delta.getContent(), isFinal);
            chunk.setFinishReason(choice.getFinishReason());
            
            return chunk;
        }
        
        return new LLMResponseChunk("", true);
    }
    
    private String buildCacheKey(String prompt, ChatHistory history) {
        StringBuilder keyBuilder = new StringBuilder(modelId);
        if (prompt != null) {
            keyBuilder.append(":prompt:").append(prompt.hashCode());
        }
        if (history != null) {
            keyBuilder.append(":history:").append(history.hashCode());
        }
        keyBuilder.append(":config:").append(config.hashCode());
        return keyBuilder.toString();
    }
    
    private List<ChatMessage> convertHistoryToMessages(ChatHistory history) {
        List<ChatMessage> messages = new ArrayList<>();
        for (ChatMessageEntry entry : history.getMessages()) {
            messages.add(ChatMessage.of(entry.getRole(), entry.getContent()));
        }
        return messages;
    }
    
    private List<ChatCompletionTool> convertToolsToChatTools(List<Tool> tools) {
        List<ChatCompletionTool> chatTools = new ArrayList<>();
        for (Tool tool : tools) {
            ChatCompletionTool chatTool = ChatCompletionTool.builder()
                .type(ChatCompletionTool.Type.FUNCTION)
                .function(buildFunctionDefinition(tool))
                .build();
            chatTools.add(chatTool);
        }
        return chatTools;
    }
    
    private ChatCompletionFunction buildFunctionDefinition(Tool tool) {
        Map<String, Object> properties = new HashMap<>();
        Map<String, Object> required = new HashMap<>();
        
        for (ToolParameter param : tool.getParameters()) {
            Map<String, Object> paramDef = new HashMap<>();
            paramDef.put("type", param.getType().toString().toLowerCase());
            paramDef.put("description", param.getDescription());
            
            if (param.getAllowedValues() != null && !param.getAllowedValues().isEmpty()) {
                paramDef.put("enum", param.getAllowedValues());
            }
            
            properties.put(param.getName(), paramDef);
            
            if (param.isRequired()) {
                ((List<String>) required.computeIfAbsent("required", k -> new ArrayList<>()))
                    .add(param.getName());
            }
        }
        
        Map<String, Object> parameters = new HashMap<>();
        parameters.put("type", "object");
        parameters.put("properties", properties);
        parameters.put("required", required.get("required"));
        
        return ChatCompletionFunction.builder()
            .name(tool.getName())
            .description(tool.getDescription())
            .parameters(parameters)
            .build();
    }
    
    private Map<String, Object> parseJson(String json) {
        try {
            ObjectMapper mapper = new ObjectMapper();
            return mapper.readValue(json, new TypeReference<Map<String, Object>>() {});
        } catch (Exception e) {
            logger.warn("Failed to parse JSON arguments: " + json, e);
            return new HashMap<>();
        }
    }
}
```

## 提示工程和上下文管理

### 提示模板管理

```java
public class PromptTemplateManager {
    private final Map<String, PromptTemplate> templates;
    private final ReadWriteLock lock;
    
    public PromptTemplateManager() {
        this.templates = new ConcurrentHashMap<>();
        this.lock = new ReentrantReadWriteLock();
        loadDefaultTemplates();
    }
    
    private void loadDefaultTemplates() {
        // 系统提示模板
        addTemplate("system.default", new PromptTemplate(
            "You are a helpful AI assistant. Provide clear and accurate responses."
        ));
        
        // 问答提示模板
        addTemplate("qa.default", new PromptTemplate(
            "Answer the following question:\n{question}\n\nContext:\n{context}"
        ));
        
        // 总结提示模板
        addTemplate("summarize.default", new PromptTemplate(
            "Summarize the following text in {length} words:\n{text}"
        ));
        
        // 翻译提示模板
        addTemplate("translate.default", new PromptTemplate(
            "Translate the following text from {source_lang} to {target_lang}:\n{text}"
        ));
    }
    
    public void addTemplate(String name, PromptTemplate template) {
        lock.writeLock().lock();
        try {
            templates.put(name, template);
        } finally {
            lock.writeLock().unlock();
        }
    }
    
    public String renderTemplate(String templateName, Map<String, Object> variables) {
        lock.readLock().lock();
        try {
            PromptTemplate template = templates.get(templateName);
            if (template == null) {
                throw new IllegalArgumentException("Template not found: " + templateName);
            }
            return template.render(variables);
        } finally {
            lock.readLock().unlock();
        }
    }
    
    public PromptTemplate getTemplate(String name) {
        lock.readLock().lock();
        try {
            return templates.get(name);
        } finally {
            lock.readLock().unlock();
        }
    }
}

public class PromptTemplate {
    private final String template;
    private final List<String> variables;
    
    public PromptTemplate(String template) {
        this.template = template;
        this.variables = extractVariables(template);
    }
    
    public String render(Map<String, Object> variables) {
        String result = template;
        for (String variable : this.variables) {
            Object value = variables.get(variable);
            if (value != null) {
                result = result.replace("{" + variable + "}", value.toString());
            }
        }
        return result;
    }
    
    private List<String> extractVariables(String template) {
        List<String> variables = new ArrayList<>();
        Pattern pattern = Pattern.compile("\\{([^}]+)\\}");
        Matcher matcher = pattern.matcher(template);
        
        while (matcher.find()) {
            variables.add(matcher.group(1));
        }
        
        return variables;
    }
    
    public String getTemplate() { return template; }
    public List<String> getVariables() { return variables; }
}
```

### 对话历史管理

```java
public class ChatHistory {
    private final List<ChatMessageEntry> messages;
    private final int maxTokens;
    private final Tokenizer tokenizer;
    private final ReadWriteLock lock;
    
    public ChatHistory(int maxTokens, Tokenizer tokenizer) {
        this.messages = new ArrayList<>();
        this.maxTokens = maxTokens;
        this.tokenizer = tokenizer;
        this.lock = new ReentrantReadWriteLock();
    }
    
    public void addMessage(String role, String content) {
        lock.writeLock().lock();
        try {
            messages.add(new ChatMessageEntry(role, content, System.currentTimeMillis()));
            trimHistoryIfNeeded();
        } finally {
            lock.writeLock().unlock();
        }
    }
    
    public void addUserMessage(String content) {
        addMessage("user", content);
    }
    
    public void addAssistantMessage(String content) {
        addMessage("assistant", content);
    }
    
    public void addSystemMessage(String content) {
        addMessage("system", content);
    }
    
    public List<ChatMessageEntry> getMessages() {
        lock.readLock().lock();
        try {
            return new ArrayList<>(messages);
        } finally {
            lock.readLock().unlock();
        }
    }
    
    public String getFormattedHistory() {
        lock.readLock().lock();
        try {
            StringBuilder sb = new StringBuilder();
            for (ChatMessageEntry entry : messages) {
                sb.append(entry.getRole()).append(": ").append(entry.getContent()).append("\n");
            }
            return sb.toString();
        } finally {
            lock.readLock().unlock();
        }
    }
    
    private void trimHistoryIfNeeded() {
        int totalTokens = calculateTotalTokens();
        while (totalTokens > maxTokens && messages.size() > 1) {
            // 移除最早的消息（保留系统消息）
            ChatMessageEntry removed = messages.remove(1); // 0是系统消息，1是最早用户消息
            totalTokens -= tokenizer.countTokens(removed.getContent());
        }
    }
    
    private int calculateTotalTokens() {
        return messages.stream()
            .mapToInt(entry -> tokenizer.countTokens(entry.getContent()))
            .sum();
    }
    
    public void clear() {
        lock.writeLock().lock();
        try {
            messages.clear();
        } finally {
            lock.writeLock().unlock();
        }
    }
    
    public int size() {
        lock.readLock().lock();
        try {
            return messages.size();
        } finally {
            lock.readLock().unlock();
        }
    }
    
    @Override
    public String toString() {
        return getFormattedHistory();
    }
}

public class ChatMessageEntry {
    private final String role;
    private final String content;
    private final long timestamp;
    
    public ChatMessageEntry(String role, String content, long timestamp) {
        this.role = role;
        this.content = content;
        this.timestamp = timestamp;
    }
    
    // Getters
    public String getRole() { return role; }
    public String getContent() { return content; }
    public long getTimestamp() { return timestamp; }
}

public interface Tokenizer {
    int countTokens(String text);
    List<String> tokenize(String text);
    String detokenize(List<String> tokens);
}

public class SimpleTokenizer implements Tokenizer {
    @Override
    public int countTokens(String text) {
        if (text == null || text.isEmpty()) {
            return 0;
        }
        // 简单的基于空格的分词
        return text.trim().split("\\s+").length;
    }
    
    @Override
    public List<String> tokenize(String text) {
        if (text == null || text.isEmpty()) {
            return new ArrayList<>();
        }
        return Arrays.asList(text.trim().split("\\s+"));
    }
    
    @Override
    public String detokenize(List<String> tokens) {
        return String.join(" ", tokens);
    }
}
```

## 响应解析和后处理

### 响应解析器

```java
public class ResponseParser {
    private final ObjectMapper objectMapper;
    
    public ResponseParser() {
        this.objectMapper = new ObjectMapper();
        this.objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
    }
    
    public <T> T parseJsonResponse(String response, Class<T> targetType) throws LLMException {
        try {
            // 清理响应中的Markdown代码块标记
            String cleanedResponse = cleanJsonResponse(response);
            return objectMapper.readValue(cleanedResponse, targetType);
        } catch (Exception e) {
            throw new LLMException("Failed to parse JSON response", e);
        }
    }
    
    public <T> T parseJsonResponse(String response, TypeReference<T> typeReference) throws LLMException {
        try {
            String cleanedResponse = cleanJsonResponse(response);
            return objectMapper.readValue(cleanedResponse, typeReference);
        } catch (Exception e) {
            throw new LLMException("Failed to parse JSON response", e);
        }
    }
    
    private String cleanJsonResponse(String response) {
        if (response == null) {
            return null;
        }
        
        // 移除Markdown代码块标记
        String cleaned = response.trim();
        if (cleaned.startsWith("```json")) {
            cleaned = cleaned.substring(7);
        } else if (cleaned.startsWith("```")) {
            cleaned = cleaned.substring(3);
        }
        
        if (cleaned.endsWith("```")) {
            cleaned = cleaned.substring(0, cleaned.length() - 3);
        }
        
        return cleaned.trim();
    }
    
    public List<String> extractCodeBlocks(String response) {
        List<String> codeBlocks = new ArrayList<>();
        Pattern pattern = Pattern.compile("```(?:\\w+)?\\s*([\\s\\S]*?)```");
        Matcher matcher = pattern.matcher(response);
        
        while (matcher.find()) {
            codeBlocks.add(matcher.group(1).trim());
        }
        
        return codeBlocks;
    }
    
    public String extractFirstCodeBlock(String response) {
        List<String> codeBlocks = extractCodeBlocks(response);
        return codeBlocks.isEmpty() ? null : codeBlocks.get(0);
    }
    
    public Map<String, Object> parseKeyValuePairs(String response) {
        Map<String, Object> pairs = new HashMap<>();
        Pattern pattern = Pattern.compile("(\\w+):\\s*(.+)");
        Matcher matcher = pattern.matcher(response);
        
        while (matcher.find()) {
            String key = matcher.group(1).trim();
            String value = matcher.group(2).trim();
            pairs.put(key, value);
        }
        
        return pairs;
    }
}

public class ToolCallParser {
    private final ResponseParser responseParser;
    
    public ToolCallParser() {
        this.responseParser = new ResponseParser();
    }
    
    public List<ToolCall> parseToolCalls(String response) {
        List<ToolCall> toolCalls = new ArrayList<>();
        
        try {
            // 尝试解析JSON格式的工具调用
            if (response.contains("\"tool_calls\"") || response.contains("\"function\"")) {
                ToolCallResponse toolCallResponse = responseParser.parseJsonResponse(
                    response, ToolCallResponse.class);
                if (toolCallResponse.getToolCalls() != null) {
                    toolCalls.addAll(toolCallResponse.getToolCalls());
                }
            }
        } catch (Exception e) {
            // 如果JSON解析失败，尝试其他格式
            toolCalls.addAll(parseToolCallsFromText(response));
        }
        
        return toolCalls;
    }
    
    private List<ToolCall> parseToolCallsFromText(String response) {
        List<ToolCall> toolCalls = new ArrayList<>();
        
        // 解析类似 "Action: calculator\nAction Input: {\"expression\": \"2 + 3\"}" 的格式
        Pattern pattern = Pattern.compile("Action:\\s*(\\w+)\\s*Action Input:\\s*(\\{[^}]+\\})");
        Matcher matcher = pattern.matcher(response);
        
        while (matcher.find()) {
            String toolName = matcher.group(1).trim();
            String argumentsJson = matcher.group(2).trim();
            
            try {
                Map<String, Object> arguments = responseParser.parseJsonResponse(
                    argumentsJson, new TypeReference<Map<String, Object>>() {});
                toolCalls.add(new ToolCall(toolName, arguments));
            } catch (Exception e) {
                // 忽略解析失败的工具调用
            }
        }
        
        return toolCalls;
    }
}

class ToolCallResponse {
    private List<ToolCall> toolCalls;
    
    // Getters and setters
    public List<ToolCall> getToolCalls() { return toolCalls; }
    public void setToolCalls(List<ToolCall> toolCalls) { this.toolCalls = toolCalls; }
}
```

## 性能优化和缓存机制

### 智能缓存系统

```java
public class IntelligentCache {
    private final Map<String, CacheEntry> cache;
    private final int maxSize;
    private final long ttlMs;
    private final ReadWriteLock lock;
    private final CacheEvictionPolicy evictionPolicy;
    
    public IntelligentCache(int maxSize, long ttlMs) {
        this.maxSize = maxSize;
        this.ttlMs = ttlMs;
        this.cache = new LinkedHashMap<String, CacheEntry>(16, 0.75f, true) {
            @Override
            protected boolean removeEldestEntry(Map.Entry<String, CacheEntry> eldest) {
                return size() > maxSize;
            }
        };
        this.lock = new ReentrantReadWriteLock();
        this.evictionPolicy = new LRUEvictionPolicy();
    }
    
    public void put(String key, Object value) {
        lock.writeLock().lock();
        try {
            cache.put(key, new CacheEntry(value, System.currentTimeMillis()));
        } finally {
            lock.writeLock().unlock();
        }
    }
    
    public <T> T get(String key, Class<T> type) {
        lock.readLock().lock();
        try {
            CacheEntry entry = cache.get(key);
            if (entry != null) {
                if (System.currentTimeMillis() - entry.getTimestamp() < ttlMs) {
                    Object value = entry.getValue();
                    if (type.isInstance(value)) {
                        return type.cast(value);
                    }
                } else {
                    // 过期，异步清理
                    CompletableFuture.runAsync(() -> remove(key));
                }
            }
            return null;
        } finally {
            lock.readLock().unlock();
        }
    }
    
    public boolean contains(String key) {
        lock.readLock().lock();
        try {
            CacheEntry entry = cache.get(key);
            return entry != null && (System.currentTimeMillis() - entry.getTimestamp() < ttlMs);
        } finally {
            lock.readLock().unlock();
        }
    }
    
    public void remove(String key) {
        lock.writeLock().lock();
        try {
            cache.remove(key);
        } finally {
            lock.writeLock().unlock();
        }
    }
    
    public void clear() {
        lock.writeLock().lock();
        try {
            cache.clear();
        } finally {
            lock.writeLock().unlock();
        }
    }
    
    public int size() {
        lock.readLock().lock();
        try {
            return cache.size();
        } finally {
            lock.readLock().unlock();
        }
    }
    
    public CacheStats getStats() {
        lock.readLock().lock();
        try {
            long hits = cache.values().stream()
                .filter(entry -> System.currentTimeMillis() - entry.getTimestamp() < ttlMs)
                .count();
            return new CacheStats(cache.size(), hits);
        } finally {
            lock.readLock().unlock();
        }
    }
}

class CacheEntry {
    private final Object value;
    private final long timestamp;
    
    public CacheEntry(Object value, long timestamp) {
        this.value = value;
        this.timestamp = timestamp;
    }
    
    public Object getValue() { return value; }
    public long getTimestamp() { return timestamp; }
}

public class CacheStats {
    private final int size;
    private final long hits;
    
    public CacheStats(int size, long hits) {
        this.size = size;
        this.hits = hits;
    }
    
    public int getSize() { return size; }
    public long getHits() { return hits; }
}

interface CacheEvictionPolicy {
    <T> T selectEntryForEviction(Map<String, T> cache);
}

class LRUEvictionPolicy implements CacheEvictionPolicy {
    @Override
    public <T> T selectEntryForEviction(Map<String, T> cache) {
        // LRU策略在LinkedHashMap中已实现
        return null;
    }
}
```

### 批处理优化

```java
public class BatchProcessor {
    private final LanguageModel languageModel;
    private final int batchSize;
    private final long batchTimeoutMs;
    private final ExecutorService executorService;
    private final Queue<BatchRequest> requestQueue;
    private final ScheduledExecutorService scheduler;
    
    public BatchProcessor(LanguageModel languageModel, int batchSize, long batchTimeoutMs) {
        this.languageModel = languageModel;
        this.batchSize = batchSize;
        this.batchTimeoutMs = batchTimeoutMs;
        this.executorService = Executors.newFixedThreadPool(4);
        this.requestQueue = new ConcurrentLinkedQueue<>();
        this.scheduler = Executors.newScheduledThreadPool(1);
        
        // 启动批处理调度器
        startBatchScheduler();
    }
    
    public CompletableFuture<LLMResponse> submitRequest(String prompt) {
        CompletableFuture<LLMResponse> future = new CompletableFuture<>();
        BatchRequest request = new BatchRequest(prompt, future);
        requestQueue.offer(request);
        return future;
    }
    
    private void startBatchScheduler() {
        scheduler.scheduleAtFixedRate(this::processBatch, 0, batchTimeoutMs, TimeUnit.MILLISECONDS);
    }
    
    private void processBatch() {
        if (requestQueue.isEmpty()) {
            return;
        }
        
        List<BatchRequest> batch = new ArrayList<>();
        int count = 0;
        
        // 收集一批请求
        while (count < batchSize && !requestQueue.isEmpty()) {
            BatchRequest request = requestQueue.poll();
            if (request != null) {
                batch.add(request);
                count++;
            }
        }
        
        if (!batch.isEmpty()) {
            executorService.submit(() -> processBatchRequests(batch));
        }
    }
    
    private void processBatchRequests(List<BatchRequest> batch) {
        try {
            // 构建批处理提示
            String batchPrompt = buildBatchPrompt(batch);
            
            // 生成响应
            LLMResponse response = languageModel.generate(batchPrompt);
            
            // 解析并分发响应
            distributeBatchResponse(batch, response);
        } catch (Exception e) {
            // 处理错误
            handleBatchError(batch, e);
        }
    }
    
    private String buildBatchPrompt(List<BatchRequest> batch) {
        StringBuilder sb = new StringBuilder();
        sb.append("Process the following requests:\n\n");
        
        for (int i = 0; i < batch.size(); i++) {
            sb.append("Request ").append(i + 1).append(": ")
              .append(batch.get(i).getPrompt()).append("\n\n");
        }
        
        sb.append("Provide responses in the same order, numbered accordingly.");
        return sb.toString();
    }
    
    private void distributeBatchResponse(List<BatchRequest> batch, LLMResponse response) {
        // 解析批处理响应并分发给各个请求
        String[] responses = parseBatchResponse(response.getContent(), batch.size());
        
        for (int i = 0; i < batch.size(); i++) {
            BatchRequest request = batch.get(i);
            String individualResponse = i < responses.length ? responses[i] : "";
            request.getFuture().complete(new LLMResponse(individualResponse));
        }
    }
    
    private String[] parseBatchResponse(String batchResponse, int expectedCount) {
        // 简单解析批处理响应
        return batchResponse.split("\n\\d+\\.\\s*");
    }
    
    private void handleBatchError(List<BatchRequest> batch, Exception e) {
        for (BatchRequest request : batch) {
            request.getFuture().completeExceptionally(e);
        }
    }
    
    public void shutdown() {
        scheduler.shutdown();
        executorService.shutdown();
    }
}

class BatchRequest {
    private final String prompt;
    private final CompletableFuture<LLMResponse> future;
    
    public BatchRequest(String prompt, CompletableFuture<LLMResponse> future) {
        this.prompt = prompt;
        this.future = future;
    }
    
    public String getPrompt() { return prompt; }
    public CompletableFuture<LLMResponse> getFuture() { return future; }
}
```

## LLM集成到智能体系统

### LLM管理组件

```java
public class LLMManagementComponent extends AgentComponent {
    private LanguageModel languageModel;
    private PromptTemplateManager promptManager;
    private ResponseParser responseParser;
    private ChatHistory chatHistory;
    private ToolCallParser toolCallParser;
    private BatchProcessor batchProcessor;
    
    public LLMManagementComponent() {
        super("LLMManagement");
    }
    
    @Override
    public void initialize(AgentContext context, EventBus eventBus) {
        // 初始化LLM
        this.languageModel = createLanguageModel();
        
        // 初始化提示管理器
        this.promptManager = new PromptTemplateManager();
        
        // 初始化响应解析器
        this.responseParser = new ResponseParser();
        
        // 初始化对话历史
        this.chatHistory = new ChatHistory(4096, new SimpleTokenizer());
        
        // 初始化工具调用解析器
        this.toolCallParser = new ToolCallParser();
        
        // 初始化批处理器
        this.batchProcessor = new BatchProcessor(languageModel, 10, 1000);
        
        logger.info("LLM Management component initialized");
    }
    
    private LanguageModel createLanguageModel() {
        // 根据配置创建相应的LLM实例
        String modelType = System.getenv("LLM_MODEL_TYPE");
        String apiKey = System.getenv("LLM_API_KEY");
        
        LLMConfig config = new LLMConfig();
        config.setTemperature(0.7);
        config.setMaxTokens(2048);
        config.setCachingEnabled(true);
        
        if ("openai".equalsIgnoreCase(modelType)) {
            return new OpenAILanguageModel("gpt-3.5-turbo", config, apiKey);
        } else {
            // 默认使用模拟模型用于测试
            return new MockLanguageModel("mock-model", config);
        }
    }
    
    @Override
    public void process(AgentRequest request, AgentResponse response) {
        try {
            // 构建提示
            String prompt = buildPrompt(request);
            
            // 添加到对话历史
            chatHistory.addUserMessage(prompt);
            
            // 生成响应
            LLMResponse llmResponse = generateResponse(prompt);
            
            // 添加到对话历史
            chatHistory.addAssistantMessage(llmResponse.getContent());
            
            // 设置响应内容
            response.setOutput(llmResponse.getContent());
            
            // 解析工具调用
            List<ToolCall> toolCalls = toolCallParser.parseToolCalls(llmResponse.getContent());
            if (!toolCalls.isEmpty()) {
                for (ToolCall toolCall : toolCalls) {
                    response.addToolCall(toolCall);
                }
            }
            
            // 发布事件
            publishEvent(new LLMResponseEvent(this, llmResponse, request));
            
        } catch (Exception e) {
            logger.error("Failed to process LLM request", e);
            response.setOutput("Sorry, I encountered an error processing your request.");
            response.setType(ResponseType.ERROR);
        }
    }
    
    private String buildPrompt(AgentRequest request) {
        // 构建包含上下文的提示
        StringBuilder promptBuilder = new StringBuilder();
        
        // 添加系统提示
        promptBuilder.append("You are a helpful AI assistant.\n\n");
        
        // 添加对话历史
        String history = chatHistory.getFormattedHistory();
        if (!history.isEmpty()) {
            promptBuilder.append("Conversation history:\n").append(history).append("\n\n");
        }
        
        // 添加当前请求
        promptBuilder.append("Current request: ").append(request.getInput());
        
        return promptBuilder.toString();
    }
    
    private LLMResponse generateResponse(String prompt) throws LLMException {
        // 根据配置决定是否使用批处理
        if (shouldUseBatchProcessing(prompt)) {
            return batchProcessor.submitRequest(prompt).join();
        } else {
            return languageModel.generate(prompt);
        }
    }
    
    private boolean shouldUseBatchProcessing(String prompt) {
        // 简单策略：短提示使用批处理
        return prompt.length() < 100;
    }
    
    @Override
    public void cleanup() {
        if (batchProcessor != null) {
            batchProcessor.shutdown();
        }
    }
    
    // 公共API
    public LanguageModel getLanguageModel() {
        return languageModel;
    }
    
    public ChatHistory getChatHistory() {
        return chatHistory;
    }
    
    public void clearChatHistory() {
        chatHistory.clear();
    }
    
    public <T> T parseJsonResponse(String response, Class<T> targetType) throws LLMException {
        return responseParser.parseJsonResponse(response, targetType);
    }
}

// 模拟语言模型用于测试
class MockLanguageModel extends LanguageModel {
    public MockLanguageModel(String modelId, LLMConfig config) {
        super(modelId, config);
    }
    
    @Override
    public LLMResponse generate(String prompt) throws LLMException {
        // 模拟响应生成
        try {
            Thread.sleep(100); // 模拟处理时间
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        
        String response = "This is a mock response for: " + prompt;
        return new LLMResponse(response);
    }
    
    @Override
    public LLMResponse generate(ChatHistory history) throws LLMException {
        return generate(history.getFormattedHistory());
    }
    
    @Override
    public LLMResponse generateWithTools(String prompt, List<Tool> tools) throws LLMException {
        return generate(prompt);
    }
    
    @Override
    public Stream<LLMResponseChunk> generateStream(String prompt) throws LLMException {
        return Stream.empty();
    }
    
    @Override
    public float[] generateEmbedding(String text) throws LLMException {
        // 返回随机嵌入向量
        float[] embedding = new float[1536];
        for (int i = 0; i < embedding.length; i++) {
            embedding[i] = (float) Math.random();
        }
        return embedding;
    }
}
```

## 本节小结

本节我们设计并实现了一个完整的LLM集成框架，包括：

1. **统一接口设计**：构建了支持多种LLM的统一接口层
2. **OpenAI集成实现**：实现了与OpenAI GPT模型的完整集成
3. **提示工程和上下文管理**：提供了强大的提示模板和对话历史管理功能
4. **响应解析和后处理**：实现了智能的响应解析和格式化机制
5. **性能优化和缓存**：构建了智能缓存和批处理优化系统
6. **智能体集成**：将LLM能力无缝集成到智能体系统中

通过本节的实现，我们为智能体提供了强大的语言处理能力，使其能够：
- 理解和生成自然语言
- 维护对话上下文和历史
- 解析复杂的响应格式
- 高效地处理大量请求
- 支持多种LLM模型的接入

LLM集成是智能体系统的核心能力，它使智能体具备了类人的语言理解和表达能力，为构建更加智能和自然的AI交互体验奠定了基础。

## 本节小结

本节我们设计并实现了一个灵活的LLM集成框架,核心内容包括:

### 1. 统一接口设计
- **适配器模式**: 通过抽象基类LanguageModel实现多模型统一接入
- **核心方法**: generate、generateWithHistory、generateWithTools、generateStream、generateEmbedding
- **灵活配置**: LLMConfig支持temperature、maxTokens、缓存、超时等参数

### 2. 多模型支持
- **OpenAI GPT**: GPT-4/3.5-turbo,支持Function Calling
- **Anthropic Claude**: 高质量对话,长上下文
- **本地模型**: Ollama/vLLM,隐私保护
- **统一响应格式**: LLMResponse封装所有模型输出

### 3. 提示工程与上下文管理
- **提示模板**: 支持变量替换、条件逻辑、循环渲染
- **上下文窗口**: 滑动窗口策略控制token消耗
- **角色设定**: System/User/Assistant角色管理
- **历史压缩**: 智能总结早期对话

### 4. 响应处理与解析
- **结构化解析**: JSON、XML、Markdown格式提取
- **工具调用提取**: 从模型响应中解析Function Call
- **错误处理**: 异常捕获、重试机制、降级策略
- **后处理管道**: 过滤敏感信息、格式化输出

### 5. 性能优化
- **缓存机制**: LRU Cache减少重复调用
- **流式输出**: Server-Sent Events实时响应
- **批量处理**: 合并多个请求降低API调用
- **超时控制**: 防止长时间阻塞

### 6. 关键技术点

| 技术点 | 实现方式 | 作用 |
|--------|----------|------|
| **重试机制** | 指数退避(2^i*1000ms) | 应对网络抖动 |
| **缓存策略** | LRU Cache | 减少API调用成本 |
| **上下文管理** | 滑动窗口 | 控制token消耗 |
| **流式输出** | SSE/WebSocket | 提升用户体验 |
| **提示模板** | Jinja2-like | 提高可维护性 |
| **错误处理** | 分级重试+降级 | 保障稳定性 |

### 7. 实际应用价值

通过本节的实现,我们为LLM集成提供了:

- ✅ **多模型兼容**: 轻松切换OpenAI/Claude/本地模型
- ✅ **成本控制**: 通过缓存和批量处理降低API费用
- ✅ **用户体验**: 流式输出实时反馈
- ✅ **稳定可靠**: 完善的错误处理和重试机制
- ✅ **灵活扩展**: 易于集成新模型和功能

### 8. 下一步学习

至此,我们已经掌握了智能体系统的所有核心组件:
- ✅ 智能体架构设计(16.1-16.2)
- ✅ 记忆系统(16.3)
- ✅ 工具调用(16.4)
- ✅ LLM集成(16.5)

**在下一节中**,我们将把这些技术**整合到一起**,创建一个完整的综合项目,展示如何构建一个生产级的智能体系统!