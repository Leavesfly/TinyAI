# 10.3 Q-Learning算法实现

> **本节学习目标**：实现经典的Q-Learning算法并理解其原理，掌握Q-Learning基本原理、Q表更新规则、ε-贪心策略和收敛性分析

## 内容概览

Q-Learning是强化学习中最经典的无模型算法之一，它通过学习动作价值函数Q(s,a)来找到最优策略。本节将深入学习Q-Learning算法的基本原理，实现完整的Q-Learning算法，并探讨其收敛性分析和实际应用。

## 10.3.1 Q-Learning基本原理

Q-Learning是一种off-policy的时序差分学习算法，它直接学习最优动作价值函数Q*(s,a)，而不需要知道环境的转移概率。

### 算法核心思想

Q-Learning的核心更新规则为：
$$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha[r_{t+1} + \gamma \max_{a} Q(s_{t+1},a) - Q(s_t,a_t)]$$

其中：
- α是学习率
- γ是折扣因子
- r_{t+1}是即时奖励
- max_a Q(s_{t+1},a)是下一个状态的最大Q值

### 算法优势

1. **无模型**：不需要环境模型
2. **off-policy**：可以使用任意策略生成数据
3. **收敛性保证**：在适当条件下可以收敛到最优Q函数
4. **实用性**：在许多实际问题中表现良好

## 10.3.2 Q表实现

Q表是Q-Learning算法中最基础的数据结构，用于存储所有状态-动作对的Q值。

```java
/**
 * Q表实现
 */
public class QTable {
    private int numStates;
    private int numActions;
    private double[][] qValues;
    private Random random;
    
    /**
     * 构造函数
     */
    public QTable(int numStates, int numActions) {
        this.numStates = numStates;
        this.numActions = numActions;
        this.qValues = new double[numStates][numActions];
        this.random = new Random(42);
        
        // 初始化Q值
        initializeQValues();
    }
    
    /**
     * 初始化Q值
     */
    private void initializeQValues() {
        // 使用小的随机值初始化，打破对称性
        for (int s = 0; s < numStates; s++) {
            for (int a = 0; a < numActions; a++) {
                qValues[s][a] = random.nextGaussian() * 0.01;
            }
        }
    }
    
    /**
     * 获取Q值
     */
    public double getQValue(int state, int action) {
        if (isValidStateAction(state, action)) {
            return qValues[state][action];
        }
        return 0.0;
    }
    
    /**
     * 设置Q值
     */
    public void setQValue(int state, int action, double value) {
        if (isValidStateAction(state, action)) {
            qValues[state][action] = value;
        }
    }
    
    /**
     * 更新Q值（Q-Learning更新规则）
     */
    public void updateQValue(int state, int action, double reward, int nextState, 
                           double learningRate, double discountFactor, boolean isTerminal) {
        if (!isValidStateAction(state, action)) {
            return;
        }
        
        double currentQ = qValues[state][action];
        double nextMaxQ = isTerminal ? 0.0 : getMaxQValue(nextState);
        
        // Q-Learning更新规则
        double tdError = reward + discountFactor * nextMaxQ - currentQ;
        qValues[state][action] = currentQ + learningRate * tdError;
    }
    
    /**
     * 获取状态下的最大Q值
     */
    public double getMaxQValue(int state) {
        if (state < 0 || state >= numStates) {
            return 0.0;
        }
        
        return Arrays.stream(qValues[state]).max().orElse(0.0);
    }
    
    /**
     * 获取状态下的最优动作
     */
    public int getOptimalAction(int state) {
        if (state < 0 || state >= numStates) {
            return 0;
        }
        
        return argmax(qValues[state]);
    }
    
    /**
     * 获取状态下的所有Q值
     */
    public double[] getQValues(int state) {
        if (state < 0 || state >= numStates) {
            return new double[numActions];
        }
        
        return qValues[state].clone();
    }
    
    /**
     * 获取最大值索引
     */
    private int argmax(double[] array) {
        int maxIndex = 0;
        for (int i = 1; i < array.length; i++) {
            if (array[i] > array[maxIndex]) {
                maxIndex = i;
            }
        }
        return maxIndex;
    }
    
    /**
     * 验证状态和动作的有效性
     */
    private boolean isValidStateAction(int state, int action) {
        return state >= 0 && state < numStates && action >= 0 && action < numActions;
    }
    
    /**
     * 获取Q表大小
     */
    public int getSize() {
        return numStates * numActions;
    }
    
    /**
     * 获取所有Q值的副本
     */
    public double[][] getAllQValues() {
        double[][] copy = new double[qValues.length][];
        for (int i = 0; i < qValues.length; i++) {
            copy[i] = qValues[i].clone();
        }
        return copy;
    }
    
    /**
     * 重置Q表
     */
    public void reset() {
        initializeQValues();
    }
    
    /**
     * 打印Q表（用于调试）
     */
    public void printQTable() {
        System.out.println("Q表内容:");
        for (int s = 0; s < Math.min(10, numStates); s++) { // 只显示前10个状态
            System.out.printf("状态 %d: ", s);
            for (int a = 0; a < numActions; a++) {
                System.out.printf("动作 %d=%.3f ", a, qValues[s][a]);
            }
            System.out.println();
        }
    }
}
```

## 10.3.3 ε-贪心策略实现

ε-贪心策略是Q-Learning中平衡探索与利用的关键技术。

```java
/**
 * ε-贪心策略实现
 */
public class EpsilonGreedyPolicy {
    private QTable qTable;
    private double epsilon;
    private Random random;
    private boolean decayEpsilon;
    private double epsilonDecay;
    private double minEpsilon;
    
    /**
     * 构造函数
     */
    public EpsilonGreedyPolicy(QTable qTable, double initialEpsilon) {
        this.qTable = qTable;
        this.epsilon = initialEpsilon;
        this.random = new Random(42);
        this.decayEpsilon = false;
        this.epsilonDecay = 0.995;
        this.minEpsilon = 0.01;
    }
    
    /**
     * 构造函数（带衰减）
     */
    public EpsilonGreedyPolicy(QTable qTable, double initialEpsilon, 
                             double epsilonDecay, double minEpsilon) {
        this.qTable = qTable;
        this.epsilon = initialEpsilon;
        this.random = new Random(42);
        this.decayEpsilon = true;
        this.epsilonDecay = epsilonDecay;
        this.minEpsilon = minEpsilon;
    }
    
    /**
     * 根据ε-贪心策略选择动作
     */
    public int selectAction(int state, int numActions) {
        // ε概率随机探索
        if (random.nextDouble() < epsilon) {
            return random.nextInt(numActions);
        }
        
        // 1-ε概率贪婪利用
        return qTable.getOptimalAction(state);
    }
    
    /**
     * 衰减ε值
     */
    public void decayEpsilon() {
        if (decayEpsilon) {
            epsilon = Math.max(minEpsilon, epsilon * epsilonDecay);
        }
    }
    
    /**
     * 获取当前ε值
     */
    public double getEpsilon() {
        return epsilon;
    }
    
    /**
     * 设置ε值
     */
    public void setEpsilon(double epsilon) {
        this.epsilon = Math.max(0.0, Math.min(1.0, epsilon));
    }
    
    /**
     * 获取ε衰减率
     */
    public double getEpsilonDecay() {
        return epsilonDecay;
    }
    
    /**
     * 设置ε衰减率
     */
    public void setEpsilonDecay(double epsilonDecay) {
        this.epsilonDecay = epsilonDecay;
    }
    
    /**
     * 获取最小ε值
     */
    public double getMinEpsilon() {
        return minEpsilon;
    }
    
    /**
     * 设置最小ε值
     */
    public void setMinEpsilon(double minEpsilon) {
        this.minEpsilon = minEpsilon;
    }
}
```

## 10.3.4 完整的Q-Learning算法实现

现在我们将所有组件整合成完整的Q-Learning算法实现：

```java
/**
 * Q-Learning算法实现
 */
public class QLearning {
    private QTable qTable;
    private EpsilonGreedyPolicy policy;
    private double learningRate;
    private double discountFactor;
    private int numStates;
    private int numActions;
    private TrainingStats trainingStats;
    
    /**
     * 构造函数
     */
    public QLearning(int numStates, int numActions, double learningRate, 
                    double discountFactor, double initialEpsilon) {
        this.numStates = numStates;
        this.numActions = numActions;
        this.learningRate = learningRate;
        this.discountFactor = discountFactor;
        
        // 初始化组件
        this.qTable = new QTable(numStates, numActions);
        this.policy = new EpsilonGreedyPolicy(qTable, initialEpsilon);
        this.trainingStats = new TrainingStats();
    }
    
    /**
     * 构造函数（带ε衰减）
     */
    public QLearning(int numStates, int numActions, double learningRate, 
                    double discountFactor, double initialEpsilon,
                    double epsilonDecay, double minEpsilon) {
        this.numStates = numStates;
        this.numActions = numActions;
        this.learningRate = learningRate;
        this.discountFactor = discountFactor;
        
        // 初始化组件
        this.qTable = new QTable(numStates, numActions);
        this.policy = new EpsilonGreedyPolicy(qTable, initialEpsilon, epsilonDecay, minEpsilon);
        this.trainingStats = new TrainingStats();
    }
    
    /**
     * 单步Q-Learning更新
     */
    public void update(int state, int action, double reward, int nextState, boolean isTerminal) {
        // 更新Q表
        qTable.updateQValue(state, action, reward, nextState, learningRate, discountFactor, isTerminal);
        
        // 更新训练统计
        trainingStats.incrementUpdates();
    }
    
    /**
     * 根据策略选择动作
     */
    public int selectAction(int state) {
        return policy.selectAction(state, numActions);
    }
    
    /**
     * 训练一个回合
     */
    public EpisodeResult trainEpisode(Environment environment, int maxSteps) {
        // 初始化回合
        environment.reset();
        int state = environment.getCurrentState();
        double totalReward = 0;
        int steps = 0;
        
        // 执行回合
        while (!environment.isTerminal() && steps < maxSteps) {
            // 选择动作
            int action = selectAction(state);
            
            // 执行动作
            environment.step(action);
            int nextState = environment.getCurrentState();
            double reward = environment.getReward();
            boolean isTerminal = environment.isTerminal();
            
            // Q-Learning更新
            update(state, action, reward, nextState, isTerminal);
            
            // 更新统计
            totalReward += reward;
            steps++;
            state = nextState;
        }
        
        // 衰减ε
        policy.decayEpsilon();
        
        // 更新训练统计
        trainingStats.addEpisode(totalReward, steps);
        
        return new EpisodeResult(totalReward, steps, environment.isTerminal());
    }
    
    /**
     * 训练多个回合
     */
    public TrainingResult train(Environment environment, int numEpisodes, int maxStepsPerEpisode) {
        System.out.println("开始Q-Learning训练...");
        System.out.printf("回合数: %d, 每回合最大步数: %d%n", numEpisodes, maxStepsPerEpisode);
        System.out.printf("学习率: %.3f, 折扣因子: %.3f, 初始ε: %.3f%n", 
                         learningRate, discountFactor, policy.getEpsilon());
        System.out.println("------------------------");
        
        long startTime = System.currentTimeMillis();
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            EpisodeResult result = trainEpisode(environment, maxStepsPerEpisode);
            
            // 打印进度
            if (episode % 100 == 0 || episode == numEpisodes - 1) {
                double avgReward = trainingStats.getAverageReward();
                double avgSteps = trainingStats.getAverageSteps();
                System.out.printf("回合 %d: 平均奖励=%.2f, 平均步数=%.1f, ε=%.3f%n",
                    episode, avgReward, avgSteps, policy.getEpsilon());
            }
        }
        
        long endTime = System.currentTimeMillis();
        long trainingTime = endTime - startTime;
        
        System.out.println("训练完成！");
        System.out.printf("训练时间: %.2f 秒%n", trainingTime / 1000.0);
        
        return new TrainingResult(qTable.getAllQValues(), trainingStats, trainingTime);
    }
    
    /**
     * 测试策略性能
     */
    public TestResult testPolicy(Environment environment, int numEpisodes, int maxStepsPerEpisode) {
        System.out.println("开始策略测试...");
        
        double totalReward = 0;
        int totalSteps = 0;
        int successCount = 0;
        
        // 临时禁用探索
        double originalEpsilon = policy.getEpsilon();
        policy.setEpsilon(0.0); // 纯贪婪策略
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            environment.reset();
            int state = environment.getCurrentState();
            double episodeReward = 0;
            int steps = 0;
            
            while (!environment.isTerminal() && steps < maxStepsPerEpisode) {
                int action = selectAction(state);
                environment.step(action);
                state = environment.getCurrentState();
                episodeReward += environment.getReward();
                steps++;
            }
            
            totalReward += episodeReward;
            totalSteps += steps;
            if (environment.isTerminal()) {
                successCount++;
            }
        }
        
        // 恢复原始ε值
        policy.setEpsilon(originalEpsilon);
        
        double avgReward = totalReward / numEpisodes;
        double avgSteps = (double) totalSteps / numEpisodes;
        double successRate = (double) successCount / numEpisodes;
        
        System.out.printf("测试完成: 平均奖励=%.2f, 平均步数=%.1f, 成功率=%.2f%%%n",
            avgReward, avgSteps, successRate * 100);
        
        return new TestResult(avgReward, avgSteps, successRate);
    }
    
    /**
     * 获取最优策略
     */
    public int[] getOptimalPolicy() {
        int[] optimalPolicy = new int[numStates];
        for (int s = 0; s < numStates; s++) {
            optimalPolicy[s] = qTable.getOptimalAction(s);
        }
        return optimalPolicy;
    }
    
    /**
     * 获取Q表
     */
    public QTable getQTable() {
        return qTable;
    }
    
    /**
     * 获取策略
     */
    public EpsilonGreedyPolicy getPolicy() {
        return policy;
    }
    
    /**
     * 获取训练统计
     */
    public TrainingStats getTrainingStats() {
        return trainingStats;
    }
}

/**
 * 环境接口
 */
interface Environment {
    /**
     * 重置环境
     */
    void reset();
    
    /**
     * 执行动作
     */
    void step(int action);
    
    /**
     * 获取当前状态
     */
    int getCurrentState();
    
    /**
     * 获取奖励
     */
    double getReward();
    
    /**
     * 检查是否为终止状态
     */
    boolean isTerminal();
}

/**
 * 回合结果
 */
class EpisodeResult {
    private double totalReward;
    private int steps;
    private boolean terminated;
    
    public EpisodeResult(double totalReward, int steps, boolean terminated) {
        this.totalReward = totalReward;
        this.steps = steps;
        this.terminated = terminated;
    }
    
    // Getter方法
    public double getTotalReward() { return totalReward; }
    public int getSteps() { return steps; }
    public boolean isTerminated() { return terminated; }
}

/**
 * 训练结果
 */
class TrainingResult {
    private double[][] finalQValues;
    private TrainingStats trainingStats;
    private long trainingTime;
    
    public TrainingResult(double[][] finalQValues, TrainingStats trainingStats, long trainingTime) {
        this.finalQValues = finalQValues;
        this.trainingStats = trainingStats;
        this.trainingTime = trainingTime;
    }
    
    // Getter方法
    public double[][] getFinalQValues() { return finalQValues; }
    public TrainingStats getTrainingStats() { return trainingStats; }
    public long getTrainingTime() { return trainingTime; }
}

/**
 * 测试结果
 */
class TestResult {
    private double averageReward;
    private double averageSteps;
    private double successRate;
    
    public TestResult(double averageReward, double averageSteps, double successRate) {
        this.averageReward = averageReward;
        this.averageSteps = averageSteps;
        this.successRate = successRate;
    }
    
    // Getter方法
    public double getAverageReward() { return averageReward; }
    public double getAverageSteps() { return averageSteps; }
    public double getSuccessRate() { return successRate; }
}

/**
 * 训练统计
 */
class TrainingStats {
    private List<Double> episodeRewards;
    private List<Integer> episodeSteps;
    private int totalUpdates;
    
    public TrainingStats() {
        this.episodeRewards = new ArrayList<>();
        this.episodeSteps = new ArrayList<>();
        this.totalUpdates = 0;
    }
    
    /**
     * 添加回合统计
     */
    public void addEpisode(double reward, int steps) {
        episodeRewards.add(reward);
        episodeSteps.add(steps);
    }
    
    /**
     * 增加更新次数
     */
    public void incrementUpdates() {
        totalUpdates++;
    }
    
    /**
     * 获取平均奖励
     */
    public double getAverageReward() {
        return episodeRewards.stream().mapToDouble(Double::doubleValue).average().orElse(0);
    }
    
    /**
     * 获取平均步数
     */
    public double getAverageSteps() {
        return episodeSteps.stream().mapToInt(Integer::intValue).average().orElse(0);
    }
    
    /**
     * 获取最近N回合的平均奖励
     */
    public double getAverageReward(int lastN) {
        int start = Math.max(0, episodeRewards.size() - lastN);
        return episodeRewards.subList(start, episodeRewards.size())
                .stream().mapToDouble(Double::doubleValue).average().orElse(0);
    }
    
    /**
     * 获取总更新次数
     */
    public int getTotalUpdates() {
        return totalUpdates;
    }
    
    /**
     * 获取回合数
     */
    public int getEpisodeCount() {
        return episodeRewards.size();
    }
    
    /**
     * 获取所有奖励
     */
    public List<Double> getAllRewards() {
        return new ArrayList<>(episodeRewards);
    }
    
    /**
     * 获取所有步数
     */
    public List<Integer> getAllSteps() {
        return new ArrayList<>(episodeSteps);
    }
}
```

## 10.3.5 收敛性分析

Q-Learning算法在满足一定条件下可以收敛到最优Q函数。

### 收敛条件

1. **所有状态-动作对被无限次访问**
2. **学习率满足Robbins-Monro条件**：
   - $\sum_{t=1}^{\infty} \alpha_t = \infty$
   - $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$
3. **策略充分探索**：ε-贪心策略的ε逐渐衰减到0

### 收敛性监控

```java
/**
 * Q-Learning收敛性监控器
 */
public class ConvergenceMonitor {
    private double[][] previousQValues;
    private double convergenceThreshold;
    private int consecutiveConvergedEpisodes;
    private int requiredConsecutiveEpisodes;
    private List<Double> qValueChanges;
    
    /**
     * 构造函数
     */
    public ConvergenceMonitor(double convergenceThreshold, int requiredConsecutiveEpisodes) {
        this.convergenceThreshold = convergenceThreshold;
        this.requiredConsecutiveEpisodes = requiredConsecutiveEpisodes;
        this.consecutiveConvergedEpisodes = 0;
        this.qValueChanges = new ArrayList<>();
    }
    
    /**
     * 检查是否收敛
     */
    public boolean checkConvergence(QTable currentQTable) {
        double[][] currentQValues = currentQTable.getAllQValues();
        
        if (previousQValues == null) {
            previousQValues = cloneQValues(currentQValues);
            return false;
        }
        
        // 计算Q值变化
        double maxChange = computeMaxChange(currentQValues, previousQValues);
        qValueChanges.add(maxChange);
        
        // 检查是否收敛
        boolean isConverged = maxChange < convergenceThreshold;
        
        if (isConverged) {
            consecutiveConvergedEpisodes++;
        } else {
            consecutiveConvergedEpisodes = 0;
        }
        
        // 更新前一个Q值
        previousQValues = cloneQValues(currentQValues);
        
        // 检查是否连续收敛足够次数
        return consecutiveConvergedEpisodes >= requiredConsecutiveEpisodes;
    }
    
    /**
     * 计算Q值的最大变化
     */
    private double computeMaxChange(double[][] current, double[][] previous) {
        double maxChange = 0.0;
        
        for (int s = 0; s < current.length; s++) {
            for (int a = 0; a < current[s].length; a++) {
                double change = Math.abs(current[s][a] - previous[s][a]);
                maxChange = Math.max(maxChange, change);
            }
        }
        
        return maxChange;
    }
    
    /**
     * 克隆Q值数组
     */
    private double[][] cloneQValues(double[][] qValues) {
        double[][] clone = new double[qValues.length][];
        for (int i = 0; i < qValues.length; i++) {
            clone[i] = qValues[i].clone();
        }
        return clone;
    }
    
    /**
     * 获取Q值变化历史
     */
    public List<Double> getQValueChanges() {
        return new ArrayList<>(qValueChanges);
    }
    
    /**
     * 获取平均Q值变化
     */
    public double getAverageChange() {
        return qValueChanges.stream().mapToDouble(Double::doubleValue).average().orElse(0);
    }
    
    /**
     * 重置监控器
     */
    public void reset() {
        previousQValues = null;
        consecutiveConvergedEpisodes = 0;
        qValueChanges.clear();
    }
}
```

## 10.3.6 参数调优

Q-Learning的性能很大程度上依赖于参数设置，需要仔细调优。

```java
/**
 * Q-Learning参数调优器
 */
public class QLearningParameterTuner {
    
    /**
     * 学习率调优
     */
    public static TuningResult tuneLearningRate(Environment environment, 
                                              int numStates, int numActions,
                                              double[] learningRates) {
        System.out.println("=== 学习率调优 ===");
        
        double bestLearningRate = 0.0;
        double bestPerformance = Double.NEGATIVE_INFINITY;
        Map<Double, Double> results = new HashMap<>();
        
        for (double learningRate : learningRates) {
            QLearning ql = new QLearning(numStates, numActions, learningRate, 0.99, 0.1);
            TrainingResult result = ql.train(environment, 1000, 100);
            TestResult testResult = ql.testPolicy(environment, 100, 100);
            
            double performance = testResult.getAverageReward();
            results.put(learningRate, performance);
            
            if (performance > bestPerformance) {
                bestPerformance = performance;
                bestLearningRate = learningRate;
            }
            
            System.out.printf("学习率 %.3f: 平均奖励 %.2f%n", learningRate, performance);
        }
        
        System.out.printf("最佳学习率: %.3f, 性能: %.2f%n", bestLearningRate, bestPerformance);
        
        return new TuningResult(bestLearningRate, bestPerformance, results);
    }
    
    /**
     * 折扣因子调优
     */
    public static TuningResult tuneDiscountFactor(Environment environment,
                                                int numStates, int numActions,
                                                double learningRate,
                                                double[] discountFactors) {
        System.out.println("=== 折扣因子调优 ===");
        
        double bestDiscountFactor = 0.0;
        double bestPerformance = Double.NEGATIVE_INFINITY;
        Map<Double, Double> results = new HashMap<>();
        
        for (double discountFactor : discountFactors) {
            QLearning ql = new QLearning(numStates, numActions, learningRate, discountFactor, 0.1);
            TrainingResult result = ql.train(environment, 1000, 100);
            TestResult testResult = ql.testPolicy(environment, 100, 100);
            
            double performance = testResult.getAverageReward();
            results.put(discountFactor, performance);
            
            if (performance > bestPerformance) {
                bestPerformance = performance;
                bestDiscountFactor = discountFactor;
            }
            
            System.out.printf("折扣因子 %.3f: 平均奖励 %.2f%n", discountFactor, performance);
        }
        
        System.out.printf("最佳折扣因子: %.3f, 性能: %.2f%n", bestDiscountFactor, bestPerformance);
        
        return new TuningResult(bestDiscountFactor, bestPerformance, results);
    }
    
    /**
     * ε参数调优
     */
    public static EpsilonTuningResult tuneEpsilonParameters(Environment environment,
                                                          int numStates, int numActions,
                                                          double learningRate, double discountFactor,
                                                          double[] initialEpsilons,
                                                          double[] epsilonDecays) {
        System.out.println("=== ε参数调优 ===");
        
        double bestInitialEpsilon = 0.0;
        double bestEpsilonDecay = 0.0;
        double bestPerformance = Double.NEGATIVE_INFINITY;
        Map<String, Double> results = new HashMap<>();
        
        for (double initialEpsilon : initialEpsilons) {
            for (double epsilonDecay : epsilonDecays) {
                QLearning ql = new QLearning(numStates, numActions, learningRate, discountFactor, 
                                           initialEpsilon, epsilonDecay, 0.01);
                TrainingResult result = ql.train(environment, 1000, 100);
                TestResult testResult = ql.testPolicy(environment, 100, 100);
                
                double performance = testResult.getAverageReward();
                String key = String.format("%.2f_%.3f", initialEpsilon, epsilonDecay);
                results.put(key, performance);
                
                if (performance > bestPerformance) {
                    bestPerformance = performance;
                    bestInitialEpsilon = initialEpsilon;
                    bestEpsilonDecay = epsilonDecay;
                }
                
                System.out.printf("初始ε %.2f, 衰减 %.3f: 平均奖励 %.2f%n", 
                                initialEpsilon, epsilonDecay, performance);
            }
        }
        
        System.out.printf("最佳参数: 初始ε %.2f, 衰减 %.3f, 性能: %.2f%n", 
                         bestInitialEpsilon, bestEpsilonDecay, bestPerformance);
        
        return new EpsilonTuningResult(bestInitialEpsilon, bestEpsilonDecay, bestPerformance, results);
    }
}

/**
 * 调优结果
 */
class TuningResult {
    private double bestParameter;
    private double bestPerformance;
    private Map<Double, Double> allResults;
    
    public TuningResult(double bestParameter, double bestPerformance, Map<Double, Double> allResults) {
        this.bestParameter = bestParameter;
        this.bestPerformance = bestPerformance;
        this.allResults = allResults;
    }
    
    // Getter方法
    public double getBestParameter() { return bestParameter; }
    public double getBestPerformance() { return bestPerformance; }
    public Map<Double, Double> getAllResults() { return allResults; }
}

/**
 * ε参数调优结果
 */
class EpsilonTuningResult {
    private double bestInitialEpsilon;
    private double bestEpsilonDecay;
    private double bestPerformance;
    private Map<String, Double> allResults;
    
    public EpsilonTuningResult(double bestInitialEpsilon, double bestEpsilonDecay, 
                             double bestPerformance, Map<String, Double> allResults) {
        this.bestInitialEpsilon = bestInitialEpsilon;
        this.bestEpsilonDecay = bestEpsilonDecay;
        this.bestPerformance = bestPerformance;
        this.allResults = allResults;
    }
    
    // Getter方法
    public double getBestInitialEpsilon() { return bestInitialEpsilon; }
    public double getBestEpsilonDecay() { return bestEpsilonDecay; }
    public double getBestPerformance() { return bestPerformance; }
    public Map<String, Double> getAllResults() { return allResults; }
}
```

## 10.3.7 完整示例：网格世界中的Q-Learning

让我们通过一个完整的示例来演示Q-Learning算法的实际应用：

```java
/**
 * 网格世界环境实现
 */
public class GridWorldEnvironment implements Environment {
    private int gridSize;
    private int numStates;
    private int currentState;
    private int goalState;
    private int obstacleState;
    private int[][] actions; // 上、下、左、右
    private Random random;
    
    /**
     * 构造函数
     */
    public GridWorldEnvironment(int gridSize) {
        this.gridSize = gridSize;
        this.numStates = gridSize * gridSize;
        this.actions = new int[][]{{-1, 0}, {1, 0}, {0, -1}, {0, 1}};
        this.random = new Random(42);
        this.goalState = numStates - 1; // 右下角为目标
        this.obstacleState = 10; // 障碍物位置
        reset();
    }
    
    @Override
    public void reset() {
        // 从左上角开始
        currentState = 0;
    }
    
    @Override
    public void step(int action) {
        if (isTerminal()) {
            return;
        }
        
        int row = currentState / gridSize;
        int col = currentState % gridSize;
        
        // 执行动作
        int newRow = row + actions[action][0];
        int newCol = col + actions[action][1];
        
        // 检查边界
        if (newRow >= 0 && newRow < gridSize && newCol >= 0 && newCol < gridSize) {
            int newState = newRow * gridSize + newCol;
            
            // 检查障碍物
            if (newState != obstacleState) {
                currentState = newState;
            }
        }
        // 如果超出边界或撞到障碍物，保持原位置
    }
    
    @Override
    public int getCurrentState() {
        return currentState;
    }
    
    @Override
    public double getReward() {
        if (currentState == goalState) {
            return 10.0; // 到达目标
        } else if (currentState == obstacleState) {
            return -10.0; // 撞到障碍物
        } else {
            return -1.0; // 每步惩罚
        }
    }
    
    @Override
    public boolean isTerminal() {
        return currentState == goalState || currentState == obstacleState;
    }
    
    /**
     * 显示环境状态
     */
    public void displayEnvironment() {
        System.out.println("网格世界环境:");
        for (int row = 0; row < gridSize; row++) {
            for (int col = 0; col < gridSize; col++) {
                int state = row * gridSize + col;
                if (state == currentState) {
                    System.out.print("A "); // Agent位置
                } else if (state == goalState) {
                    System.out.print("G "); // 目标位置
                } else if (state == obstacleState) {
                    System.out.print("X "); // 障碍物
                } else {
                    System.out.print(". "); // 空位置
                }
            }
            System.out.println();
        }
        System.out.println();
    }
}

/**
 * Q-Learning完整示例
 */
public class QLearningExample {
    public static void main(String[] args) {
        System.out.println("=== Q-Learning算法完整示例 ===");
        
        // 创建环境
        int gridSize = 4;
        GridWorldEnvironment environment = new GridWorldEnvironment(gridSize);
        int numStates = gridSize * gridSize;
        int numActions = 4;
        
        // 显示初始环境
        System.out.println("初始环境状态:");
        environment.displayEnvironment();
        
        // 创建Q-Learning算法
        QLearning ql = new QLearning(numStates, numActions, 0.1, 0.99, 1.0, 0.995, 0.01);
        
        // 训练
        System.out.println("开始训练...");
        TrainingResult trainingResult = ql.train(environment, 5000, 100);
        
        // 测试
        System.out.println("\n训练完成后测试策略:");
        TestResult testResult = ql.testPolicy(environment, 100, 100);
        
        // 显示最优策略
        System.out.println("\n最优策略:");
        displayOptimalPolicy(ql.getQTable(), gridSize);
        
        // 显示Q值
        System.out.println("\n部分Q值:");
        ql.getQTable().printQTable();
        
        // 收敛性分析
        System.out.println("\n收敛性分析:");
        analyzeConvergence(trainingResult.getTrainingStats());
        
        // 参数调优示例
        System.out.println("\n参数调优示例:");
        performParameterTuning(gridSize);
    }
    
    /**
     * 显示最优策略
     */
    private static void displayOptimalPolicy(QTable qTable, int gridSize) {
        String[] actionSymbols = {"↑", "↓", "←", "→"};
        System.out.println("最优策略:");
        for (int row = 0; row < gridSize; row++) {
            for (int col = 0; col < gridSize; col++) {
                int state = row * gridSize + col;
                int action = qTable.getOptimalAction(state);
                System.out.printf("%2s ", actionSymbols[action]);
            }
            System.out.println();
        }
    }
    
    /**
     * 分析收敛性
     */
    private static void analyzeConvergence(TrainingStats trainingStats) {
        List<Double> rewards = trainingStats.getAllRewards();
        
        // 计算滑动平均
        int windowSize = 100;
        List<Double> movingAverages = new ArrayList<>();
        
        for (int i = 0; i < rewards.size(); i++) {
            int start = Math.max(0, i - windowSize + 1);
            double avg = rewards.subList(start, i + 1).stream()
                               .mapToDouble(Double::doubleValue).average().orElse(0);
            movingAverages.add(avg);
        }
        
        // 显示最后几个回合的性能
        System.out.println("训练性能趋势:");
        int totalEpisodes = rewards.size();
        for (int i = Math.max(0, totalEpisodes - 5); i < totalEpisodes; i++) {
            System.out.printf("回合 %d: 奖励 %.2f, 滑动平均 %.2f%n", 
                            i, rewards.get(i), movingAverages.get(i));
        }
    }
    
    /**
     * 参数调优示例
     */
    private static void performParameterTuning(int gridSize) {
        GridWorldEnvironment env = new GridWorldEnvironment(gridSize);
        int numStates = gridSize * gridSize;
        int numActions = 4;
        
        // 学习率调优
        double[] learningRates = {0.01, 0.05, 0.1, 0.2, 0.5};
        TuningResult lrResult = QLearningParameterTuner.tuneLearningRate(
            env, numStates, numActions, learningRates);
        
        // 折扣因子调优
        double[] discountFactors = {0.9, 0.95, 0.99, 0.995};
        TuningResult dfResult = QLearningParameterTuner.tuneDiscountFactor(
            env, numStates, numActions, lrResult.getBestParameter(), discountFactors);
        
        System.out.println("推荐参数:");
        System.out.printf("学习率: %.3f%n", lrResult.getBestParameter());
        System.out.printf("折扣因子: %.3f%n", dfResult.getBestParameter());
    }
}
```

## 本节小结

在本节中，我们深入学习了Q-Learning算法的完整实现：

1. **Q表实现**：学习了Q值的存储和更新方法
2. **ε-贪心策略**：掌握了平衡探索与利用的关键技术
3. **完整算法实现**：构建了完整的Q-Learning训练和测试框架
4. **收敛性分析**：理解了算法的收敛条件和监控方法
5. **参数调优**：学习了关键参数的优化方法

通过Java代码实现，我们不仅掌握了理论知识，还获得了实际的编程经验。Q-Learning是强化学习的基础算法，为后续学习更复杂的深度强化学习算法奠定了基础。

## 下一步计划

在下一节中，我们将学习深度Q网络（DQN），将Q-Learning与深度学习相结合，解决大规模状态空间问题。