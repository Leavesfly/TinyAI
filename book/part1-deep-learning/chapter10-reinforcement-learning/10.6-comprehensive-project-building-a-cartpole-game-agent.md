# 10.6 ç»¼åˆé¡¹ç›®: æ„å»ºCartPoleæ¸¸æˆæ™ºèƒ½ä½“

> **æœ¬èŠ‚å­¦ä¹ ç›®æ ‡**: é€šè¿‡å®Œæ•´é¡¹ç›®æ•´åˆæ‰€å­¦çŸ¥è¯†,ä»é›¶å¼€å§‹æ„å»ºã€è®­ç»ƒã€ä¼˜åŒ–å’Œéƒ¨ç½²ä¸€ä¸ªDQNæ™ºèƒ½ä½“

## å†…å®¹æ¦‚è§ˆ

å­¦äº†è¿™ä¹ˆå¤šç†è®º,ç°åœ¨æ˜¯å®æˆ˜çš„æ—¶å€™äº†!æœ¬èŠ‚æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ é¡¹ç›®â€”â€”è®­ç»ƒAIç©CartPole(å€’ç«‹æ‘†)æ¸¸æˆã€‚å°±åƒå­¦å®Œé©¾é©¶ç†è®ºå,ç»ˆäºå¯ä»¥ä¸Šè·¯å¼€è½¦äº†!

## 10.6.1 é¡¹ç›®æ¦‚è¿°: CartPoleæ¸¸æˆ

### ä»€ä¹ˆæ˜¯CartPole?

CartPoleæ˜¯å¼ºåŒ–å­¦ä¹ çš„"Hello World",ç±»ä¼¼äºç¼–ç¨‹ä¸­çš„ç¬¬ä¸€ä¸ªç¨‹åºã€‚

**æ¸¸æˆè§„åˆ™**: 
- æœ‰ä¸€æ ¹æ†å­ç«‹åœ¨å°è½¦ä¸Š
- å°è½¦å¯ä»¥å·¦å³ç§»åŠ¨
- ç›®æ ‡: é€šè¿‡ç§»åŠ¨å°è½¦ä¿æŒæ†å­ä¸å€’

```mermaid
graph TB
    subgraph CartPoleç¯å¢ƒ
        Cart[å°è½¦Cart] --> Pole[æ†å­Pole]
        Cart --> Left[åŠ¨ä½œ:å‘å·¦]
        Cart --> Right[åŠ¨ä½œ:å‘å³]
        
        Pole --> Angle[è§’åº¦Î¸]
        Pole --> Velocity[è§’é€Ÿåº¦Ï‰]
        
        Cart --> Position[ä½ç½®x]
        Cart --> Speed[é€Ÿåº¦v]
    end
    
    Goal[ç›®æ ‡: ä¿æŒå¹³è¡¡]
    Fail[å¤±è´¥æ¡ä»¶]
    
    Goal --> F1[æ†å­å€¾æ–œ > 12Â°]
    Goal --> F2[å°è½¦ç§»å‡ºè¾¹ç•Œ > Â±2.4]
    
    style Cart fill:#bbdefb
    style Pole fill:#ffe0b2
    style Goal fill:#c8e6c9
    style Fail fill:#ffccbc
```

### ç”Ÿæ´»ç±»æ¯”: é¡¶æ‰«å¸šæ¸¸æˆ

æƒ³è±¡ä½ ç”¨æ‰‹æŒé¡¶ä¸€æ ¹æ‰«å¸š:
- **æ†å­**: æ‰«å¸š
- **å°è½¦**: ä½ çš„æ‰‹æŒ
- **åŠ¨ä½œ**: æ‰‹æŒå·¦å³ç§»åŠ¨
- **ç›®æ ‡**: ä¸è®©æ‰«å¸šå€’ä¸‹

```mermaid
graph LR
    Start[æ‰«å¸šå¼€å§‹å€¾æ–œ] --> Detect[æ„ŸçŸ¥è§’åº¦]
    Detect --> Decide[å†³å®šå‘å“ªç§»åŠ¨]
    Decide --> Move[å¿«é€Ÿç§»åŠ¨]
    Move --> Balance[æ¢å¤å¹³è¡¡]
    Balance --> Repeat[ç»§ç»­ç›‘æ§]
    Repeat --> Start
    
    style Start fill:#ffe0b2
    style Decide fill:#fff3e0
    style Balance fill:#c8e6c9
```

### çŠ¶æ€ç©ºé—´(4ç»´)

| ç»´åº¦ | å«ä¹‰ | èŒƒå›´ |
|------|------|------|
| x | å°è½¦ä½ç½® | [-2.4, 2.4] |
| v | å°è½¦é€Ÿåº¦ | [-âˆ, +âˆ] |
| Î¸ | æ†å­è§’åº¦ | [-12Â°, 12Â°] |
| Ï‰ | æ†å­è§’é€Ÿåº¦ | [-âˆ, +âˆ] |

### åŠ¨ä½œç©ºé—´(2ä¸ªç¦»æ•£åŠ¨ä½œ)

- **åŠ¨ä½œ0**: å‘å·¦æ¨ â†
- **åŠ¨ä½œ1**: å‘å³æ¨ â†’

### å¥–åŠ±æœºåˆ¶

```mermaid
graph LR
    Step[æ¯ä¸ªæ—¶é—´æ­¥] --> Reward[+1åˆ†]
    Pole[æ†å­å€’ä¸‹] --> End[å›åˆç»“æŸ]
    Cart[å°è½¦å‡ºç•Œ] --> End
    
    Target[ç›®æ ‡: å¾—åˆ†>195åˆ†<br/>å¹³å‡100å›åˆ]
    
    style Reward fill:#c8e6c9
    style End fill:#ffccbc
    style Target fill:#ffd54f
```

- æ¯ä¿æŒå¹³è¡¡1æ­¥: +1åˆ†
- æ†å­å€’ä¸‹æˆ–å‡ºç•Œ: å›åˆç»“æŸ
- **æˆåŠŸæ ‡å‡†**: è¿ç»­100å›åˆå¹³å‡åˆ†>195

## 10.6.2 ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph ç¯å¢ƒå±‚
        Env[CartPoleç¯å¢ƒ]
    end
    
    subgraph æ™ºèƒ½ä½“å±‚
        Agent[DQNæ™ºèƒ½ä½“]
        Network[ç¥ç»ç½‘ç»œ]
        Policy[Îµ-è´ªå¿ƒç­–ç•¥]
    end
    
    subgraph å­¦ä¹ å±‚
        Buffer[ç»éªŒå›æ”¾æ± ]
        Trainer[è®­ç»ƒå™¨]
    end
    
    subgraph è¯„ä¼°å±‚
        Evaluator[æ€§èƒ½è¯„ä¼°]
        Monitor[è®­ç»ƒç›‘æ§]
        Visualizer[ç»“æœå¯è§†åŒ–]
    end
    
    Env <-->|äº¤äº’| Agent
    Agent --> Network
    Agent --> Policy
    Agent --> Buffer
    Buffer --> Trainer
    Trainer --> Network
    Trainer --> Monitor
    Monitor --> Evaluator
    Evaluator --> Visualizer
    
    style Env fill:#e3f2fd
    style Agent fill:#f3e5f5
    style Buffer fill:#e8f5e9
    style Trainer fill:#fff3e0
    style Evaluator fill:#ffe0b2
```

### æ ¸å¿ƒæ¨¡å—è®¾è®¡

```mermaid
graph LR
    subgraph æ ¸å¿ƒæ¨¡å—
        M1[ç¯å¢ƒæ¥å£<br/>Environment]
        M2[ç½‘ç»œæ¶æ„<br/>DQN Network]
        M3[ç»éªŒæ± <br/>Replay Buffer]
        M4[è®­ç»ƒå¼•æ“<br/>Trainer]
        M5[è¯„ä¼°ç³»ç»Ÿ<br/>Evaluator]
    end
    
    M1 -.å®šä¹‰.-> I1[reset, step<br/>render, close]
    M2 -.å®šä¹‰.-> I2[forward, train<br/>save, load]
    M3 -.å®šä¹‰.-> I3[store, sample<br/>size, clear]
    M4 -.å®šä¹‰.-> I4[train_step<br/>train_episode<br/>save_checkpoint]
    M5 -.å®šä¹‰.-> I5[evaluate<br/>plot_results<br/>save_metrics]
```

## 10.6.3 ç¯å¢ƒå°è£…

### CartPoleç¯å¢ƒæ¥å£

```java
/**
 * CartPoleç¯å¢ƒ(ç®€åŒ–ç‰ˆ)
 */
public class CartPoleEnvironment {
    // ç‰©ç†å‚æ•°
    private double gravity = 9.8;
    private double cartMass = 1.0;
    private double poleMass = 0.1;
    private double poleLength = 0.5;
    private double forceMag = 10.0;
    private double tau = 0.02; // æ—¶é—´æ­¥é•¿
    
    // çŠ¶æ€å˜é‡
    private double x;      // å°è½¦ä½ç½®
    private double xDot;   // å°è½¦é€Ÿåº¦
    private double theta;  // æ†å­è§’åº¦
    private double thetaDot; // æ†å­è§’é€Ÿåº¦
    
    // è¾¹ç•Œ
    private double xThreshold = 2.4;
    private double thetaThreshold = Math.toRadians(12);
    
    /**
     * é‡ç½®ç¯å¢ƒ
     */
    public double[] reset() {
        // éšæœºåˆå§‹åŒ–çŠ¶æ€(å°èŒƒå›´)
        x = randomBetween(-0.05, 0.05);
        xDot = randomBetween(-0.05, 0.05);
        theta = randomBetween(-0.05, 0.05);
        thetaDot = randomBetween(-0.05, 0.05);
        
        return getState();
    }
    
    /**
     * æ‰§è¡ŒåŠ¨ä½œ
     */
    public StepResult step(int action) {
        // 1. è®¡ç®—ä½œç”¨åŠ›
        double force = (action == 1) ? forceMag : -forceMag;
        
        // 2. ç‰©ç†ä»¿çœŸ(ç®€åŒ–ç‰ˆ)
        double cosTheta = Math.cos(theta);
        double sinTheta = Math.sin(theta);
        
        double temp = (force + poleMass * poleLength * thetaDot * thetaDot * sinTheta) 
                     / (cartMass + poleMass);
        
        double thetaAcc = (gravity * sinTheta - cosTheta * temp) 
                        / (poleLength * (4.0/3.0 - poleMass * cosTheta * cosTheta 
                           / (cartMass + poleMass)));
        
        double xAcc = temp - poleMass * poleLength * thetaAcc * cosTheta 
                     / (cartMass + poleMass);
        
        // 3. æ›´æ–°çŠ¶æ€
        x += tau * xDot;
        xDot += tau * xAcc;
        theta += tau * thetaDot;
        thetaDot += tau * thetaAcc;
        
        // 4. æ£€æŸ¥æ˜¯å¦ç»“æŸ
        boolean done = Math.abs(x) > xThreshold || 
                      Math.abs(theta) > thetaThreshold;
        
        // 5. è®¡ç®—å¥–åŠ±
        double reward = done ? 0.0 : 1.0;
        
        return new StepResult(getState(), reward, done);
    }
    
    /**
     * è·å–å½“å‰çŠ¶æ€
     */
    private double[] getState() {
        return new double[]{x, xDot, theta, thetaDot};
    }
}

/**
 * æ­¥è¿›ç»“æœ
 */
class StepResult {
    double[] state;
    double reward;
    boolean done;
    
    public StepResult(double[] state, double reward, boolean done) {
        this.state = state;
        this.reward = reward;
        this.done = done;
    }
}
```

## 10.6.4 DQNæ™ºèƒ½ä½“å®ç°

### ç½‘ç»œæ¶æ„

```java
/**
 * CartPole DQNç½‘ç»œ
 * è¾“å…¥: 4ç»´çŠ¶æ€ â†’ è¾“å‡º: 2ä¸ªåŠ¨ä½œçš„Qå€¼
 */
public class CartPoleDQN {
    private int stateSize = 4;
    private int actionSize = 2;
    private int[] hiddenSizes = {24, 24}; // ä¸¤å±‚éšè—å±‚
    
    private NeuralNetwork network;
    
    public CartPoleDQN() {
        buildNetwork();
    }
    
    /**
     * æ„å»ºç½‘ç»œ
     */
    private void buildNetwork() {
        network = new NeuralNetwork();
        
        // è¾“å…¥å±‚ â†’ éšè—å±‚1 (24ç¥ç»å…ƒ, ReLUæ¿€æ´»)
        network.addLayer(new DenseLayer(stateSize, hiddenSizes[0], "relu"));
        
        // éšè—å±‚1 â†’ éšè—å±‚2 (24ç¥ç»å…ƒ, ReLUæ¿€æ´»)
        network.addLayer(new DenseLayer(hiddenSizes[0], hiddenSizes[1], "relu"));
        
        // éšè—å±‚2 â†’ è¾“å‡ºå±‚ (2ä¸ªQå€¼, çº¿æ€§æ¿€æ´»)
        network.addLayer(new DenseLayer(hiddenSizes[1], actionSize, "linear"));
    }
    
    /**
     * é¢„æµ‹Qå€¼
     */
    public double[] predict(double[] state) {
        return network.forward(state);
    }
    
    /**
     * è·å–æœ€ä¼˜åŠ¨ä½œ
     */
    public int getBestAction(double[] state) {
        double[] qValues = predict(state);
        return qValues[0] > qValues[1] ? 0 : 1;
    }
}
```

### å®Œæ•´DQNæ™ºèƒ½ä½“

```java
/**
 * CartPole DQNæ™ºèƒ½ä½“
 */
public class CartPoleAgent {
    private CartPoleDQN mainNetwork;
    private CartPoleDQN targetNetwork;
    private ReplayBuffer replayBuffer;
    
    // è¶…å‚æ•°
    private double gamma = 0.99;         // æŠ˜æ‰£å› å­
    private double learningRate = 0.001; // å­¦ä¹ ç‡
    private double epsilon = 1.0;        // æ¢ç´¢ç‡
    private double epsilonMin = 0.01;
    private double epsilonDecay = 0.995;
    private int batchSize = 32;
    private int targetUpdateFreq = 10;   // æ¯10å›åˆæ›´æ–°ç›®æ ‡ç½‘ç»œ
    
    private int trainCounter = 0;
    
    public CartPoleAgent() {
        mainNetwork = new CartPoleDQN();
        targetNetwork = new CartPoleDQN();
        replayBuffer = new ReplayBuffer(10000);
        
        // åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
        targetNetwork.copyWeightsFrom(mainNetwork);
    }
    
    /**
     * é€‰æ‹©åŠ¨ä½œ(Îµ-è´ªå¿ƒ)
     */
    public int selectAction(double[] state) {
        // Îµæ¦‚ç‡éšæœºæ¢ç´¢
        if (Math.random() < epsilon) {
            return (int)(Math.random() * 2);
        }
        // 1-Îµæ¦‚ç‡è´ªå¿ƒåˆ©ç”¨
        return mainNetwork.getBestAction(state);
    }
    
    /**
     * è®°å¿†ç»éªŒ
     */
    public void remember(double[] state, int action, double reward, 
                        double[] nextState, boolean done) {
        replayBuffer.store(state, action, reward, nextState, done);
    }
    
    /**
     * è®­ç»ƒ(ç»éªŒå›æ”¾)
     */
    public void train() {
        // ç»éªŒä¸è¶³,ä¸è®­ç»ƒ
        if (replayBuffer.size() < batchSize) {
            return;
        }
        
        // é‡‡æ ·ä¸€æ‰¹ç»éªŒ
        List<Experience> batch = replayBuffer.sample(batchSize);
        
        // å¯¹æ¯ä¸ªç»éªŒè¿›è¡Œè®­ç»ƒ
        for (Experience exp : batch) {
            // è®¡ç®—ç›®æ ‡Qå€¼
            double targetQ;
            if (exp.done) {
                targetQ = exp.reward;
            } else {
                double[] nextQValues = targetNetwork.predict(exp.nextState);
                double maxNextQ = Math.max(nextQValues[0], nextQValues[1]);
                targetQ = exp.reward + gamma * maxNextQ;
            }
            
            // è·å–å½“å‰Qå€¼
            double[] currentQValues = mainNetwork.predict(exp.state);
            
            // æ›´æ–°å¯¹åº”åŠ¨ä½œçš„Qå€¼
            double[] targetQValues = currentQValues.clone();
            targetQValues[exp.action] = targetQ;
            
            // è®­ç»ƒç½‘ç»œ
            mainNetwork.fit(exp.state, targetQValues, learningRate);
        }
        
        // è¡°å‡æ¢ç´¢ç‡
        if (epsilon > epsilonMin) {
            epsilon *= epsilonDecay;
        }
    }
    
    /**
     * æ›´æ–°ç›®æ ‡ç½‘ç»œ
     */
    public void updateTargetNetwork() {
        targetNetwork.copyWeightsFrom(mainNetwork);
    }
}
```

## 10.6.5 è®­ç»ƒæµç¨‹

### è®­ç»ƒä¸»å¾ªç¯

```java
/**
 * CartPoleè®­ç»ƒå™¨
 */
public class CartPoleTrainer {
    private CartPoleEnvironment env;
    private CartPoleAgent agent;
    private List<Double> scores;     // è®°å½•æ¯å›åˆå¾—åˆ†
    private List<Double> avgScores;  // è®°å½•å¹³å‡å¾—åˆ†
    
    public CartPoleTrainer() {
        env = new CartPoleEnvironment();
        agent = new CartPoleAgent();
        scores = new ArrayList<>();
        avgScores = new ArrayList<>();
    }
    
    /**
     * è®­ç»ƒæ™ºèƒ½ä½“
     */
    public void train(int numEpisodes) {
        System.out.println("å¼€å§‹è®­ç»ƒCartPoleæ™ºèƒ½ä½“...");
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            // 1. é‡ç½®ç¯å¢ƒ
            double[] state = env.reset();
            double totalReward = 0;
            int steps = 0;
            
            // 2. ç©ä¸€å›åˆ
            while (steps < 500) { // æœ€å¤š500æ­¥
                // é€‰æ‹©åŠ¨ä½œ
                int action = agent.selectAction(state);
                
                // æ‰§è¡ŒåŠ¨ä½œ
                StepResult result = env.step(action);
                
                // è®°å¿†ç»éªŒ
                agent.remember(state, action, result.reward, 
                             result.state, result.done);
                
                // è®­ç»ƒ
                agent.train();
                
                // æ›´æ–°çŠ¶æ€
                state = result.state;
                totalReward += result.reward;
                steps++;
                
                // å›åˆç»“æŸ
                if (result.done) {
                    break;
                }
            }
            
            // 3. è®°å½•å¾—åˆ†
            scores.add(totalReward);
            
            // 4. è®¡ç®—æœ€è¿‘100å›åˆå¹³å‡åˆ†
            double avgScore = calculateAverage(scores, 100);
            avgScores.add(avgScore);
            
            // 5. å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
            if (episode % 10 == 0) {
                agent.updateTargetNetwork();
            }
            
            // 6. æ‰“å°è¿›åº¦
            if (episode % 50 == 0) {
                System.out.printf("Episode: %d, Score: %.0f, Avg: %.2f, Îµ: %.3f%n",
                    episode, totalReward, avgScore, agent.getEpsilon());
            }
            
            // 7. æ£€æŸ¥æ˜¯å¦è¾¾æˆç›®æ ‡
            if (avgScore >= 195.0) {
                System.out.printf("ç¯å¢ƒå·²è§£å†³! Episode: %d, Avg Score: %.2f%n",
                    episode, avgScore);
                break;
            }
        }
        
        System.out.println("è®­ç»ƒå®Œæˆ!");
    }
    
    /**
     * è®¡ç®—æœ€è¿‘Nå›åˆçš„å¹³å‡åˆ†
     */
    private double calculateAverage(List<Double> list, int n) {
        int start = Math.max(0, list.size() - n);
        double sum = 0;
        for (int i = start; i < list.size(); i++) {
            sum += list.get(i);
        }
        return sum / (list.size() - start);
    }
}
```

### è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–

```mermaid
graph LR
    P1[åˆæœŸ<br/>éšæœºæ¢ç´¢] --> P2[å­¦ä¹ é˜¶æ®µ<br/>æ€§èƒ½æå‡]
    P2 --> P3[ç¨³å®šé˜¶æ®µ<br/>æ¥è¿‘ç›®æ ‡]
    P3 --> P4[è¾¾æˆç›®æ ‡<br/>å¹³å‡åˆ†>195]
    
    P1 -.å›åˆ0-100<br/>å¹³å‡åˆ†: 20.-> P1
    P2 -.å›åˆ100-300<br/>å¹³å‡åˆ†: 100.-> P2
    P3 -.å›åˆ300-500<br/>å¹³å‡åˆ†: 180.-> P3
    P4 -.å›åˆ500+<br/>å¹³å‡åˆ†: 200.-> P4
    
    style P1 fill:#ffccbc
    style P2 fill:#ffe0b2
    style P3 fill:#fff9c4
    style P4 fill:#c8e6c9
```

## 10.6.6 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### è¶…å‚æ•°è°ƒä¼˜å»ºè®®

```mermaid
graph TB
    Tuning[è¶…å‚æ•°è°ƒä¼˜]
    
    Tuning --> T1[ç½‘ç»œç»“æ„<br/>24-24æ•ˆæœå¥½]
    Tuning --> T2[å­¦ä¹ ç‡<br/>0.001-0.01]
    Tuning --> T3[æ‰¹æ¬¡å¤§å°<br/>32-64]
    Tuning --> T4[Îµè¡°å‡<br/>0.995-0.999]
    
    T1 --> R1[å¤ªå¤§: è¿‡æ‹Ÿåˆ<br/>å¤ªå°: æ¬ æ‹Ÿåˆ]
    T2 --> R2[å¤ªå¤§: ä¸ç¨³å®š<br/>å¤ªå°: å­¦ä¹ æ…¢]
    T3 --> R3[å¤ªå¤§: æ…¢<br/>å¤ªå°: ä¸ç¨³å®š]
    T4 --> R4[å¤ªå¿«: å±€éƒ¨æœ€ä¼˜<br/>å¤ªæ…¢: æ•ˆç‡ä½]
    
    style Tuning fill:#e1bee7
    style T1 fill:#bbdefb
    style T2 fill:#c5e1a5
    style T3 fill:#ffe0b2
    style T4 fill:#f8bbd0
```

### è®­ç»ƒåŠ é€ŸæŠ€å·§

| æŠ€å·§ | æ•ˆæœ | å®ç°éš¾åº¦ |
|------|------|----------|
| ä½¿ç”¨Double DQN | â†‘20% | â­ |
| å¢å¤§ç»éªŒæ±  | â†‘10% | â­ |
| è°ƒæ•´ç½‘ç»œç»“æ„ | â†‘15% | â­â­ |
| å¥–åŠ±å½’ä¸€åŒ– | â†‘5% | â­ |
| æ¢¯åº¦è£å‰ª | â†‘ç¨³å®šæ€§ | â­ |

### å¸¸è§é—®é¢˜ä¸è§£å†³

```mermaid
graph TB
    Problems[è®­ç»ƒé—®é¢˜]
    
    Problems --> P1[ä¸æ”¶æ•›]
    Problems --> P2[éœ‡è¡ä¸¥é‡]
    Problems --> P3[è¿‡æ‹Ÿåˆ]
    
    P1 --> S1[æ£€æŸ¥å­¦ä¹ ç‡<br/>å°è¯•é™ä½]
    P2 --> S2[å¢å¤§æ‰¹æ¬¡<br/>ç¨³å®šæ¢¯åº¦]
    P3 --> S3[å¢å¤§ç»éªŒæ± <br/>å¢åŠ æ­£åˆ™åŒ–]
    
    style Problems fill:#ffebee
    style P1 fill:#ffccbc
    style P2 fill:#ffe0b2
    style P3 fill:#fff9c4
    style S1 fill:#c8e6c9
    style S2 fill:#c8e6c9
    style S3 fill:#c8e6c9
```

## 10.6.7 è¯„ä¼°ä¸å¯è§†åŒ–

### æ€§èƒ½è¯„ä¼°

```java
/**
 * è¯„ä¼°å™¨
 */
public class Evaluator {
    
    /**
     * è¯„ä¼°è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
     */
    public EvaluationResult evaluate(CartPoleAgent agent, 
                                     CartPoleEnvironment env, 
                                     int numEpisodes) {
        List<Double> scores = new ArrayList<>();
        
        // å…³é—­æ¢ç´¢,ä½¿ç”¨çº¯è´ªå¿ƒç­–ç•¥
        double oldEpsilon = agent.getEpsilon();
        agent.setEpsilon(0.0);
        
        for (int i = 0; i < numEpisodes; i++) {
            double[] state = env.reset();
            double totalReward = 0;
            int steps = 0;
            
            while (steps < 500) {
                int action = agent.selectAction(state);
                StepResult result = env.step(action);
                
                state = result.state;
                totalReward += result.reward;
                steps++;
                
                if (result.done) break;
            }
            
            scores.add(totalReward);
        }
        
        // æ¢å¤æ¢ç´¢ç‡
        agent.setEpsilon(oldEpsilon);
        
        return new EvaluationResult(scores);
    }
}

/**
 * è¯„ä¼°ç»“æœ
 */
class EvaluationResult {
    private List<Double> scores;
    
    public double getAverageScore() {
        return scores.stream().mapToDouble(Double::doubleValue).average().orElse(0.0);
    }
    
    public double getMaxScore() {
        return scores.stream().mapToDouble(Double::doubleValue).max().orElse(0.0);
    }
    
    public double getMinScore() {
        return scores.stream().mapToDouble(Double::doubleValue).min().orElse(0.0);
    }
    
    public double getSuccessRate() {
        long successCount = scores.stream().filter(s -> s >= 195.0).count();
        return (double)successCount / scores.size();
    }
}
```

### è®­ç»ƒæ›²çº¿ç»˜åˆ¶

```java
/**
 * ç»˜åˆ¶è®­ç»ƒæ›²çº¿
 */
public void plotTrainingCurve(List<Double> scores, List<Double> avgScores) {
    // ä½¿ç”¨JFreeChartæˆ–å…¶ä»–å›¾è¡¨åº“ç»˜åˆ¶
    // Xè½´: å›åˆæ•°
    // Yè½´: å¾—åˆ†
    // ä¸¤æ¡çº¿: å•å›åˆå¾—åˆ† + å¹³å‡å¾—åˆ†
}
```

è®­ç»ƒæ›²çº¿ç¤ºæ„:
```
åˆ†æ•°
  |
500|                           ___________
  |                      ____/
400|                 ____/
  |            ____/
300|       ____/
  |   ___/
200|__/________________ç›®æ ‡çº¿(195åˆ†)_____
  |  /
100| /
  |/
  0|_________________________________å›åˆ
    0   100  200  300  400  500
    
    â”€â”€â”€ å•å›åˆå¾—åˆ†(æ³¢åŠ¨å¤§)
    â”€â”€â”€ å¹³å‡å¾—åˆ†(å¹³æ»‘)
```

## 10.6.8 æ¨¡å‹ä¿å­˜ä¸éƒ¨ç½²

### æ¨¡å‹ä¿å­˜

```java
/**
 * ä¿å­˜æ¨¡å‹
 */
public void saveModel(CartPoleAgent agent, String filepath) {
    // ä¿å­˜ç½‘ç»œæƒé‡
    agent.getMainNetwork().save(filepath + "_main.model");
    agent.getTargetNetwork().save(filepath + "_target.model");
    
    // ä¿å­˜è¶…å‚æ•°
    saveHyperparameters(agent, filepath + "_params.json");
    
    System.out.println("æ¨¡å‹å·²ä¿å­˜åˆ°: " + filepath);
}

/**
 * åŠ è½½æ¨¡å‹
 */
public CartPoleAgent loadModel(String filepath) {
    CartPoleAgent agent = new CartPoleAgent();
    
    // åŠ è½½æƒé‡
    agent.getMainNetwork().load(filepath + "_main.model");
    agent.getTargetNetwork().load(filepath + "_target.model");
    
    // åŠ è½½è¶…å‚æ•°
    loadHyperparameters(agent, filepath + "_params.json");
    
    System.out.println("æ¨¡å‹å·²åŠ è½½ä»: " + filepath);
    return agent;
}
```

### æ™ºèƒ½ä½“æ¼”ç¤º

```java
/**
 * æ¼”ç¤ºè®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
 */
public void demo(CartPoleAgent agent, CartPoleEnvironment env) {
    agent.setEpsilon(0.0); // å…³é—­æ¢ç´¢
    
    double[] state = env.reset();
    int steps = 0;
    
    System.out.println("å¼€å§‹æ¼”ç¤º...");
    
    while (steps < 500) {
        // æ˜¾ç¤ºå½“å‰çŠ¶æ€
        env.render();
        
        // é€‰æ‹©å¹¶æ‰§è¡ŒåŠ¨ä½œ
        int action = agent.selectAction(state);
        StepResult result = env.step(action);
        
        state = result.state;
        steps++;
        
        // å»¶è¿Ÿä¸€ä¸‹,ä¾¿äºè§‚å¯Ÿ
        sleep(20); // 20æ¯«ç§’
        
        if (result.done) {
            System.out.printf("å›åˆç»“æŸ,åšæŒäº† %d æ­¥%n", steps);
            break;
        }
    }
    
    if (steps >= 500) {
        System.out.println("å®Œç¾! è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶!");
    }
}
```

## 10.6.9 å®Œæ•´é¡¹ç›®ä»£ç ç»“æ„

```
cartpole-dqn/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â””â”€â”€ CartPoleEnvironment.java    # ç¯å¢ƒå®ç°
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ CartPoleDQN.java           # ç½‘ç»œæ¶æ„
â”‚   â”‚   â”œâ”€â”€ CartPoleAgent.java         # æ™ºèƒ½ä½“
â”‚   â”‚   â””â”€â”€ ReplayBuffer.java          # ç»éªŒå›æ”¾
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ CartPoleTrainer.java       # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ Evaluator.java             # è¯„ä¼°å™¨
â”‚   â””â”€â”€ Main.java                      # ä¸»ç¨‹åº
â”œâ”€â”€ models/                            # ä¿å­˜çš„æ¨¡å‹
â”œâ”€â”€ logs/                              # è®­ç»ƒæ—¥å¿—
â””â”€â”€ README.md                          # é¡¹ç›®è¯´æ˜
```

### ä¸»ç¨‹åºç¤ºä¾‹

```java
/**
 * CartPole DQNé¡¹ç›®ä¸»ç¨‹åº
 */
public class Main {
    public static void main(String[] args) {
        System.out.println("=== CartPole DQNé¡¹ç›® ===\n");
        
        // 1. åˆ›å»ºè®­ç»ƒå™¨
        CartPoleTrainer trainer = new CartPoleTrainer();
        
        // 2. è®­ç»ƒæ™ºèƒ½ä½“
        System.out.println("æ­¥éª¤1: è®­ç»ƒæ™ºèƒ½ä½“");
        trainer.train(1000);
        
        // 3. è¯„ä¼°æ€§èƒ½
        System.out.println("\næ­¥éª¤2: è¯„ä¼°æ€§èƒ½");
        Evaluator evaluator = new Evaluator();
        EvaluationResult result = evaluator.evaluate(
            trainer.getAgent(), 
            trainer.getEnvironment(), 
            100
        );
        
        System.out.printf("è¯„ä¼°ç»“æœ (100å›åˆ):%n");
        System.out.printf("  å¹³å‡åˆ†: %.2f%n", result.getAverageScore());
        System.out.printf("  æœ€é«˜åˆ†: %.0f%n", result.getMaxScore());
        System.out.printf("  æœ€ä½åˆ†: %.0f%n", result.getMinScore());
        System.out.printf("  æˆåŠŸç‡: %.1f%%%n", result.getSuccessRate() * 100);
        
        // 4. ä¿å­˜æ¨¡å‹
        System.out.println("\næ­¥éª¤3: ä¿å­˜æ¨¡å‹");
        ModelManager.saveModel(trainer.getAgent(), "models/cartpole_dqn");
        
        // 5. æ¼”ç¤º
        System.out.println("\næ­¥éª¤4: æ¼”ç¤ºæ™ºèƒ½ä½“");
        trainer.demo();
        
        System.out.println("\né¡¹ç›®å®Œæˆ!");
    }
}
```

## æœ¬èŠ‚å°ç»“

### é¡¹ç›®å®Œæ•´æµç¨‹

```mermaid
graph TB
    Start[é¡¹ç›®å¼€å§‹] --> Design[1. ç³»ç»Ÿè®¾è®¡<br/>æ¶æ„è§„åˆ’]
    Design --> Impl[2. æ¨¡å—å®ç°<br/>ç¯å¢ƒ+æ™ºèƒ½ä½“+è®­ç»ƒ]
    Impl --> Train[3. è®­ç»ƒæ™ºèƒ½ä½“<br/>è°ƒå‚ä¼˜åŒ–]
    Train --> Eval[4. æ€§èƒ½è¯„ä¼°<br/>æµ‹è¯•éªŒè¯]
    Eval --> Deploy[5. æ¨¡å‹éƒ¨ç½²<br/>ä¿å­˜æ¼”ç¤º]
    Deploy --> End[é¡¹ç›®å®Œæˆ]
    
    style Start fill:#e8f5e9
    style Design fill:#e3f2fd
    style Impl fill:#f3e5f5
    style Train fill:#fff3e0
    style Eval fill:#ffe0b2
    style Deploy fill:#ffd54f
    style End fill:#c8e6c9
```

### æ ¸å¿ƒè¦ç‚¹

1. **ç¯å¢ƒç†è§£**: CartPoleæ˜¯ç»å…¸çš„å¼ºåŒ–å­¦ä¹ æµ‹è¯•ç¯å¢ƒ
2. **ç³»ç»Ÿæ¶æ„**: æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†,ä¾¿äºå¼€å‘å’Œç»´æŠ¤
3. **DQNå®ç°**: å®Œæ•´çš„ç½‘ç»œ+ç»éªŒå›æ”¾+ç›®æ ‡ç½‘ç»œ
4. **è®­ç»ƒç­–ç•¥**: Îµ-è´ªå¿ƒæ¢ç´¢,å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
5. **æ€§èƒ½ä¼˜åŒ–**: è¶…å‚æ•°è°ƒä¼˜,è®­ç»ƒæŠ€å·§åº”ç”¨
6. **è¯„ä¼°éƒ¨ç½²**: å…¨é¢è¯„ä¼°,æ¨¡å‹ä¿å­˜,å¯è§†åŒ–æ¼”ç¤º

### å…³é”®æŠ€æœ¯æ€»ç»“

| æŠ€æœ¯ | ä½œç”¨ | é‡è¦æ€§ |
|------|------|--------|
| ç¥ç»ç½‘ç»œ | Qå€¼è¿‘ä¼¼ | â­â­â­â­â­ |
| ç»éªŒå›æ”¾ | æå‡æ ·æœ¬æ•ˆç‡ | â­â­â­â­â­ |
| ç›®æ ‡ç½‘ç»œ | ç¨³å®šè®­ç»ƒ | â­â­â­â­ |
| Îµ-è´ªå¿ƒ | æ¢ç´¢åˆ©ç”¨å¹³è¡¡ | â­â­â­â­ |
| è¶…å‚æ•°è°ƒä¼˜ | æ€§èƒ½ä¼˜åŒ– | â­â­â­ |

### é¡¹ç›®æˆæœ

âœ… **ç†è®ºä¸å®è·µç»“åˆ**: å°†DQNç†è®ºä»˜è¯¸å®è·µ  
âœ… **å®Œæ•´é¡¹ç›®æµç¨‹**: ä»è®¾è®¡åˆ°éƒ¨ç½²å…¨æµç¨‹  
âœ… **å¯æ‰©å±•æ¶æ„**: å¯è½»æ¾åº”ç”¨åˆ°å…¶ä»–ç¯å¢ƒ  
âœ… **æ€§èƒ½è¾¾æ ‡**: å¹³å‡åˆ†>195,æˆåŠŸç‡>90%

### ä¸‹ä¸€æ­¥æ–¹å‘

ğŸš€ **å°è¯•å…¶ä»–ç¯å¢ƒ**: LunarLander, MountainCar  
ğŸš€ **ç®—æ³•æ”¹è¿›**: å®ç°Double DQN, Dueling DQN  
ğŸš€ **é«˜çº§æŠ€æœ¯**: å°è¯•A3C, PPOç­‰ç®—æ³•  
ğŸš€ **å®é™…åº”ç”¨**: å°†RLåº”ç”¨åˆ°å®é™…é—®é¢˜

### ç”Ÿæ´»å¯ç¤º

è¿™ä¸ªé¡¹ç›®å‘Šè¯‰æˆ‘ä»¬:
- **ç³»ç»Ÿæ€ç»´**: å¤æ‚é—®é¢˜éœ€è¦æ¨¡å—åŒ–åˆ†è§£
- **è¿­ä»£ä¼˜åŒ–**: ä»ç®€å•ç‰ˆæœ¬é€æ­¥æ”¹è¿›
- **æ•°æ®é©±åŠ¨**: ç”¨æ•°æ®æŒ‡å¯¼ä¼˜åŒ–æ–¹å‘
- **æŒç»­å­¦ä¹ **: AIåœ¨ä¸æ–­è¯•é”™ä¸­æˆé•¿,æˆ‘ä»¬ä¹Ÿä¸€æ ·

---

## ğŸ‰ æ­å–œå®Œæˆç¬¬10ç« !

ä½ å·²ç»æŒæ¡äº†å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒçŸ¥è¯†:
- âœ… MDPæ•°å­¦åŸºç¡€
- âœ… ä»·å€¼å‡½æ•°ä¸ç­–ç•¥æ¢¯åº¦  
- âœ… Q-Learningç®—æ³•
- âœ… DQNåŠå…¶å˜ä½“
- âœ… å®Œæ•´é¡¹ç›®å®æˆ˜

**ä½ ç°åœ¨å¯ä»¥**:
- æ„å»ºå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
- è®­ç»ƒAIç©æ¸¸æˆ
- åº”ç”¨RLè§£å†³å®é™…é—®é¢˜

ç»§ç»­åŠ æ²¹,ä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹ æ¨¡å‹è¯„ä¼°ä¸è¯Šæ–­æŠ€æœ¯!

---

**æ‹“å±•é˜…è¯»**:
- OpenAI Gymæ–‡æ¡£: gym.openai.com
- DQNè®ºæ–‡: "Playing Atari with Deep Reinforcement Learning"
- Rainbow DQN: "Rainbow: Combining Improvements in DRL"
