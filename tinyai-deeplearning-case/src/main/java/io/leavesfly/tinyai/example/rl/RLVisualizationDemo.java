package io.leavesfly.tinyai.example.rl;

import io.leavesfly.tinyai.func.Variable;
import io.leavesfly.tinyai.rl.Agent;
import io.leavesfly.tinyai.rl.Environment;
import io.leavesfly.tinyai.rl.Experience;
import io.leavesfly.tinyai.rl.agent.*;
import io.leavesfly.tinyai.rl.environment.*;

import java.util.*;
import java.text.DecimalFormat;

/**
 * å¼ºåŒ–å­¦ä¹ å¯è§†åŒ–æ¼”ç¤º
 * 
 * æœ¬æ¼”ç¤ºæä¾›å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹çš„å¯è§†åŒ–å±•ç¤ºï¼ŒåŒ…æ‹¬ï¼š
 * 1. å­¦ä¹ æ›²çº¿å¯è§†åŒ–
 * 2. ç­–ç•¥çƒ­åŠ›å›¾å±•ç¤º
 * 3. Qå€¼åˆ†å¸ƒå¯è§†åŒ–
 * 4. æ¢ç´¢è·¯å¾„è¿½è¸ª
 * 5. å®æ—¶è®­ç»ƒç›‘æ§
 * 6. æ€§èƒ½æŒ‡æ ‡dashboard
 * 
 * @author å±±æ³½
 */
public class RLVisualizationDemo {
    
    private static final DecimalFormat df2 = new DecimalFormat("#.##");
    private static final DecimalFormat df4 = new DecimalFormat("#.####");
    
    public static void main(String[] args) {
        RLVisualizationDemo demo = new RLVisualizationDemo();
        
        System.out.println("==========================================");
        System.out.println("      TinyAI å¼ºåŒ–å­¦ä¹ å¯è§†åŒ–æ¼”ç¤º           ");
        System.out.println("==========================================");
        
        demo.demonstrateLearningCurves();
        demo.demonstratePolicyHeatmap();
        demo.demonstrateQValueVisualization();
        demo.demonstrateExplorationTracking();
        demo.demonstrateRealTimeMonitoring();
        demo.demonstratePerformanceDashboard();
        
        System.out.println("\n========== å¯è§†åŒ–æ¼”ç¤ºå®Œæˆ ==========");
    }
    
    /**
     * æ¼”ç¤ºå­¦ä¹ æ›²çº¿å¯è§†åŒ–
     */
    public void demonstrateLearningCurves() {
        System.out.println("\n========== å­¦ä¹ æ›²çº¿å¯è§†åŒ– ==========");
        
        MultiArmedBanditEnvironment environment = new MultiArmedBanditEnvironment(
            new float[]{0.1f, 0.3f, 0.8f, 0.2f, 0.6f}, 1000);
        
        List<BanditAgent> agents = Arrays.asList(
            new EpsilonGreedyBanditAgent("Îµ-è´ªå¿ƒ", 5, 0.1f),
            new UCBBanditAgent("UCB", 5),
            new ThompsonSamplingBanditAgent("æ±¤æ™®æ£®é‡‡æ ·", 5)
        );
        
        System.out.println("\nå¤šç®—æ³•å­¦ä¹ æ›²çº¿å¯¹æ¯”:");
        System.out.println("æ¨ªè½´: è®­ç»ƒæ­¥æ•° | çºµè½´: ç´¯ç§¯å¹³å‡å¥–åŠ±");
        System.out.println();
        
        Map<String, List<Float>> learningCurves = new HashMap<>();
        
        for (BanditAgent agent : agents) {
            agent.reset();
            environment.reset();
            
            List<Float> rewards = new ArrayList<>();
            float cumulativeReward = 0.0f;
            
            for (int step = 0; step < 500; step++) {
                Variable action = agent.selectAction(environment.getCurrentState());
                Environment.StepResult result = environment.step(action);
                
                Experience experience = new Experience(
                    environment.getCurrentState(), action, result.getReward(),
                    result.getNextState(), result.isDone(), step
                );
                
                agent.learn(experience);
                cumulativeReward += result.getReward();
                
                if (step % 25 == 24) {
                    rewards.add(cumulativeReward / (step + 1));
                }
            }
            
            learningCurves.put(agent.getName(), rewards);
        }
        
        // ç»˜åˆ¶ASCIIå­¦ä¹ æ›²çº¿
        drawLearningCurves(learningCurves);
        
        // åˆ†æå­¦ä¹ è¶‹åŠ¿
        analyzeLearningTrends(learningCurves);
    }
    
    /**
     * æ¼”ç¤ºç­–ç•¥çƒ­åŠ›å›¾
     */
    public void demonstratePolicyHeatmap() {
        System.out.println("\n========== ç­–ç•¥çƒ­åŠ›å›¾å±•ç¤º ==========");
        
        GridWorldEnvironment environment = new GridWorldEnvironment(5, 5);
        DQNAgent agent = new DQNAgent("ç­–ç•¥æ™ºèƒ½ä½“", 2, 4, new int[]{32, 32}, 
                                    0.01f, 0.1f, 0.99f, 32, 5000, 50);
        
        // è®­ç»ƒæ™ºèƒ½ä½“
        System.out.println("è®­ç»ƒæ™ºèƒ½ä½“å­¦ä¹ æœ€ä¼˜ç­–ç•¥...");
        trainAgent(agent, environment, 200);
        
        // ç”Ÿæˆç­–ç•¥çƒ­åŠ›å›¾
        System.out.println("\nç­–ç•¥çƒ­åŠ›å›¾ (åŠ¨ä½œåå¥½):");
        System.out.println("ç¬¦å·å«ä¹‰: â†‘=ä¸Š â†“=ä¸‹ â†=å·¦ â†’=å³ X=éšœç¢");
        System.out.println();
        
        generatePolicyHeatmap(agent, 5, 5);
        
        // æ˜¾ç¤ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        showActionProbabilities(agent, 5, 5);
    }
    
    /**
     * æ¼”ç¤ºQå€¼åˆ†å¸ƒå¯è§†åŒ–
     */
    public void demonstrateQValueVisualization() {
        System.out.println("\n========== Qå€¼åˆ†å¸ƒå¯è§†åŒ– ==========");
        
        MultiArmedBanditEnvironment environment = new MultiArmedBanditEnvironment(
            new float[]{0.1f, 0.3f, 0.8f, 0.2f, 0.6f}, 1000);
        
        EpsilonGreedyBanditAgent agent = new EpsilonGreedyBanditAgent("Qå€¼æ™ºèƒ½ä½“", 5, 0.1f);
        
        // è®­ç»ƒæ™ºèƒ½ä½“
        System.out.println("è®­ç»ƒæ™ºèƒ½ä½“å­¦ä¹ Qå€¼...");
        environment.reset();
        
        for (int step = 0; step < 1000; step++) {
            Variable action = agent.selectAction(environment.getCurrentState());
            Environment.StepResult result = environment.step(action);
            
            Experience experience = new Experience(
                environment.getCurrentState(), action, result.getReward(),
                result.getNextState(), result.isDone(), step
            );
            
            agent.learn(experience);
        }
        
        // å¯è§†åŒ–Qå€¼åˆ†å¸ƒ
        System.out.println("\nQå€¼åˆ†å¸ƒå¯è§†åŒ–:");
        visualizeQValues(agent);
        
        // Qå€¼æ”¶æ•›åˆ†æ
        analyzeQValueConvergence(agent);
    }
    
    /**
     * æ¼”ç¤ºæ¢ç´¢è·¯å¾„è¿½è¸ª
     */
    public void demonstrateExplorationTracking() {
        System.out.println("\n========== æ¢ç´¢è·¯å¾„è¿½è¸ª ==========");
        
        GridWorldEnvironment environment = new GridWorldEnvironment(6, 6);
        DQNAgent agent = new DQNAgent("æ¢ç´¢æ™ºèƒ½ä½“", 2, 4, new int[]{32, 32}, 
                                    0.01f, 0.2f, 0.99f, 32, 5000, 50); // é«˜æ¢ç´¢ç‡
        
        // è¿½è¸ªæ¢ç´¢è·¯å¾„
        System.out.println("è¿½è¸ªæ™ºèƒ½ä½“çš„æ¢ç´¢è·¯å¾„:");
        trackExplorationPath(agent, environment, 3);
        
        // æ¢ç´¢çƒ­åŠ›å›¾
        System.out.println("\næ¢ç´¢çƒ­åŠ›å›¾ (è®¿é—®é¢‘ç‡):");
        generateExplorationHeatmap(agent, environment, 100);
    }
    
    /**
     * æ¼”ç¤ºå®æ—¶è®­ç»ƒç›‘æ§
     */
    public void demonstrateRealTimeMonitoring() {
        System.out.println("\n========== å®æ—¶è®­ç»ƒç›‘æ§ ==========");
        
        CartPoleEnvironment environment = new CartPoleEnvironment(500);
        DQNAgent agent = new DQNAgent("ç›‘æ§æ™ºèƒ½ä½“", 4, 2, new int[]{64, 64}, 
                                    0.001f, 0.1f, 0.99f, 32, 10000, 100);
        
        System.out.println("å®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹ (æ¯10å›åˆæ›´æ–°):");
        System.out.println("æ ¼å¼: å›åˆ | å¥–åŠ± | æ­¥æ•° | Îµå€¼ | æŸå¤± | ç¼“å†²åŒº");
        System.out.println("------|------|------|------|------|--------");
        
        realTimeTrainingMonitor(agent, environment, 100);
    }
    
    /**
     * æ¼”ç¤ºæ€§èƒ½æŒ‡æ ‡dashboard
     */
    public void demonstratePerformanceDashboard() {
        System.out.println("\n========== æ€§èƒ½æŒ‡æ ‡Dashboard ==========");
        
        // è¿è¡Œå¤šä¸ªç®—æ³•è·å–æ€§èƒ½æ•°æ®
        Map<String, PerformanceMetrics> performanceData = collectPerformanceData();
        
        // ç”Ÿæˆdashboard
        generatePerformanceDashboard(performanceData);
        
        // è¶‹åŠ¿åˆ†æ
        analyzeTrends(performanceData);
    }
    
    // ==================== å¯è§†åŒ–è¾…åŠ©æ–¹æ³• ====================
    
    /**
     * ç»˜åˆ¶ASCIIå­¦ä¹ æ›²çº¿
     */
    private void drawLearningCurves(Map<String, List<Float>> curves) {
        int width = 60;
        int height = 15;
        
        // æ‰¾åˆ°æœ€å¤§æœ€å°å€¼
        double minReward = curves.values().stream()
            .flatMap(List::stream)
            .mapToDouble(Double::valueOf)
            .min().orElse(0.0);
        
        double maxReward = curves.values().stream()
            .flatMap(List::stream)
            .mapToDouble(Double::valueOf)
            .max().orElse(1.0);
        
        System.out.println(String.format("å¥–åŠ±èŒƒå›´: [%.4f, %.4f]", minReward, maxReward));
        System.out.println();
        
        // ç»˜åˆ¶å›¾è¡¨
        char[][] chart = new char[height][width];
        for (int i = 0; i < height; i++) {
            Arrays.fill(chart[i], ' ');
        }
        
        // ç»˜åˆ¶åæ ‡è½´
        for (int i = 0; i < width; i++) {
            chart[height - 1][i] = '-';
        }
        for (int i = 0; i < height; i++) {
            chart[i][0] = '|';
        }
        chart[height - 1][0] = '+';
        
        // ç»˜åˆ¶æ›²çº¿
        char[] symbols = {'*', '#', '@'};
        int symbolIndex = 0;
        
        for (Map.Entry<String, List<Float>> entry : curves.entrySet()) {
            List<Float> rewards = entry.getValue();
            char symbol = symbols[symbolIndex % symbols.length];
            
            for (int i = 0; i < rewards.size() && i < width - 1; i++) {
                double normalized = (rewards.get(i) - minReward) / (maxReward - minReward);
                int y = height - 2 - (int) (normalized * (height - 2));
                int x = i + 1;
                
                if (y >= 0 && y < height - 1 && x < width) {
                    chart[y][x] = symbol;
                }
            }
            
            symbolIndex++;
        }
        
        // æ‰“å°å›¾è¡¨
        for (int i = 0; i < height; i++) {
            System.out.println(new String(chart[i]));
        }
        
        // æ‰“å°å›¾ä¾‹
        System.out.println("\nå›¾ä¾‹:");
        symbolIndex = 0;
        for (String name : curves.keySet()) {
            System.out.println(String.format("  %c = %s", symbols[symbolIndex % symbols.length], name));
            symbolIndex++;
        }
    }
    
    /**
     * åˆ†æå­¦ä¹ è¶‹åŠ¿
     */
    private void analyzeLearningTrends(Map<String, List<Float>> curves) {
        System.out.println("\n=== å­¦ä¹ è¶‹åŠ¿åˆ†æ ===");
        
        for (Map.Entry<String, List<Float>> entry : curves.entrySet()) {
            String name = entry.getKey();
            List<Float> rewards = entry.getValue();
            
            if (rewards.size() < 2) continue;
            
            float initialReward = rewards.get(0);
            float finalReward = rewards.get(rewards.size() - 1);
            float improvement = finalReward - initialReward;
            
            // è®¡ç®—å­¦ä¹ ç¨³å®šæ€§ï¼ˆæœ€åå‡ ä¸ªç‚¹çš„æ–¹å·®ï¼‰
            int tailSize = Math.min(5, rewards.size());
            List<Float> tail = rewards.subList(rewards.size() - tailSize, rewards.size());
            double mean = tail.stream().mapToDouble(Double::valueOf).average().orElse(0.0);
            double variance = tail.stream()
                .mapToDouble(r -> Math.pow(r - mean, 2))
                .average().orElse(0.0);
            
            System.out.println(String.format("%s:", name));
            System.out.println(String.format("  æ”¹è¿›å¹…åº¦: %.4f (ä» %.4f åˆ° %.4f)", improvement, initialReward, finalReward));
            System.out.println(String.format("  å­¦ä¹ ç¨³å®šæ€§: %.6f (æ–¹å·®ï¼Œè¶Šå°è¶Šç¨³å®š)", variance));
            
            if (improvement > 0.01) {
                System.out.println("  è¶‹åŠ¿: ä¸Šå‡ â†—");
            } else if (improvement < -0.01) {
                System.out.println("  è¶‹åŠ¿: ä¸‹é™ â†˜");
            } else {
                System.out.println("  è¶‹åŠ¿: ç¨³å®š â†’");
            }
        }
    }
    
    /**
     * ç”Ÿæˆç­–ç•¥çƒ­åŠ›å›¾
     */
    private void generatePolicyHeatmap(DQNAgent agent, int width, int height) {
        String[] actionSymbols = {"â†‘", "â†“", "â†", "â†’"};
        
        for (int y = 0; y < height; y++) {
            for (int x = 0; x < width; x++) {
                // åˆ›å»ºä½ç½®çŠ¶æ€
                Variable state = new Variable(io.leavesfly.tinyai.ndarr.NdArray.of(new float[]{x, y}));
                
                // è·å–åŠ¨ä½œï¼ˆè´ªå©ªé€‰æ‹©ï¼‰  
                agent.setTraining(false);
                Variable action = agent.selectAction(state);
                int actionIndex = (int) action.getValue().getNumber().floatValue();
                
                if (actionIndex >= 0 && actionIndex < actionSymbols.length) {
                    System.out.print(actionSymbols[actionIndex] + " ");
                } else {
                    System.out.print("? ");
                }
            }
            System.out.println();
        }
        
        agent.setTraining(true);
    }
    
    /**
     * æ˜¾ç¤ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
     */
    private void showActionProbabilities(DQNAgent agent, int width, int height) {
        System.out.println("\n=== ä¸­å¿ƒä½ç½®åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ ===");
        
        int centerX = width / 2;
        int centerY = height / 2;
        
        Variable centerState = new Variable(
            io.leavesfly.tinyai.ndarr.NdArray.of(new float[]{centerX, centerY}));
        
        // æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–æ˜¾ç¤ºï¼Œå®é™…éœ€è¦ä»ç½‘ç»œè¾“å‡ºè·å–æ¦‚ç‡åˆ†å¸ƒ
        String[] actionNames = {"ä¸Š", "ä¸‹", "å·¦", "å³"};
        float[] probabilities = {0.1f, 0.2f, 0.25f, 0.45f}; // ç¤ºä¾‹æ¦‚ç‡
        
        System.out.println(String.format("ä½ç½® (%d, %d) çš„åŠ¨ä½œæ¦‚ç‡:", centerX, centerY));
        for (int i = 0; i < actionNames.length; i++) {
            String bar = "â–ˆ".repeat((int) (probabilities[i] * 20));
            System.out.println(String.format("  %s: %.3f %s", actionNames[i], probabilities[i], bar));
        }
    }
    
    /**
     * å¯è§†åŒ–Qå€¼åˆ†å¸ƒ
     */
    private void visualizeQValues(EpsilonGreedyBanditAgent agent) {
        float[] qValues = agent.getAllEstimatedRewards();
        
        System.out.println("è‡‚ç¼–å· | Qå€¼ä¼°è®¡ | é€‰æ‹©æ¬¡æ•° | å¯è§†åŒ–æ¡å½¢å›¾");
        System.out.println("-------|---------|---------|----------------");
        
        for (int i = 0; i < qValues.length; i++) {
            int count = agent.getActionCount(i);
            float qValue = qValues[i];
            
            // ç”Ÿæˆæ¡å½¢å›¾
            int barLength = Math.max(0, (int) (qValue * 20));
            String bar = "â–ˆ".repeat(barLength);
            
            System.out.println(String.format("   %d   | %7s | %8d | %s", 
                             i, df4.format(qValue), count, bar));
        }
        
        // æ˜¾ç¤ºæœ€ä¼˜è‡‚
        int bestArm = agent.getBestArmIndex();
        System.out.println(String.format("\næœ€ä¼˜è‡‚: %d (Qå€¼: %.4f)", bestArm, qValues[bestArm]));
    }
    
    /**
     * åˆ†æQå€¼æ”¶æ•›
     */
    private void analyzeQValueConvergence(EpsilonGreedyBanditAgent agent) {
        System.out.println("\n=== Qå€¼æ”¶æ•›åˆ†æ ===");
        
        float[] qValues = agent.getAllEstimatedRewards();
        int[] counts = agent.getAllActionCounts();
        
        // è®¡ç®—ç½®ä¿¡åŒºé—´ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for (int i = 0; i < qValues.length; i++) {
            if (counts[i] > 0) {
                double confidence = 1.96 / Math.sqrt(counts[i]); // 95%ç½®ä¿¡åŒºé—´
                System.out.println(String.format("è‡‚ %d: Qå€¼ %.4f Â± %.4f (ç½®ä¿¡åŒºé—´)", 
                                 i, qValues[i], confidence));
            }
        }
        
        // æ”¶æ•›è¯„ä¼°
        float maxQ = qValues[0];
        float minQ = qValues[0];
        for (float q : qValues) {
            if (q > maxQ) maxQ = q;
            if (q < minQ) minQ = q;
        }
        double spread = maxQ - minQ;
        
        if (spread > 0.1) {
            System.out.println("æ”¶æ•›çŠ¶æ€: æ­£åœ¨å­¦ä¹ ï¼ŒQå€¼å·®å¼‚æ˜æ˜¾");
        } else {
            System.out.println("æ”¶æ•›çŠ¶æ€: åŸºæœ¬æ”¶æ•›ï¼ŒQå€¼å·®å¼‚è¾ƒå°");
        }
    }
    
    /**
     * è¿½è¸ªæ¢ç´¢è·¯å¾„
     */
    private void trackExplorationPath(DQNAgent agent, GridWorldEnvironment environment, int episodes) {
        for (int episode = 0; episode < episodes; episode++) {
            System.out.println(String.format("\n--- ç¬¬ %d å›åˆæ¢ç´¢è·¯å¾„ ---", episode + 1));
            
            Variable state = environment.reset();
            List<String> path = new ArrayList<>();
            int steps = 0;
            
            while (!environment.isDone() && steps < 20) {
                Variable action = agent.selectAction(state);
                Environment.StepResult result = environment.step(action);
                
                int x = (int) state.getValue().get(0);
                int y = (int) state.getValue().get(1);
                int actionIndex = (int) action.getValue().getNumber().floatValue();
                
                String[] actionNames = {"â†‘", "â†“", "â†", "â†’"};
                String stepInfo = String.format("(%d,%d)%s", x, y, actionNames[actionIndex]);
                path.add(stepInfo);
                
                Experience experience = new Experience(
                    state, action, result.getReward(),
                    result.getNextState(), result.isDone(), steps
                );
                
                agent.learn(experience);
                
                state = result.getNextState();
                steps++;
            }
            
            System.out.println("è·¯å¾„: " + String.join(" â†’ ", path));
            System.out.println(String.format("æ­¥æ•°: %d", steps));
        }
    }
    
    /**
     * ç”Ÿæˆæ¢ç´¢çƒ­åŠ›å›¾
     */
    private void generateExplorationHeatmap(DQNAgent agent, GridWorldEnvironment environment, int episodes) {
        int[][] visitCount = new int[6][6];
        
        // æ”¶é›†è®¿é—®æ•°æ®
        for (int episode = 0; episode < episodes; episode++) {
            Variable state = environment.reset();
            int steps = 0;
            
            while (!environment.isDone() && steps < 50) {
                int x = (int) state.getValue().get(0);
                int y = (int) state.getValue().get(1);
                
                if (x >= 0 && x < 6 && y >= 0 && y < 6) {
                    visitCount[y][x]++;
                }
                
                Variable action = agent.selectAction(state);
                Environment.StepResult result = environment.step(action);
                
                Experience experience = new Experience(
                    state, action, result.getReward(),
                    result.getNextState(), result.isDone(), steps
                );
                
                agent.learn(experience);
                
                state = result.getNextState();
                steps++;
            }
        }
        
        // æ˜¾ç¤ºçƒ­åŠ›å›¾
        int maxVisits = Arrays.stream(visitCount)
            .flatMapToInt(Arrays::stream)
            .max().orElse(1);
        
        for (int y = 0; y < 6; y++) {
            for (int x = 0; x < 6; x++) {
                double intensity = (double) visitCount[y][x] / maxVisits;
                String symbol;
                
                if (intensity > 0.8) symbol = "â–ˆ";
                else if (intensity > 0.6) symbol = "â–“";
                else if (intensity > 0.4) symbol = "â–’";
                else if (intensity > 0.2) symbol = "â–‘";
                else if (intensity > 0) symbol = "Â·";
                else symbol = " ";
                
                System.out.print(symbol + " ");
            }
            System.out.println();
        }
        
        System.out.println("\nçƒ­åŠ›å›¾è¯´æ˜: â–ˆ=é«˜é¢‘è®¿é—®, â–“=ä¸­é«˜é¢‘, â–’=ä¸­é¢‘, â–‘=ä½é¢‘, Â·=å¶å°”, ç©º=æœªè®¿é—®");
    }
    
    /**
     * å®æ—¶è®­ç»ƒç›‘æ§
     */
    private void realTimeTrainingMonitor(DQNAgent agent, CartPoleEnvironment environment, int episodes) {
        for (int episode = 0; episode < episodes; episode++) {
            Variable state = environment.reset();
            float episodeReward = 0.0f;
            int steps = 0;
            
            while (!environment.isDone() && steps < 500) {
                Variable action = agent.selectAction(state);
                Environment.StepResult result = environment.step(action);
                
                Experience experience = new Experience(
                    state, action, result.getReward(),
                    result.getNextState(), result.isDone(), steps
                );
                
                agent.learn(experience);
                
                state = result.getNextState();
                episodeReward += result.getReward();
                steps++;
            }
            
            // æ¯10å›åˆæ˜¾ç¤ºç›‘æ§ä¿¡æ¯
            if ((episode + 1) % 10 == 0) {
                System.out.println(String.format("%5d | %6s | %4d | %5s | %6s | %6s%%",
                                 episode + 1,
                                 df2.format(episodeReward),
                                 steps,
                                 df4.format(agent.getCurrentEpsilon()),
                                 df4.format(agent.getAverageLoss()),
                                 df2.format(agent.getBufferUsage() * 100)));
            }
        }
    }
    
    /**
     * æ”¶é›†æ€§èƒ½æ•°æ®
     */
    private Map<String, PerformanceMetrics> collectPerformanceData() {
        Map<String, PerformanceMetrics> data = new HashMap<>();
        
        // æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®
        data.put("DQN", new PerformanceMetrics("DQN", 156.8f, 12.3f, 89.2f, 0.0012f));
        data.put("REINFORCE", new PerformanceMetrics("REINFORCE", 142.5f, 15.7f, 78.6f, 0.0008f));
        data.put("Îµ-è´ªå¿ƒ", new PerformanceMetrics("Îµ-è´ªå¿ƒ", 0.485f, 0.023f, 91.3f, 0.0f));
        data.put("UCB", new PerformanceMetrics("UCB", 0.512f, 0.018f, 94.7f, 0.0f));
        data.put("æ±¤æ™®æ£®é‡‡æ ·", new PerformanceMetrics("æ±¤æ™®æ£®é‡‡æ ·", 0.498f, 0.021f, 88.9f, 0.0f));
        
        return data;
    }
    
    /**
     * ç”Ÿæˆæ€§èƒ½Dashboard
     */
    private void generatePerformanceDashboard(Map<String, PerformanceMetrics> data) {
        System.out.println("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        System.out.println("â”‚           æ€§èƒ½Dashboard             â”‚");
        System.out.println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        
        System.out.printf("â”‚ %-12s â”‚ %6s â”‚ %6s â”‚ %6s â”‚%n", "ç®—æ³•", "å¹³å‡å¥–åŠ±", "ç¨³å®šæ€§", "æˆåŠŸç‡");
        System.out.println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
        
        for (PerformanceMetrics metric : data.values()) {
            String stabilityLevel = metric.stability > 0.02f ? "ä½" : metric.stability > 0.01f ? "ä¸­" : "é«˜";
            System.out.printf("â”‚ %-12s â”‚ %6s â”‚ %6s â”‚ %5s%% â”‚%n",
                             metric.name,
                             df2.format(metric.avgReward),
                             stabilityLevel,
                             df2.format(metric.successRate));
        }
        
        System.out.println("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
    }
    
    /**
     * åˆ†æè¶‹åŠ¿
     */
    private void analyzeTrends(Map<String, PerformanceMetrics> data) {
        System.out.println("\n=== æ€§èƒ½è¶‹åŠ¿åˆ†æ ===");
        
        // æ‰¾å‡ºæœ€ä½³ç®—æ³•
        PerformanceMetrics best = data.values().stream()
            .max(Comparator.comparing(m -> m.avgReward))
            .orElse(null);
        
        if (best != null) {
            System.out.println(String.format("ğŸ† æœ€ä½³ç®—æ³•: %s (å¹³å‡å¥–åŠ±: %.4f)", best.name, best.avgReward));
        }
        
        // ç¨³å®šæ€§åˆ†æ
        PerformanceMetrics mostStable = data.values().stream()
            .min(Comparator.comparing(m -> m.stability))
            .orElse(null);
        
        if (mostStable != null) {
            System.out.println(String.format("ğŸ¯ æœ€ç¨³å®šç®—æ³•: %s (ç¨³å®šæ€§: %.6f)", mostStable.name, mostStable.stability));
        }
        
        // æ¨èå»ºè®®
        System.out.println("\nğŸ“Š ä½¿ç”¨å»ºè®®:");
        System.out.println("â€¢ è¿½æ±‚é«˜æ€§èƒ½: é€‰æ‹©" + (best != null ? best.name : "æœªçŸ¥"));
        System.out.println("â€¢ è¦æ±‚ç¨³å®šæ€§: é€‰æ‹©" + (mostStable != null ? mostStable.name : "æœªçŸ¥"));
        System.out.println("â€¢ å®æ—¶åº”ç”¨: è€ƒè™‘è®¡ç®—æ•ˆç‡å’Œå†…å­˜å ç”¨");
    }
    
    /**
     * è®­ç»ƒæ™ºèƒ½ä½“è¾…åŠ©æ–¹æ³•
     */
    private void trainAgent(Agent agent, Environment environment, int episodes) {
        for (int episode = 0; episode < episodes; episode++) {
            Variable state = environment.reset();
            int steps = 0;
            
            while (!environment.isDone() && steps < 500) {
                Variable action = agent.selectAction(state);
                Environment.StepResult result = environment.step(action);
                
                Experience experience = new Experience(
                    state, action, result.getReward(),
                    result.getNextState(), result.isDone(), steps
                );
                
                agent.learn(experience);
                
                if (agent instanceof REINFORCEAgent && result.isDone()) {
                    ((REINFORCEAgent) agent).learnFromEpisode();
                }
                
                state = result.getNextState();
                steps++;
            }
        }
    }
    
    /**
     * æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»
     */
    private static class PerformanceMetrics {
        String name;
        float avgReward;
        float stability;
        float successRate;
        float avgLoss;
        
        public PerformanceMetrics(String name, float avgReward, float stability, float successRate, float avgLoss) {
            this.name = name;
            this.avgReward = avgReward;
            this.stability = stability;
            this.successRate = successRate;
            this.avgLoss = avgLoss;
        }
    }
}