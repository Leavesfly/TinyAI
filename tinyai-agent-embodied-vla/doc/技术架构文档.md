# TinyAI VLA 模块技术架构文档

## 1. 模块概述

`tinyai-agent-embodied-vla` 是 TinyAI 智能体系统层的高级具身智能模块，实现了基于**视觉-语言-动作（Vision-Language-Action, VLA）架构**的端到端具身智能系统。

### 1.1 核心特性

- ✅ **多模态融合**：统一建模视觉、语言、动作三种模态
- ✅ **端到端学习**：从原始感知输入直接到动作输出
- ✅ **语言引导**：自然语言指令引导视觉注意力和动作生成
- ✅ **零样本泛化**：通过语言组合完成未见过的新任务
- ✅ **纯Java实现**：完全基于TinyAI生态，无外部依赖

## 2. 系统架构

### 2.1 分层架构

```
┌─────────────────────────────────────────────────────────────┐
│                     VLA智能体核心层                           │
│  ┌──────────────┬──────────────┬──────────────┬───────────┐ │
│  │ VLA编码器     │ 跨模态融合    │ VLA解码器     │ 学习引擎   │ │
│  └──────────────┴──────────────┴──────────────┴───────────┘ │
└─────────────────────────────────────────────────────────────┘
                              ↕
┌─────────────────────────────────────────────────────────────┐
│                     模态处理层                                │
│  ┌──────────────┬──────────────┬──────────────┐             │
│  │ 视觉处理      │ 语言处理      │ 动作处理      │             │
│  └──────────────┴──────────────┴──────────────┘             │
└─────────────────────────────────────────────────────────────┘
                              ↕
┌─────────────────────────────────────────────────────────────┐
│                     环境仿真层                                │
│  ┌──────────────┬──────────────┬──────────────┐             │
│  │ 机器人环境    │ 操作任务      │ 场景管理      │             │
│  └──────────────┴──────────────┴──────────────┘             │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 核心模块

| 模块 | 功能 | 关键类 |
|-----|------|--------|
| **数据模型层** | 定义多模态数据结构 | VLAState, VLAAction, VisionInput, LanguageInput |
| **编码器层** | 多模态特征提取 | VisionEncoder, LanguageEncoder, ProprioceptionEncoder |
| **融合层** | 跨模态注意力融合 | CrossModalAttention, VLATransformerCore |
| **解码器层** | 动作生成与反馈 | ActionDecoder, LanguageFeedbackGenerator |
| **环境层** | 仿真环境交互 | RobotEnvironment, SimpleRobotEnv, TaskScenario |
| **学习层** | 训练与优化 | VLALearningEngine, BehaviorCloningLearner |

## 3. 核心组件设计

### 3.1 数据模型层

#### VLAState - 统一状态表示

```java
public class VLAState {
    private VisionInput visionInput;           // 视觉模态
    private LanguageInput languageInput;       // 语言模态
    private ProprioceptionInput proprioceptionInput; // 本体感知
    private NdArray fusedFeatures;            // 融合特征
    private Map<String, NdArray> attentionWeights; // 注意力权重
}
```

#### VLAAction - 动作表示

```java
public class VLAAction {
    private NdArray continuousAction;    // 连续动作 [action_dim]
    private int discreteAction;          // 离散动作索引
    private ActionType actionType;       // 动作类型
    private double confidence;           // 置信度 [0, 1]
    private String languageFeedback;     // 语言反馈
}
```

### 3.2 编码器架构

#### VisionEncoder - 视觉编码器

**架构流程**：
```
RGB图像 [H, W, 3]
    ↓
卷积特征提取（ResNet-style）
    ├─ Conv2D (3→64, kernel=7, stride=2)
    ├─ Conv2D (64→128, kernel=3, stride=2)
    └─ Conv2D (128→256, kernel=3, stride=2)
    ↓
空间展平 + Linear投影 (256→hiddenDim)
    ↓
位置编码注入
    ↓
视觉Token序列 [num_patches, hiddenDim]
```

**关键特性**：
- 分层特征提取保留空间结构
- 2D正弦位置编码建模空间关系
- 输出维度与其他模态对齐

#### LanguageEncoder - 语言编码器

**架构流程**：
```
自然语言指令
    ↓
Tokenization（简化词表）
    ↓
Token Embedding [seq_len, hiddenDim]
    ↓
位置编码（可学习式）
    ↓
Transformer编码器 × 6层
    ↓
语言Token序列 [seq_len, hiddenDim]
```

**复用策略**：
- 复用 GPT Transformer Block 结构
- 自定义简化的 Tokenizer
- 支持多语言指令（中英文）

#### ProprioceptionEncoder - 本体感知编码器

**架构流程**：
```
关节状态 [n_joints * 2 + 1]
    ↓
MLP编码器
    ├─ Linear (input→256) + LayerNorm + ReLU
    ├─ Linear (256→512) + LayerNorm + ReLU
    └─ Linear (512→hiddenDim)
    ↓
本体感知嵌入 [1, hiddenDim]
```

### 3.3 跨模态融合层

#### CrossModalAttention - 跨模态注意力

**注意力计算**：
```python
Query: 语言特征 [seq_len_lang, hiddenDim]
Key/Value: [视觉特征; 本体特征] [num_visual + num_proprio, hiddenDim]

Attention(Q, K, V) = softmax(QK^T / √d_k) V

输出: 语言引导的多模态特征
```

**三种注意力类型**：
1. **Vision-to-Language**: 语言理解视觉场景
2. **Language-to-Vision**: 视觉关注语言指令
3. **Proprioception-to-All**: 当前状态全局感知

#### VLATransformerCore - 统一Transformer

**网络结构**：
```
拼接Token序列 [语言; 视觉; 本体]
    ↓
Transformer Layer × 12
    ├─ Multi-Head Self-Attention (全局注意力)
    ├─ Cross-Modal Attention (跨模态注意力)
    ├─ Feed-Forward Network (hiddenDim * 4)
    └─ Layer Norm + Residual
    ↓
融合特征表示 [total_seq_len, hiddenDim]
```

### 3.4 解码器架构

#### ActionDecoder - 动作解码器

**多头输出设计**：
```
融合特征 [seq_len, hiddenDim]
    ↓
特征聚合（取最后token）
    ↓
    ├─ 连续动作头
    │   └─ MLP (hiddenDim→512→256→action_dim)
    │       └─ Tanh激活（归一化到[-1, 1]）
    │
    ├─ 离散动作头
    │   └─ Linear (hiddenDim→num_discrete_actions)
    │       └─ Softmax
    │
    └─ 语言反馈头
        └─ 基于ActionType和置信度生成文本
```

## 4. 环境仿真层

### 4.1 RobotEnvironment 接口

**核心方法**：
```java
public interface RobotEnvironment {
    VLAState reset();                    // 重置环境
    EnvironmentStep step(VLAAction);     // 执行动作
    byte[] render();                     // 渲染图像
    ActionSpaceSpec getActionSpace();    // 动作空间
    ObservationSpaceSpec getObservationSpace(); // 观测空间
    void close();                        // 清理资源
}
```

### 4.2 SimpleRobotEnv - 简单机器人环境

**PickAndPlace 任务实现**：
- 7自由度机械臂仿真
- 末端执行器位置控制
- 夹爪开合状态模拟
- 物体抓取与放置检测
- 奖励函数设计

**奖励函数**：
```python
R_total = w1 * (-distance_to_object)     # 接近物体
        + w2 * grasp_success              # 抓取成功 +10
        + w3 * (-distance_to_target)      # 接近目标
        + w4 * task_completion            # 任务完成 +50
```

### 4.3 TaskScenario - 任务场景

| 任务类型 | 难度 | 描述 |
|---------|------|------|
| PICK_AND_PLACE | ⭐⭐ | 拾取物体并放置到目标位置 |
| STACK_BLOCKS | ⭐⭐⭐ | 堆叠多个方块 |
| OPEN_DRAWER | ⭐⭐⭐ | 打开抽屉 |
| POUR_WATER | ⭐⭐⭐⭐ | 倒水任务 |
| ASSEMBLE_PARTS | ⭐⭐⭐⭐⭐ | 组装零件 |

## 5. 学习引擎

### 5.1 学习范式

| 学习方式 | 说明 | 适用场景 |
|---------|------|---------|
| 行为克隆（BC） | 监督学习，模仿专家演示 | 有大量标注演示数据 |
| 强化学习（RL） | 通过奖励信号学习策略 | 可定义明确奖励函数 |
| 混合学习 | BC预训练 + RL微调 | 实际应用推荐方式 |

### 5.2 BehaviorCloningLearner

**训练流程**：
```java
for (int episode = 0; episode < numEpisodes; episode++) {
    VLAState state = env.reset();
    while (!done) {
        VLAAction action = agent.predict(state);
        EnvironmentStep step = env.step(action);
        // 收集轨迹数据
        state = step.getNextState();
    }
}
```

## 6. 数据流与推理流程

### 6.1 完整推理流程

```
用户指令 + 图像
    ↓
1. 多模态编码
   ├─ VisionEncoder: RGB图像 → 视觉特征 [196, 768]
   ├─ LanguageEncoder: 文本指令 → 语言特征 [seq_len, 768]
   └─ ProprioceptionEncoder: 关节状态 → 本体特征 [1, 768]
    ↓
2. 特征拼接
   → 多模态Token序列 [total_len, 768]
    ↓
3. 跨模态融合
   → VLATransformerCore → 融合特征 [total_len, 768]
    ↓
4. 动作解码
   ├─ 连续动作 [7]：末端执行器位姿 + 夹爪
   ├─ 离散动作 [1]：动作类型索引
   └─ 语言反馈：自然语言描述
    ↓
5. 环境执行
   → 新状态 + 奖励 + 终止标志
```

### 6.2 性能指标

| 指标 | 目标值 | 说明 |
|-----|--------|------|
| 推理延迟 | < 50ms | 单次动作预测时间 |
| 训练速度 | > 100 steps/s | 训练吞吐量 |
| 简单任务成功率 | > 90% | PickAndPlace |
| 复杂任务成功率 | > 60% | AssembleParts |
| 模型参数量 | ~35M | 可部署到边缘设备 |

## 7. 依赖关系

### 7.1 内部模块依赖

```
tinyai-agent-embodied-vla
    ├─ tinyai-deeplearning-ndarr (NdArray多维数组)
    ├─ tinyai-deeplearning-func (Variable自动微分)
    ├─ tinyai-deeplearning-nnet (神经网络层)
    │   ├─ Linear
    │   ├─ Conv2D
    │   ├─ LayerNorm
    │   └─ Embedding
    ├─ tinyai-deeplearning-ml (优化器与损失函数)
    ├─ tinyai-deeplearning-rl (强化学习算法)
    └─ tinyai-model-gpt (Transformer结构复用)
```

### 7.2 外部依赖

- **JUnit 5**: 单元测试框架
- **无其他外部依赖**

## 8. 扩展性设计

### 8.1 新模态接入

```java
// 1. 定义触觉输入模型
public class TactileInput {
    private NdArray forceSensor;
    private NdArray pressureMap;
}

// 2. 实现触觉编码器
public class TactileEncoder extends Block {
    public NdArray encode(TactileInput input) { ... }
}

// 3. 注册到融合层
CrossModalFusion.registerModality("tactile", tactileEncoder);
```

### 8.2 新任务场景

```java
// 实现新任务环境
public class PourWaterEnv implements RobotEnvironment {
    @Override
    public VLAState reset() { ... }
    
    @Override
    public EnvironmentStep step(VLAAction action) {
        // 倒水物理仿真
        // 奖励函数设计
    }
}
```

## 9. 技术亮点

### 9.1 创新点

1. **统一多模态建模**：三种模态在统一Transformer框架下建模
2. **语言引导注意力**：自然语言动态调节视觉注意力权重
3. **端到端优化**：从感知到动作的全链路可微分学习
4. **零样本泛化**：通过语言组合完成未见过的新任务

### 9.2 工程优势

1. **纯Java实现**：完全基于TinyAI生态，无外部依赖
2. **模块化设计**：各组件松耦合，易于替换和扩展
3. **高度可复用**：充分复用GPT、RL等已有模块
4. **文档完善**：详细的设计文档与代码注释

## 10. 应用场景

### 10.1 机器人操作

- 物体抓取与放置
- 组装任务
- 工具使用

### 10.2 人机协作

- 辅助装配
- 物料搬运
- 质量检查

### 10.3 服务机器人

- 家庭服务
- 餐饮服务
- 医疗辅助

## 11. 参考资料

- OpenVLA: An Open-Source Vision-Language-Action Model
- RT-1: Robotics Transformer for Real-World Control at Scale
- PaLM-E: An Embodied Multimodal Language Model
- TinyAI 项目文档

---

**文档版本**: v1.0  
**最后更新**: 2025-10-18  
**维护者**: TinyAI 开发团队
