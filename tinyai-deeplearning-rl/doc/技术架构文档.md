# TinyAI Reinforcement Learning æŠ€æœ¯æ¶æ„æ–‡æ¡£

## æ¦‚è¿°

`tinyai-deeplearning-rl` æ˜¯ TinyAI æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¨¡å—ï¼Œå®ç°äº†ä»ç»å…¸å¤šè‡‚è€è™æœºåˆ°ç°ä»£æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚æœ¬æ¨¡å—é‡‡ç”¨æ ‡å‡†åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¶æ„è®¾è®¡ï¼Œéµå¾ª OpenAI Gym æ¥å£è§„èŒƒï¼Œä¸ºæ„å»ºæ™ºèƒ½å†³ç­–ç³»ç»Ÿæä¾›äº†å¯é çš„åŸºç¡€ã€‚

## æŠ€æœ¯æ¶æ„

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

æœ¬æ¨¡å—é‡‡ç”¨ç»å…¸çš„æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’èŒƒå¼ï¼ˆAgent-Environment Interaction Paradigmï¼‰ï¼Œå°†å¼ºåŒ–å­¦ä¹ é—®é¢˜åˆ†è§£ä¸ºä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š

- **Agentï¼ˆæ™ºèƒ½ä½“ï¼‰**ï¼šå†³ç­–åˆ¶å®šè€…ï¼Œè´Ÿè´£ç­–ç•¥å­¦ä¹ å’ŒåŠ¨ä½œé€‰æ‹©
- **Environmentï¼ˆç¯å¢ƒï¼‰**ï¼šäº¤äº’ç¯å¢ƒï¼Œæä¾›çŠ¶æ€è½¬ç§»å’Œå¥–åŠ±åé¦ˆ
- **Policyï¼ˆç­–ç•¥ï¼‰**ï¼šåŠ¨ä½œé€‰æ‹©æœºåˆ¶ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
- **Experienceï¼ˆç»éªŒï¼‰**ï¼šäº¤äº’è®°å½•ï¼Œæ”¯æŒç»éªŒå›æ”¾å­¦ä¹ 
- **ReplayBufferï¼ˆç»éªŒç¼“å†²åŒºï¼‰**ï¼šç»éªŒå­˜å‚¨å’Œé‡‡æ ·ç®¡ç†

### æ¶æ„å±‚æ¬¡è®¾è®¡

```mermaid
graph TB
    subgraph "åº”ç”¨å±‚"
        App[å¼ºåŒ–å­¦ä¹ åº”ç”¨]
        TrainingLoop[è®­ç»ƒå¾ªç¯]
        Evaluation[è¯„ä¼°æµ‹è¯•]
    end
    
    subgraph "ç®—æ³•å±‚"
        DQN[DQNæ·±åº¦Qç½‘ç»œ]
        REINFORCE[REINFORCEç­–ç•¥æ¢¯åº¦]
        Bandits[å¤šè‡‚è€è™æœºç®—æ³•]
    end
    
    subgraph "æ ¸å¿ƒç»„ä»¶å±‚"
        Agent[Agentæ™ºèƒ½ä½“åŸºç±»]
        Environment[Environmentç¯å¢ƒåŸºç±»]
        Policy[Policyç­–ç•¥åŸºç±»]
        Experience[Experienceç»éªŒæ•°æ®]
        ReplayBuffer[ReplayBufferç»éªŒç¼“å†²]
    end
    
    subgraph "åŸºç¡€è®¾æ–½å±‚"
        ML[tinyai-deeplearning-ml]
        NNet[ç¥ç»ç½‘ç»œå±‚]
        Optimizer[ä¼˜åŒ–å™¨]
        Loss[æŸå¤±å‡½æ•°]
    end
    
    App --> TrainingLoop
    TrainingLoop --> DQN
    TrainingLoop --> REINFORCE
    TrainingLoop --> Bandits
    
    DQN --> Agent
    REINFORCE --> Agent
    Bandits --> Agent
    
    Agent --> Experience
    Agent --> ReplayBuffer
    Agent --> Policy
    
    Environment --> Experience
    
    Agent --> ML
    ML --> NNet
    ML --> Optimizer
    ML --> Loss
```

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. Agent æ™ºèƒ½ä½“æ¶æ„

#### æŠ½è±¡åŸºç±» Agent

`Agent` æ˜¯æ‰€æœ‰æ™ºèƒ½ä½“çš„æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰äº†å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“çš„æ ‡å‡†æ¥å£ï¼š

**æ ¸å¿ƒå±æ€§ï¼š**
- `name`: æ™ºèƒ½ä½“åç§°æ ‡è¯†
- `stateDim`: çŠ¶æ€ç©ºé—´ç»´åº¦
- `actionDim`: åŠ¨ä½œç©ºé—´ç»´åº¦
- `model`: ä¸»è¦ç¥ç»ç½‘ç»œæ¨¡å‹
- `learningRate`: å­¦ä¹ ç‡å‚æ•°
- `epsilon`: æ¢ç´¢ç‡ï¼ˆÎµ-è´ªå¿ƒç­–ç•¥ï¼‰
- `gamma`: æŠ˜æ‰£å› å­
- `trainingStep`: è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨

**æ ¸å¿ƒæ–¹æ³•ï¼š**
```java
// åŠ¨ä½œé€‰æ‹©
public abstract Variable selectAction(Variable state);

// å•æ­¥å­¦ä¹ 
public abstract void learn(Experience experience);

// æ‰¹é‡å­¦ä¹ 
public abstract void learnBatch(Experience[] experiences);

// ç»éªŒå­˜å‚¨
public abstract void storeExperience(Experience experience);
```

#### æ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“

##### DQNAgent - æ·±åº¦Qç½‘ç»œ

**ç®—æ³•ç‰¹ç‚¹ï¼š**
- ä½¿ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘Qå‡½æ•°
- ç»éªŒå›æ”¾æœºåˆ¶æ‰“ç ´æ•°æ®ç›¸å…³æ€§
- ç›®æ ‡ç½‘ç»œç¨³å®šè®­ç»ƒè¿‡ç¨‹
- Îµ-è´ªå¿ƒç­–ç•¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨

**æ ¸å¿ƒå®ç°ï¼š**
```java
// ä¸»è¦æˆå‘˜å˜é‡
private final int batchSize;              // æ‰¹æ¬¡å¤§å°
private final int targetUpdateFreq;       // ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
private final ReplayBuffer replayBuffer;  // ç»éªŒå›æ”¾ç¼“å†²åŒº
private final Model targetModel;          // ç›®æ ‡ç½‘ç»œ
private final EpsilonGreedyPolicy policy; // Îµ-è´ªå¿ƒç­–ç•¥
private final Optimizer optimizer;        // ä¼˜åŒ–å™¨
private final Loss lossFunction;          // æŸå¤±å‡½æ•°
```

**å­¦ä¹ è¿‡ç¨‹ï¼š**
1. ç»éªŒå­˜å‚¨ï¼šå°†(s,a,r,s',done)å­˜å…¥å›æ”¾ç¼“å†²åŒº
2. æ‰¹é‡é‡‡æ ·ï¼šä»ç¼“å†²åŒºéšæœºé‡‡æ ·è®­ç»ƒæ‰¹æ¬¡
3. ç›®æ ‡è®¡ç®—ï¼šä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—Qç›®æ ‡å€¼
4. æŸå¤±è®¡ç®—ï¼šè®¡ç®—å½“å‰Qå€¼ä¸ç›®æ ‡Qå€¼çš„MSEæŸå¤±
5. åå‘ä¼ æ’­ï¼šæ›´æ–°ä¸»ç½‘ç»œå‚æ•°
6. ç½‘ç»œåŒæ­¥ï¼šå®šæœŸå°†ä¸»ç½‘ç»œæƒé‡å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ

##### REINFORCEAgent - ç­–ç•¥æ¢¯åº¦

**ç®—æ³•ç‰¹ç‚¹ï¼š**
- ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‡½æ•°
- æ”¯æŒè¿ç»­å’Œç¦»æ•£åŠ¨ä½œç©ºé—´
- åŸºçº¿å‡½æ•°å‡å°‘æ–¹å·®
- è’™ç‰¹å¡æ´›é‡‡æ ·ä¼°è®¡æ¢¯åº¦

#### å¤šè‡‚è€è™æœºæ™ºèƒ½ä½“

##### BanditAgent - è€è™æœºåŸºç±»

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- ç»´æŠ¤æ¯ä¸ªè‡‚çš„ç»Ÿè®¡ä¿¡æ¯
- è®¡ç®—åŠ¨ä½œé€‰æ‹©æ¬¡æ•°å’Œç´¯ç§¯å¥–åŠ±
- æä¾›å…±åŒçš„å¥–åŠ±ä¼°è®¡æœºåˆ¶

**ç»Ÿè®¡ä¿¡æ¯ï¼š**
```java
protected int[] actionCounts;        // æ¯ä¸ªè‡‚è¢«é€‰æ‹©çš„æ¬¡æ•°
protected float[] totalRewards;      // æ¯ä¸ªè‡‚çš„ç´¯ç§¯å¥–åŠ±
protected float[] estimatedRewards;  // æ¯ä¸ªè‡‚çš„ä¼°è®¡å¹³å‡å¥–åŠ±
protected int totalActions;          // æ€»çš„åŠ¨ä½œé€‰æ‹©æ¬¡æ•°
```

##### EpsilonGreedyBanditAgent - Îµ-è´ªå¿ƒè€è™æœº

**ç®—æ³•åŸç†ï¼š**
- ä»¥æ¦‚ç‡ Îµ éšæœºæ¢ç´¢
- ä»¥æ¦‚ç‡ (1-Îµ) é€‰æ‹©å½“å‰æœ€ä¼˜è‡‚
- ç®€å•æœ‰æ•ˆçš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ç­–ç•¥

##### UCBBanditAgent - ä¸Šç½®ä¿¡åŒºé—´

**ç®—æ³•åŸç†ï¼š**
- åŸºäºä¸Šç½®ä¿¡åŒºé—´çš„é€‰æ‹©ç­–ç•¥
- åŒæ—¶è€ƒè™‘å¥–åŠ±å‡å€¼å’Œä¸ç¡®å®šæ€§
- å…·æœ‰ç†è®ºæœ€ä¼˜çš„é—æ†¾ç•Œé™

**UCBå…¬å¼ï¼š**
```
UCB(i) = Q(i) + c * sqrt(ln(t) / N(i))
```
å…¶ä¸­ Q(i) æ˜¯è‡‚içš„ä¼°è®¡å¥–åŠ±ï¼ŒN(i) æ˜¯è‡‚iè¢«é€‰æ‹©çš„æ¬¡æ•°ï¼Œt æ˜¯æ€»æ—¶é—´æ­¥æ•°ï¼Œc æ˜¯æ¢ç´¢å‚æ•°ã€‚

##### ThompsonSamplingBanditAgent - æ±¤æ™®æ£®é‡‡æ ·

**ç®—æ³•åŸç†ï¼š**
- åŸºäºè´å¶æ–¯æ¨ç†çš„é‡‡æ ·ç­–ç•¥
- ç»´æŠ¤æ¯ä¸ªè‡‚çš„åéªŒåˆ†å¸ƒ
- æ ¹æ®åéªŒåˆ†å¸ƒé‡‡æ ·è¿›è¡Œå†³ç­–

### 2. Environment ç¯å¢ƒæ¶æ„

#### æŠ½è±¡åŸºç±» Environment

**æ ¸å¿ƒå±æ€§ï¼š**
- `stateDim`: çŠ¶æ€ç©ºé—´ç»´åº¦
- `actionDim`: åŠ¨ä½œç©ºé—´ç»´åº¦
- `currentState`: å½“å‰çŠ¶æ€
- `done`: å›åˆæ˜¯å¦ç»“æŸ
- `currentStep`: å½“å‰æ­¥æ•°
- `maxSteps`: æœ€å¤§æ­¥æ•°é™åˆ¶

**æ ‡å‡†æ¥å£ï¼š**
```java
// ç¯å¢ƒé‡ç½®
public abstract Variable reset();

// çŠ¶æ€è½¬ç§»
public abstract StepResult step(Variable action);

// éšæœºåŠ¨ä½œé‡‡æ ·
public abstract Variable sampleAction();

// åŠ¨ä½œæœ‰æ•ˆæ€§æ£€æŸ¥
public abstract boolean isValidAction(Variable action);
```

#### ç¯å¢ƒå®ç°

##### CartPoleEnvironment - å€’ç«‹æ‘†ç¯å¢ƒ

**ç‰©ç†æ¨¡å‹ï¼š**
- çŠ¶æ€ç©ºé—´ï¼š4ç»´ [ä½ç½®, é€Ÿåº¦, è§’åº¦, è§’é€Ÿåº¦]
- åŠ¨ä½œç©ºé—´ï¼š2ç»´ [å‘å·¦æ¨, å‘å³æ¨]
- ç‰©ç†å‚æ•°ï¼šé‡åŠ›ã€è´¨é‡ã€é•¿åº¦ç­‰ç‰©ç†å¸¸æ•°
- ç»ˆæ­¢æ¡ä»¶ï¼šæ†å­å€’ä¸‹æˆ–å°è½¦è¶…å‡ºè¾¹ç•Œ

**çŠ¶æ€æ›´æ–°æ–¹ç¨‹ï¼š**
åŸºäºç»å…¸ç‰©ç†æ–¹ç¨‹çš„æ•°å€¼ç§¯åˆ†ï¼Œä½¿ç”¨æ¬§æ‹‰æ–¹æ³•è¿›è¡ŒçŠ¶æ€è½¬ç§»è®¡ç®—ã€‚

##### GridWorldEnvironment - ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ

**ç¯å¢ƒç‰¹æ€§ï¼š**
- ç¦»æ•£ç½‘æ ¼çŠ¶æ€ç©ºé—´
- 4æ–¹å‘ç§»åŠ¨åŠ¨ä½œï¼ˆä¸Šã€ä¸‹ã€å·¦ã€å³ï¼‰
- å¯é…ç½®å¥–åŠ±å’Œéšœç¢ç‰©
- é€‚åˆæµ‹è¯•åŸºç¡€å¼ºåŒ–å­¦ä¹ ç®—æ³•

##### MultiArmedBanditEnvironment - å¤šè‡‚è€è™æœºç¯å¢ƒ

**ç¯å¢ƒè®¾ç½®ï¼š**
- å¤šä¸ªè€è™æœºè‡‚ï¼ˆåŠ¨ä½œé€‰æ‹©ï¼‰
- æ¯ä¸ªè‡‚æœ‰ä¸åŒçš„å¥–åŠ±åˆ†å¸ƒ
- æ”¯æŒé«˜æ–¯åˆ†å¸ƒå¥–åŠ±ç”Ÿæˆ
- è®°å½•æ‚”æ¨å€¼å’Œæ€§èƒ½æŒ‡æ ‡

### 3. Policy ç­–ç•¥æ¶æ„

#### EpsilonGreedyPolicy - Îµ-è´ªå¿ƒç­–ç•¥

**ç­–ç•¥æœºåˆ¶ï¼š**
- å¯é…ç½®çš„æ¢ç´¢ç‡ Îµ
- è‡ªåŠ¨æ¢ç´¢ç‡è¡°å‡
- æ”¯æŒä¸åŒè¡°å‡ç­–ç•¥
- Qå€¼å‡½æ•°ä¾èµ–

**åŠ¨ä½œé€‰æ‹©é€»è¾‘ï¼š**
```java
public Variable selectAction(Variable state) {
    if (random.nextFloat() < epsilon) {
        // æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
        return sampleRandomAction();
    } else {
        // åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
        return selectGreedyAction(state);
    }
}
```

### 4. ç»éªŒç®¡ç†ç³»ç»Ÿ

#### Experience - ç»éªŒæ•°æ®ç»“æ„

**æ•°æ®ç»„æˆï¼š**
```java
private final Variable state;      // å½“å‰çŠ¶æ€
private final Variable action;     // æ‰§è¡Œçš„åŠ¨ä½œ
private final float reward;        // è·å¾—çš„å¥–åŠ±
private final Variable nextState;  // ä¸‹ä¸€çŠ¶æ€
private final boolean done;        // æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€
private final int timeStep;        // æ—¶é—´æ­¥ç´¢å¼•
```

#### ReplayBuffer - ç»éªŒå›æ”¾ç¼“å†²åŒº

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- å›ºå®šå¤§å°çš„å¾ªç¯ç¼“å†²åŒº
- éšæœºé‡‡æ ·é˜²æ­¢æ•°æ®ç›¸å…³æ€§
- é«˜æ•ˆçš„å†…å­˜ç®¡ç†
- æ”¯æŒæ‰¹é‡é‡‡æ ·å’Œè·å–æœ€è¿‘ç»éªŒ

**ç¼“å†²åŒºç‰¹æ€§ï¼š**
- å®¹é‡ç®¡ç†ï¼šå½“ç¼“å†²åŒºæ»¡æ—¶è¦†ç›–æœ€æ—§çš„ç»éªŒ
- é‡‡æ ·ç­–ç•¥ï¼šå‡åŒ€éšæœºé‡‡æ ·ç¡®ä¿æ•°æ®ç‹¬ç«‹æ€§
- æ€§èƒ½ç›‘æ§ï¼šæä¾›ä½¿ç”¨ç‡å’Œå¤§å°ç»Ÿè®¡ä¿¡æ¯

## æŠ€æœ¯ç‰¹æ€§

### ç®—æ³•ä¸°å¯Œæ€§

1. **å¤šè‡‚è€è™æœºç®—æ³•ç³»åˆ—**
   - Îµ-è´ªå¿ƒï¼šç®€å•æœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥
   - UCBï¼šç†è®ºæœ€ä¼˜çš„ç½®ä¿¡åŒºé—´æ–¹æ³•
   - æ±¤æ™®æ£®é‡‡æ ·ï¼šè´å¶æ–¯æ¨ç†çš„é‡‡æ ·ç­–ç•¥

2. **æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•**
   - DQNï¼šå¼€åˆ›æ€§çš„æ·±åº¦Qç½‘ç»œç®—æ³•
   - REINFORCEï¼šç»å…¸çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•

### è®¾è®¡ä¼˜åŠ¿

1. **æ ‡å‡†åŒ–æ¥å£**
   - éµå¾ª OpenAI Gym æ ‡å‡†
   - ç»Ÿä¸€çš„æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’æ¨¡å¼
   - å¯æ‰©å±•çš„ç®—æ³•å®ç°æ¡†æ¶

2. **é«˜æ€§èƒ½å®ç°**
   - é«˜æ•ˆçš„ç»éªŒå›æ”¾æœºåˆ¶
   - ä¼˜åŒ–çš„å†…å­˜ç®¡ç†
   - æ”¯æŒæ‰¹é‡å­¦ä¹ 

3. **æ˜“ç”¨æ€§è®¾è®¡**
   - ç®€æ´çš„APIæ¥å£
   - ä¸°å¯Œçš„é¢„ç½®ç¯å¢ƒå’Œç®—æ³•
   - å®Œå–„çš„æ–‡æ¡£å’Œç¤ºä¾‹

## ä¾èµ–å…³ç³»

### å†…éƒ¨ä¾èµ–

- **tinyai-deeplearning-ml**ï¼šæä¾›æ¨¡å‹è®­ç»ƒã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰æœºå™¨å­¦ä¹ æ ¸å¿ƒåŠŸèƒ½
- **tinyai-deeplearning-func**ï¼šæä¾›è‡ªåŠ¨å¾®åˆ†å’Œå˜é‡ç³»ç»Ÿ
- **tinyai-deeplearning-ndarr**ï¼šæä¾›å¤šç»´æ•°ç»„æ•°æ®ç»“æ„
- **tinyai-deeplearning-nnet**ï¼šæä¾›ç¥ç»ç½‘ç»œå±‚å’Œå—

### å¤–éƒ¨ä¾èµ–

- **JUnit 4**ï¼šå•å…ƒæµ‹è¯•æ¡†æ¶ï¼Œç”¨äºç®—æ³•æ­£ç¡®æ€§éªŒè¯

## ç®—æ³•æ€§èƒ½å¯¹æ¯”

### å¤šè‡‚è€è™æœºç®—æ³•å¯¹æ¯”

| ç®—æ³• | æ¢ç´¢ç­–ç•¥ | ç†è®ºä¿è¯ | è®¡ç®—å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|------------|----------|
| Îµ-è´ªå¿ƒ | å›ºå®šæ¦‚ç‡æ¢ç´¢ | ç®€å•é—æ†¾ç•Œ | O(1) | åœ¨çº¿å­¦ä¹ ã€å¿«é€Ÿå†³ç­– |
| UCB | ç½®ä¿¡åŒºé—´æ¢ç´¢ | æœ€ä¼˜é—æ†¾ç•Œ | O(1) | ç†è®ºæœ€ä¼˜ã€ç¨³å®šç¯å¢ƒ |
| æ±¤æ™®æ£®é‡‡æ · | è´å¶æ–¯é‡‡æ · | æœ€ä¼˜é—æ†¾ç•Œ | O(k) | è´å¶æ–¯ä¼˜åŒ–ã€ä¸ç¡®å®šç¯å¢ƒ |

### æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ç‰¹ç‚¹

| ç®—æ³• | ç±»å‹ | çŠ¶æ€ç©ºé—´ | åŠ¨ä½œç©ºé—´ | æ ·æœ¬æ•ˆç‡ | ç¨³å®šæ€§ |
|------|------|----------|----------|----------|---------|
| DQN | å€¼å‡½æ•° | è¿ç»­ | ç¦»æ•£ | ä¸­ç­‰ | è¾ƒå¥½ |
| REINFORCE | ç­–ç•¥æ¢¯åº¦ | è¿ç»­ | è¿ç»­/ç¦»æ•£ | è¾ƒä½ | ä¸€èˆ¬ |

## æµ‹è¯•ä½“ç³»

### å•å…ƒæµ‹è¯•è¦†ç›–

1. **ç»„ä»¶çº§æµ‹è¯•**
   - `ExperienceTest`: ç»éªŒæ•°æ®ç»“æ„æµ‹è¯•
   - `ReplayBufferTest`: ç»éªŒç¼“å†²åŒºåŠŸèƒ½æµ‹è¯•
   - å„ç®—æ³•æ™ºèƒ½ä½“çš„æ­£ç¡®æ€§æµ‹è¯•

2. **é›†æˆæµ‹è¯•**
   - `RLIntegrationTest`: å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•
   - æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’æµ‹è¯•
   - å¤šå›åˆå­¦ä¹ æ”¶æ•›æ€§æµ‹è¯•

3. **æ€§èƒ½æµ‹è¯•**
   - ç®—æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯•
   - è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§æµ‹è¯•

### æµ‹è¯•ç­–ç•¥

```java
// ç¤ºä¾‹ï¼šå®Œæ•´è®­ç»ƒå¾ªç¯æµ‹è¯•
@Test
public void testCompleteTrainingLoop() {
    Variable state = environment.reset();
    
    while (!environment.isDone()) {
        Variable action = agent.selectAction(state);
        Environment.StepResult result = environment.step(action);
        
        Experience experience = new Experience(
            state, action, result.getReward(), 
            result.getNextState(), result.isDone()
        );
        agent.learn(experience);
        
        state = result.getNextState();
    }
    
    // éªŒè¯å­¦ä¹ æ•ˆæœ
    assertTrue(agent.getOptimalActionRate() > 0);
}
```

## ä½¿ç”¨æŒ‡å—

### åŸºæœ¬ä½¿ç”¨æµç¨‹

1. **åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“**
```java
// åˆ›å»ºç¯å¢ƒ
CartPoleEnvironment env = new CartPoleEnvironment();

// åˆ›å»ºDQNæ™ºèƒ½ä½“
DQNAgent agent = new DQNAgent(
    "CartPole-DQN", env.getStateDim(), env.getActionDim(),
    new int[]{128, 128}, 0.001f, 1.0f, 0.99f,
    32, 10000, 100
);
```

2. **è®­ç»ƒå¾ªç¯**
```java
for (int episode = 0; episode < 1000; episode++) {
    Variable state = env.reset();
    
    while (!env.isDone()) {
        Variable action = agent.selectAction(state);
        Environment.StepResult result = env.step(action);
        
        Experience experience = new Experience(
            state, action, result.getReward(),
            result.getNextState(), result.isDone()
        );
        
        agent.learn(experience);
        state = result.getNextState();
    }
    
    agent.decayEpsilon(0.995f);
}
```

### æ‰©å±•å¼€å‘æŒ‡å—

#### æ·»åŠ æ–°çš„æ™ºèƒ½ä½“ç®—æ³•

```java
public class CustomAgent extends Agent {
    
    public CustomAgent(String name, int stateDim, int actionDim, 
                      float learningRate, float epsilon, float gamma) {
        super(name, stateDim, actionDim, learningRate, epsilon, gamma);
        // åˆå§‹åŒ–è‡ªå®šä¹‰å‚æ•°
    }
    
    @Override
    public Variable selectAction(Variable state) {
        // å®ç°åŠ¨ä½œé€‰æ‹©é€»è¾‘
        return customActionSelection(state);
    }
    
    @Override
    public void learn(Experience experience) {
        // å®ç°å­¦ä¹ æ›´æ–°é€»è¾‘
        customLearningUpdate(experience);
    }
    
    // å…¶ä»–å¿…éœ€æ–¹æ³•çš„å®ç°...
}
```

#### æ·»åŠ æ–°çš„ç¯å¢ƒ

```java
public class CustomEnvironment extends Environment {
    
    public CustomEnvironment() {
        super(stateDim, actionDim, maxSteps);
        // ç¯å¢ƒç‰¹å®šçš„åˆå§‹åŒ–
    }
    
    @Override
    public Variable reset() {
        // å®ç°ç¯å¢ƒé‡ç½®é€»è¾‘
        return initialState;
    }
    
    @Override
    public StepResult step(Variable action) {
        // å®ç°çŠ¶æ€è½¬ç§»é€»è¾‘
        return new StepResult(nextState, reward, done, info);
    }
    
    // å…¶ä»–å¿…éœ€æ–¹æ³•çš„å®ç°...
}
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–

1. **ç»éªŒå›æ”¾ä¼˜åŒ–**
   - ä½¿ç”¨è¶³å¤Ÿå¤§çš„ç¼“å†²åŒºï¼ˆ10000-100000ï¼‰
   - åˆç†è®¾ç½®æ‰¹æ¬¡å¤§å°ï¼ˆ32-128ï¼‰
   - ç¡®ä¿å……åˆ†çš„é¢„çƒ­æœŸ

2. **ç½‘ç»œæ›´æ–°ç­–ç•¥**
   - åˆç†è®¾ç½®ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
   - ä½¿ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
   - é€‚å½“çš„å­¦ä¹ ç‡è°ƒåº¦

3. **æ¢ç´¢ç­–ç•¥è°ƒä¼˜**
   - åˆç†è®¾ç½®åˆå§‹æ¢ç´¢ç‡ï¼ˆ0.9-1.0ï¼‰
   - é€‚å½“çš„æ¢ç´¢ç‡è¡°å‡ï¼ˆ0.995-0.999ï¼‰
   - ä¿æŒæœ€å°æ¢ç´¢ç‡ï¼ˆ0.01-0.05ï¼‰

### è¶…å‚æ•°è°ƒä¼˜æŒ‡å—

| å‚æ•° | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|----------|------|
| å­¦ä¹ ç‡ | 0.0001-0.01 | ä»å°å¼€å§‹ï¼Œæ ¹æ®æ”¶æ•›æƒ…å†µè°ƒæ•´ |
| æ‰¹æ¬¡å¤§å° | 32-128 | å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ¢¯åº¦ç¨³å®šæ€§ |
| ç¼“å†²åŒºå¤§å° | 10000-100000 | æ ¹æ®å†…å­˜é™åˆ¶å’Œä»»åŠ¡å¤æ‚åº¦é€‰æ‹© |
| æ¢ç´¢ç‡è¡°å‡ | 0.995-0.999 | ä¿æŒé€‚åº¦æ¢ç´¢èƒ½åŠ› |
| æŠ˜æ‰£å› å­ | 0.9-0.99 | æ ¹æ®ä»»åŠ¡çš„æ—¶é—´è·¨åº¦è°ƒæ•´ |

## ç‰ˆæœ¬ä¿¡æ¯

- **æ¨¡å—ç‰ˆæœ¬**: 1.0-SNAPSHOT
- **Javaç‰ˆæœ¬**: 17+
- **æ„å»ºå·¥å…·**: Maven 3.6+
- **ä¾èµ–æ¡†æ¶**: TinyAI DeepLearning Framework

## æœªæ¥å‘å±•æ–¹å‘

### ç®—æ³•æ‰©å±•è®¡åˆ’

1. **é«˜çº§DQNå˜ç§**
   - Double DQNï¼šè§£å†³Qå€¼è¿‡ä¼°è®¡é—®é¢˜
   - Dueling DQNï¼šåˆ†ç¦»å€¼å‡½æ•°å’Œä¼˜åŠ¿å‡½æ•°
   - Rainbow DQNï¼šé›†æˆå¤šç§æ”¹è¿›æŠ€æœ¯

2. **ç­–ç•¥æ¢¯åº¦ç®—æ³•**
   - Actor-Criticï¼šç»“åˆå€¼å‡½æ•°å’Œç­–ç•¥å‡½æ•°
   - PPOï¼šè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
   - SACï¼šè½¯æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•

3. **é«˜çº§è€è™æœºç®—æ³•**
   - Contextual Banditsï¼šä¸Šä¸‹æ–‡å¤šè‡‚è€è™æœº
   - Adversarial Banditsï¼šå¯¹æŠ—æ€§å¤šè‡‚è€è™æœº

### æ€§èƒ½ä¼˜åŒ–è®¡åˆ’

1. **å¹¶è¡ŒåŒ–æ”¯æŒ**
   - å¤šçº¿ç¨‹è®­ç»ƒ
   - åˆ†å¸ƒå¼å­¦ä¹ 
   - GPUåŠ é€Ÿè®¡ç®—

2. **å†…å­˜ä¼˜åŒ–**
   - ä¼˜å…ˆç»éªŒå›æ”¾
   - å‹ç¼©å­˜å‚¨
   - æµå¼å­¦ä¹ 

---

**TinyAI Reinforcement Learning æ¨¡å—** - æ„å»ºæ™ºèƒ½å†³ç­–ç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€ ğŸ¯