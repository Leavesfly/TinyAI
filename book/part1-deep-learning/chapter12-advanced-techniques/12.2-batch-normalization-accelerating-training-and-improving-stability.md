# 12.2 批量归一化：加速训练与提升稳定性

批量归一化（Batch Normalization）是深度学习中的一项重要技术，由Sergey Ioffe和Christian Szegedy在2015年提出。它通过在神经网络的每一层对输入进行归一化处理，显著加速了训练过程并提高了模型的稳定性。批量归一化已成为现代深度学习架构中的标准组件。

## 本节内容概览

- 批量归一化的数学原理和实现机制
- 训练模式与推理模式的区别
- 层归一化和组归一化的变种
- 归一化层的参数更新策略

## 设计思考与技术理念

批量归一化的核心思想是解决深度神经网络训练过程中的内部协变量偏移（Internal Covariate Shift）问题。随着网络层数的加深，每一层的输入分布会发生变化，这会减慢训练速度并增加训练的不稳定性。

批量归一化通过以下方式解决这个问题：
1. **标准化输入**：对每层的输入进行标准化处理
2. **可学习参数**：引入可学习的缩放和平移参数
3. **批处理统计**：利用小批量数据的统计信息

这种设计既保持了网络的表达能力，又提升了训练的稳定性和速度。

## 12.2.1 批量归一化的数学原理

批量归一化的数学公式如下：

对于一个批次中的每个特征维度 $k$：

$$\mu_B \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i,k}$$

$$\sigma_B^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m} (x_{i,k} - \mu_B)^2$$

$$\hat{x}_{i,k} \leftarrow \frac{x_{i,k} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$

$$y_{i,k} \leftarrow \gamma_k \hat{x}_{i,k} + \beta_k$$

其中：
- $x_{i,k}$ 是第 $i$ 个样本的第 $k$ 个特征
- $\mu_B$ 和 $\sigma_B^2$ 是批次均值和方差
- $\epsilon$ 是防止除零的小常数
- $\gamma_k$ 和 $\beta_k$ 是可学习的缩放和平移参数

```java
import java.util.Arrays;

/**
 * 批量归一化实现
 */
public class BatchNormalization {
    private int featureSize;
    private double[] gamma;      // 缩放参数
    private double[] beta;       // 平移参数
    private double[] runningMean; // 运行时均值
    private double[] runningVar;  // 运行时方差
    private double momentum;     // 动量参数
    private double epsilon;      // 防止除零的小常数
    private boolean isTraining;  // 训练模式标志
    
    /**
     * 构造函数
     * @param featureSize 特征维度大小
     * @param momentum 动量参数，用于更新运行时统计量
     * @param epsilon 防止除零的小常数
     */
    public BatchNormalization(int featureSize, double momentum, double epsilon) {
        this.featureSize = featureSize;
        this.momentum = momentum;
        this.epsilon = epsilon;
        this.isTraining = true;
        
        // 初始化参数
        this.gamma = new double[featureSize];
        this.beta = new double[featureSize];
        this.runningMean = new double[featureSize];
        this.runningVar = new double[featureSize];
        
        // 初始化gamma为1，beta为0
        Arrays.fill(gamma, 1.0);
        Arrays.fill(beta, 0.0);
        // runningMean和runningVar初始化为0和1
        Arrays.fill(runningVar, 1.0);
    }
    
    /**
     * 前向传播
     * @param inputs 输入数据 (batchSize x featureSize)
     * @return 归一化后的输出
     */
    public double[][] forward(double[][] inputs) {
        int batchSize = inputs.length;
        
        if (isTraining) {
            return forwardTraining(inputs, batchSize);
        } else {
            return forwardInference(inputs, batchSize);
        }
    }
    
    /**
     * 训练模式前向传播
     */
    private double[][] forwardTraining(double[][] inputs, int batchSize) {
        // 计算批次均值
        double[] batchMean = computeBatchMean(inputs, batchSize);
        
        // 计算批次方差
        double[] batchVar = computeBatchVariance(inputs, batchSize, batchMean);
        
        // 更新运行时统计量
        updateRunningStatistics(batchMean, batchVar);
        
        // 归一化
        double[][] normalized = normalize(inputs, batchSize, batchMean, batchVar);
        
        // 缩放和平移
        return scaleAndShift(normalized, batchSize);
    }
    
    /**
     * 推理模式前向传播
     */
    private double[][] forwardInference(double[][] inputs, int batchSize) {
        // 使用运行时统计量进行归一化
        double[][] normalized = normalize(inputs, batchSize, runningMean, runningVar);
        
        // 缩放和平移
        return scaleAndShift(normalized, batchSize);
    }
    
    /**
     * 计算批次均值
     */
    private double[] computeBatchMean(double[][] inputs, int batchSize) {
        double[] mean = new double[featureSize];
        
        for (int j = 0; j < featureSize; j++) {
            double sum = 0.0;
            for (int i = 0; i < batchSize; i++) {
                sum += inputs[i][j];
            }
            mean[j] = sum / batchSize;
        }
        
        return mean;
    }
    
    /**
     * 计算批次方差
     */
    private double[] computeBatchVariance(double[][] inputs, int batchSize, double[] mean) {
        double[] variance = new double[featureSize];
        
        for (int j = 0; j < featureSize; j++) {
            double sum = 0.0;
            for (int i = 0; i < batchSize; i++) {
                double diff = inputs[i][j] - mean[j];
                sum += diff * diff;
            }
            variance[j] = sum / batchSize;
        }
        
        return variance;
    }
    
    /**
     * 更新运行时统计量
     */
    private void updateRunningStatistics(double[] batchMean, double[] batchVar) {
        for (int j = 0; j < featureSize; j++) {
            runningMean[j] = momentum * runningMean[j] + (1 - momentum) * batchMean[j];
            runningVar[j] = momentum * runningVar[j] + (1 - momentum) * batchVar[j];
        }
    }
    
    /**
     * 归一化
     */
    private double[][] normalize(double[][] inputs, int batchSize, double[] mean, double[] variance) {
        double[][] normalized = new double[batchSize][featureSize];
        
        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < featureSize; j++) {
                normalized[i][j] = (inputs[i][j] - mean[j]) / Math.sqrt(variance[j] + epsilon);
            }
        }
        
        return normalized;
    }
    
    /**
     * 缩放和平移
     */
    private double[][] scaleAndShift(double[][] normalized, int batchSize) {
        double[][] outputs = new double[batchSize][featureSize];
        
        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < featureSize; j++) {
                outputs[i][j] = gamma[j] * normalized[i][j] + beta[j];
            }
        }
        
        return outputs;
    }
    
    /**
     * 反向传播
     * @param inputs 输入数据
     * @param gradOutputs 输出梯度
     * @return 输入梯度
     */
    public double[][] backward(double[][] inputs, double[][] gradOutputs) {
        int batchSize = inputs.length;
        
        // 计算批次统计量
        double[] batchMean = computeBatchMean(inputs, batchSize);
        double[] batchVar = computeBatchVariance(inputs, batchSize, batchMean);
        
        // 计算归一化值
        double[][] normalized = normalize(inputs, batchSize, batchMean, batchVar);
        
        // 计算参数梯度
        double[] gradGamma = new double[featureSize];
        double[] gradBeta = new double[featureSize];
        
        for (int j = 0; j < featureSize; j++) {
            for (int i = 0; i < batchSize; i++) {
                gradGamma[j] += gradOutputs[i][j] * normalized[i][j];
                gradBeta[j] += gradOutputs[i][j];
            }
        }
        
        // 更新参数梯度（在实际应用中会传递给优化器）
        // 这里简化处理，直接更新参数
        double learningRate = 0.01;
        for (int j = 0; j < featureSize; j++) {
            gamma[j] -= learningRate * gradGamma[j];
            beta[j] -= learningRate * gradBeta[j];
        }
        
        // 计算输入梯度
        return computeInputGradients(inputs, gradOutputs, batchMean, batchVar, normalized);
    }
    
    /**
     * 计算输入梯度
     */
    private double[][] computeInputGradients(double[][] inputs, double[][] gradOutputs, 
                                           double[] mean, double[] variance, double[][] normalized) {
        int batchSize = inputs.length;
        double[][] gradInputs = new double[batchSize][featureSize];
        
        for (int j = 0; j < featureSize; j++) {
            // 计算该特征维度的标准差
            double stdInv = 1.0 / Math.sqrt(variance[j] + epsilon);
            
            // 计算该特征维度的梯度相关项
            double gradNormSum = 0.0;
            double gradNormDotNormSum = 0.0;
            
            for (int i = 0; i < batchSize; i++) {
                gradNormSum += gradOutputs[i][j] * gamma[j];
                gradNormDotNormSum += gradOutputs[i][j] * gamma[j] * normalized[i][j];
            }
            
            // 计算输入梯度
            for (int i = 0; i < batchSize; i++) {
                double gradNorm = gradOutputs[i][j] * gamma[j];
                double gradVar = gradNormDotNormSum * stdInv * stdInv * stdInv / batchSize;
                double gradMean = gradNormSum * stdInv / batchSize;
                
                gradInputs[i][j] = gradNorm * stdInv - gradMean - normalized[i][j] * gradVar;
            }
        }
        
        return gradInputs;
    }
    
    /**
     * 设置训练模式
     * @param training 是否为训练模式
     */
    public void setTraining(boolean training) {
        this.isTraining = training;
    }
    
    /**
     * 获取运行时均值
     */
    public double[] getRunningMean() {
        return Arrays.copyOf(runningMean, featureSize);
    }
    
    /**
     * 获取运行时方差
     */
    public double[] getRunningVar() {
        return Arrays.copyOf(runningVar, featureSize);
    }
    
    /**
     * 获取缩放参数
     */
    public double[] getGamma() {
        return Arrays.copyOf(gamma, featureSize);
    }
    
    /**
     * 获取平移参数
     */
    public double[] getBeta() {
        return Arrays.copyOf(beta, featureSize);
    }
}
```

## 12.2.2 训练与推理模式的区别

批量归一化在训练和推理模式下有不同的行为：

```java
/**
 * 批量归一化模式管理器
 */
public class BatchNormModeManager {
    
    /**
     * 训练模式特点：
     * 1. 使用当前批次的统计量进行归一化
     * 2. 更新运行时统计量
     * 3. 添加噪声，提高泛化能力
     */
    public static class TrainingMode {
        private BatchNormalization batchNorm;
        
        public TrainingMode(BatchNormalization batchNorm) {
            this.batchNorm = batchNorm;
            this.batchNorm.setTraining(true);
        }
        
        /**
         * 训练时的前向传播
         * @param inputs 输入数据
         * @return 处理后的数据
         */
        public double[][] forward(double[][] inputs) {
            System.out.println("使用训练模式进行批量归一化");
            return batchNorm.forward(inputs);
        }
    }
    
    /**
     * 推理模式特点：
     * 1. 使用运行时统计量进行归一化
     * 2. 不更新统计量
     * 3. 确定性输出
     */
    public static class InferenceMode {
        private BatchNormalization batchNorm;
        
        public InferenceMode(BatchNormalization batchNorm) {
            this.batchNorm = batchNorm;
            this.batchNorm.setTraining(false);
        }
        
        /**
         * 推理时的前向传播
         * @param inputs 输入数据
         * @return 处理后的数据
         */
        public double[][] forward(double[][] inputs) {
            System.out.println("使用推理模式进行批量归一化");
            return batchNorm.forward(inputs);
        }
    }
    
    /**
     * 模式切换示例
     */
    public static void demonstrateModeSwitching() {
        int featureSize = 3;
        BatchNormalization batchNorm = new BatchNormalization(featureSize, 0.1, 1e-5);
        
        // 训练模式
        TrainingMode trainingMode = new TrainingMode(batchNorm);
        double[][] trainInputs = {{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}};
        double[][] trainOutputs = trainingMode.forward(trainInputs);
        System.out.println("训练模式输出: " + Arrays.deepToString(trainOutputs));
        
        // 切换到推理模式
        InferenceMode inferenceMode = new InferenceMode(batchNorm);
        double[][] inferenceInputs = {{2.0, 3.0, 4.0}};
        double[][] inferenceOutputs = inferenceMode.forward(inferenceInputs);
        System.out.println("推理模式输出: " + Arrays.deepToString(inferenceOutputs));
    }
}
```

## 12.2.3 层归一化（Layer Normalization）

层归一化是对整个层的所有特征进行归一化，而不是对批次中的样本进行归一化。

```java
/**
 * 层归一化实现
 */
public class LayerNormalization {
    private int featureSize;
    private double[] gamma;      // 缩放参数
    private double[] beta;       // 平移参数
    private double epsilon;      // 防止除零的小常数
    
    /**
     * 构造函数
     * @param featureSize 特征维度大小
     * @param epsilon 防止除零的小常数
     */
    public LayerNormalization(int featureSize, double epsilon) {
        this.featureSize = featureSize;
        this.epsilon = epsilon;
        
        // 初始化参数
        this.gamma = new double[featureSize];
        this.beta = new double[featureSize];
        
        // 初始化gamma为1，beta为0
        Arrays.fill(gamma, 1.0);
        Arrays.fill(beta, 0.0);
    }
    
    /**
     * 前向传播
     * @param inputs 输入数据 (batchSize x featureSize)
     * @return 归一化后的输出
     */
    public double[][] forward(double[][] inputs) {
        int batchSize = inputs.length;
        double[][] outputs = new double[batchSize][featureSize];
        
        for (int i = 0; i < batchSize; i++) {
            // 计算当前样本的均值和方差
            double mean = 0.0;
            for (int j = 0; j < featureSize; j++) {
                mean += inputs[i][j];
            }
            mean /= featureSize;
            
            double variance = 0.0;
            for (int j = 0; j < featureSize; j++) {
                double diff = inputs[i][j] - mean;
                variance += diff * diff;
            }
            variance /= featureSize;
            
            // 归一化
            for (int j = 0; j < featureSize; j++) {
                double normalized = (inputs[i][j] - mean) / Math.sqrt(variance + epsilon);
                outputs[i][j] = gamma[j] * normalized + beta[j];
            }
        }
        
        return outputs;
    }
    
    /**
     * 反向传播
     * @param inputs 输入数据
     * @param gradOutputs 输出梯度
     * @return 输入梯度
     */
    public double[][] backward(double[][] inputs, double[][] gradOutputs) {
        int batchSize = inputs.length;
        double[][] gradInputs = new double[batchSize][featureSize];
        
        for (int i = 0; i < batchSize; i++) {
            // 计算均值和方差
            double mean = 0.0;
            for (int j = 0; j < featureSize; j++) {
                mean += inputs[i][j];
            }
            mean /= featureSize;
            
            double variance = 0.0;
            for (int j = 0; j < featureSize; j++) {
                double diff = inputs[i][j] - mean;
                variance += diff * diff;
            }
            variance /= featureSize;
            
            double stdInv = 1.0 / Math.sqrt(variance + epsilon);
            
            // 计算参数梯度
            for (int j = 0; j < featureSize; j++) {
                double normalized = (inputs[i][j] - mean) * stdInv;
                
                // 更新参数（简化处理）
                gamma[j] -= 0.01 * gradOutputs[i][j] * normalized;
                beta[j] -= 0.01 * gradOutputs[i][j];
                
                // 计算输入梯度
                double gradNorm = gradOutputs[i][j] * gamma[j];
                double gradVar = 0.0;
                double gradMean = 0.0;
                
                for (int k = 0; k < featureSize; k++) {
                    double normK = (inputs[i][k] - mean) * stdInv;
                    gradVar += gradOutputs[i][k] * gamma[k] * normK;
                    gradMean += gradOutputs[i][k] * gamma[k];
                }
                
                gradVar = gradVar * stdInv * stdInv * stdInv / featureSize;
                gradMean = gradMean * stdInv / featureSize;
                
                gradInputs[i][j] = gradNorm * stdInv - gradMean - normalized * gradVar;
            }
        }
        
        return gradInputs;
    }
}
```

## 12.2.4 组归一化（Group Normalization）

组归一化将特征通道分组，对每组进行归一化。

```java
/**
 * 组归一化实现
 */
public class GroupNormalization {
    private int featureSize;
    private int numGroups;
    private int channelsPerGroup;
    private double[][] gamma;    // 缩放参数
    private double[][] beta;     // 平移参数
    private double epsilon;      // 防止除零的小常数
    
    /**
     * 构造函数
     * @param featureSize 特征维度大小
     * @param numGroups 分组数量
     * @param epsilon 防止除零的小常数
     */
    public GroupNormalization(int featureSize, int numGroups, double epsilon) {
        if (featureSize % numGroups != 0) {
            throw new IllegalArgumentException("特征维度必须能被分组数量整除");
        }
        
        this.featureSize = featureSize;
        this.numGroups = numGroups;
        this.channelsPerGroup = featureSize / numGroups;
        this.epsilon = epsilon;
        
        // 初始化参数
        this.gamma = new double[numGroups][channelsPerGroup];
        this.beta = new double[numGroups][channelsPerGroup];
        
        // 初始化gamma为1，beta为0
        for (int i = 0; i < numGroups; i++) {
            Arrays.fill(gamma[i], 1.0);
            Arrays.fill(beta[i], 0.0);
        }
    }
    
    /**
     * 前向传播
     * @param inputs 输入数据 (batchSize x featureSize)
     * @return 归一化后的输出
     */
    public double[][] forward(double[][] inputs) {
        int batchSize = inputs.length;
        double[][] outputs = new double[batchSize][featureSize];
        
        for (int i = 0; i < batchSize; i++) {
            for (int g = 0; g < numGroups; g++) {
                // 计算当前组的均值和方差
                double sum = 0.0;
                int startChannel = g * channelsPerGroup;
                int endChannel = startChannel + channelsPerGroup;
                
                for (int j = startChannel; j < endChannel; j++) {
                    sum += inputs[i][j];
                }
                
                double mean = sum / channelsPerGroup;
                
                double variance = 0.0;
                for (int j = startChannel; j < endChannel; j++) {
                    double diff = inputs[i][j] - mean;
                    variance += diff * diff;
                }
                variance /= channelsPerGroup;
                
                // 归一化组内通道
                for (int j = startChannel; j < endChannel; j++) {
                    int channelInGroup = j - startChannel;
                    double normalized = (inputs[i][j] - mean) / Math.sqrt(variance + epsilon);
                    outputs[i][j] = gamma[g][channelInGroup] * normalized + beta[g][channelInGroup];
                }
            }
        }
        
        return outputs;
    }
    
    /**
     * 反向传播
     * @param inputs 输入数据
     * @param gradOutputs 输出梯度
     * @return 输入梯度
     */
    public double[][] backward(double[][] inputs, double[][] gradOutputs) {
        int batchSize = inputs.length;
        double[][] gradInputs = new double[batchSize][featureSize];
        
        for (int i = 0; i < batchSize; i++) {
            for (int g = 0; g < numGroups; g++) {
                int startChannel = g * channelsPerGroup;
                int endChannel = startChannel + channelsPerGroup;
                
                // 计算当前组的均值和方差
                double sum = 0.0;
                for (int j = startChannel; j < endChannel; j++) {
                    sum += inputs[i][j];
                }
                double mean = sum / channelsPerGroup;
                
                double variance = 0.0;
                for (int j = startChannel; j < endChannel; j++) {
                    double diff = inputs[i][j] - mean;
                    variance += diff * diff;
                }
                variance /= channelsPerGroup;
                
                double stdInv = 1.0 / Math.sqrt(variance + epsilon);
                
                // 更新参数并计算梯度
                for (int j = startChannel; j < endChannel; j++) {
                    int channelInGroup = j - startChannel;
                    double normalized = (inputs[i][j] - mean) * stdInv;
                    
                    // 更新参数（简化处理）
                    gamma[g][channelInGroup] -= 0.01 * gradOutputs[i][j] * normalized;
                    beta[g][channelInGroup] -= 0.01 * gradOutputs[i][j];
                    
                    // 计算输入梯度
                    double gradNorm = gradOutputs[i][j] * gamma[g][channelInGroup];
                    double gradVar = 0.0;
                    double gradMean = 0.0;
                    
                    for (int k = startChannel; k < endChannel; k++) {
                        double normK = (inputs[i][k] - mean) * stdInv;
                        gradVar += gradOutputs[i][k] * gamma[g][k - startChannel] * normK;
                        gradMean += gradOutputs[i][k] * gamma[g][k - startChannel];
                    }
                    
                    gradVar = gradVar * stdInv * stdInv * stdInv / channelsPerGroup;
                    gradMean = gradMean * stdInv / channelsPerGroup;
                    
                    gradInputs[i][j] = gradNorm * stdInv - gradMean - normalized * gradVar;
                }
            }
        }
        
        return gradInputs;
    }
}
```

## 12.2.5 归一化层的参数更新

归一化层的参数更新需要考虑梯度计算和优化策略：

```java
/**
 * 归一化层参数更新器
 */
public class NormalizationParameterUpdater {
    
    /**
     * 优化器接口
     */
    public interface Optimizer {
        void update(double[] parameters, double[] gradients, double learningRate);
    }
    
    /**
     * SGD优化器
     */
    public static class SGDOptimizer implements Optimizer {
        @Override
        public void update(double[] parameters, double[] gradients, double learningRate) {
            for (int i = 0; i < parameters.length; i++) {
                parameters[i] -= learningRate * gradients[i];
            }
        }
    }
    
    /**
     * 带动量的SGD优化器
     */
    public static class MomentumSGDOptimizer implements Optimizer {
        private double[] velocities;
        private double momentum;
        
        public MomentumSGDOptimizer(int paramSize, double momentum) {
            this.velocities = new double[paramSize];
            this.momentum = momentum;
        }
        
        @Override
        public void update(double[] parameters, double[] gradients, double learningRate) {
            for (int i = 0; i < parameters.length; i++) {
                velocities[i] = momentum * velocities[i] - learningRate * gradients[i];
                parameters[i] += velocities[i];
            }
        }
    }
    
    /**
     * Adam优化器（简化版）
     */
    public static class AdamOptimizer implements Optimizer {
        private double[] m;  // 一阶矩估计
        private double[] v;  // 二阶矩估计
        private double beta1;
        private double beta2;
        private double epsilon;
        private int t;       // 时间步
        
        public AdamOptimizer(int paramSize, double beta1, double beta2, double epsilon) {
            this.m = new double[paramSize];
            this.v = new double[paramSize];
            this.beta1 = beta1;
            this.beta2 = beta2;
            this.epsilon = epsilon;
            this.t = 0;
        }
        
        @Override
        public void update(double[] parameters, double[] gradients, double learningRate) {
            t++;
            
            for (int i = 0; i < parameters.length; i++) {
                // 更新一阶矩估计
                m[i] = beta1 * m[i] + (1 - beta1) * gradients[i];
                
                // 更新二阶矩估计
                v[i] = beta2 * v[i] + (1 - beta2) * gradients[i] * gradients[i];
                
                // 偏差修正
                double mHat = m[i] / (1 - Math.pow(beta1, t));
                double vHat = v[i] / (1 - Math.pow(beta2, t));
                
                // 参数更新
                parameters[i] -= learningRate * mHat / (Math.sqrt(vHat) + epsilon);
            }
        }
    }
    
    /**
     * 归一化层参数更新示例
     */
    public static void demonstrateParameterUpdate() {
        int featureSize = 4;
        BatchNormalization batchNorm = new BatchNormalization(featureSize, 0.1, 1e-5);
        
        // 模拟训练数据
        double[][] inputs = {
            {1.0, 2.0, 3.0, 4.0},
            {2.0, 3.0, 4.0, 5.0},
            {3.0, 4.0, 5.0, 6.0}
        };
        
        double[][] outputs = batchNorm.forward(inputs);
        System.out.println("前向传播输出: " + Arrays.deepToString(outputs));
        
        // 模拟梯度
        double[][] gradOutputs = {
            {0.1, 0.2, 0.3, 0.4},
            {0.2, 0.3, 0.4, 0.5},
            {0.3, 0.4, 0.5, 0.6}
        };
        
        // 反向传播
        double[][] gradInputs = batchNorm.backward(inputs, gradOutputs);
        System.out.println("反向传播输入梯度: " + Arrays.deepToString(gradInputs));
        
        // 使用不同优化器更新参数
        System.out.println("Gamma参数: " + Arrays.toString(batchNorm.getGamma()));
        System.out.println("Beta参数: " + Arrays.toString(batchNorm.getBeta()));
    }
}
```

## 综合示例：归一化技术对比

```java
/**
 * 归一化技术对比示例
 */
public class NormalizationComparison {
    
    public static void main(String[] args) {
        // 演示不同归一化技术
        demonstrateBatchNormalization();
        demonstrateLayerNormalization();
        demonstrateGroupNormalization();
        demonstrateParameterUpdate();
    }
    
    /**
     * 演示批量归一化
     */
    private static void demonstrateBatchNormalization() {
        System.out.println("=== 批量归一化演示 ===");
        
        int featureSize = 3;
        BatchNormalization batchNorm = new BatchNormalization(featureSize, 0.1, 1e-5);
        
        double[][] inputs = {
            {1.0, 2.0, 3.0},
            {4.0, 5.0, 6.0},
            {7.0, 8.0, 9.0}
        };
        
        System.out.println("输入数据: " + Arrays.deepToString(inputs));
        
        // 训练模式
        batchNorm.setTraining(true);
        double[][] trainOutputs = batchNorm.forward(inputs);
        System.out.println("训练模式输出: " + Arrays.deepToString(trainOutputs));
        
        // 推理模式
        batchNorm.setTraining(false);
        double[][] inferenceOutputs = batchNorm.forward(inputs);
        System.out.println("推理模式输出: " + Arrays.deepToString(inferenceOutputs));
        
        System.out.println();
    }
    
    /**
     * 演示层归一化
     */
    private static void demonstrateLayerNormalization() {
        System.out.println("=== 层归一化演示 ===");
        
        int featureSize = 4;
        LayerNormalization layerNorm = new LayerNormalization(featureSize, 1e-5);
        
        double[][] inputs = {
            {1.0, 2.0, 3.0, 4.0},
            {2.0, 4.0, 6.0, 8.0}
        };
        
        System.out.println("输入数据: " + Arrays.deepToString(inputs));
        
        double[][] outputs = layerNorm.forward(inputs);
        System.out.println("层归一化输出: " + Arrays.deepToString(outputs));
        
        System.out.println();
    }
    
    /**
     * 演示组归一化
     */
    private static void demonstrateGroupNormalization() {
        System.out.println("=== 组归一化演示 ===");
        
        int featureSize = 6;
        int numGroups = 2;
        GroupNormalization groupNorm = new GroupNormalization(featureSize, numGroups, 1e-5);
        
        double[][] inputs = {
            {1.0, 2.0, 3.0, 4.0, 5.0, 6.0},
            {2.0, 4.0, 6.0, 8.0, 10.0, 12.0}
        };
        
        System.out.println("输入数据: " + Arrays.deepToString(inputs));
        
        double[][] outputs = groupNorm.forward(inputs);
        System.out.println("组归一化输出: " + Arrays.deepToString(outputs));
        
        System.out.println();
    }
    
    /**
     * 演示参数更新
     */
    private static void demonstrateParameterUpdate() {
        System.out.println("=== 参数更新演示 ===");
        
        NormalizationParameterUpdater.demonstrateParameterUpdate();
        
        System.out.println();
    }
}
```

## 总结

本节详细介绍了批量归一化及其变种技术：

1. **批量归一化**：
   - 通过标准化每层输入解决内部协变量偏移问题
   - 在训练和推理模式下有不同的行为
   - 包含可学习的缩放和平移参数

2. **层归一化**：
   - 对单个样本的所有特征进行归一化
   - 适用于RNN等序列模型
   - 不依赖于批次大小

3. **组归一化**：
   - 将特征通道分组进行归一化
   - 在小批次和大模型场景下表现良好
   - 是批量归一化和层归一化的折中方案

4. **参数更新策略**：
   - 归一化层参数需要通过反向传播计算梯度
   - 可以使用不同的优化器进行参数更新
   - 参数更新对模型性能有重要影响

这些归一化技术在现代深度学习中发挥着重要作用，能够显著提升模型的训练速度和稳定性。在实际应用中，需要根据具体任务和模型架构选择合适的归一化方法。