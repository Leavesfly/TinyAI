# 10.1 马尔可夫决策过程：强化学习的数学框架

> **本节学习目标**：理解强化学习的数学基础和MDP框架，掌握MDP的基本要素、贝尔曼方程和价值函数的数学定义

## 内容概览

马尔可夫决策过程（Markov Decision Process, MDP）是强化学习的数学基础，为描述和解决强化学习问题提供了严格的数学框架。本节将详细介绍MDP的基本要素、贝尔曼方程、价值函数和策略的数学定义，为后续学习强化学习算法奠定坚实的理论基础。

## 10.1.1 强化学习基本概念

强化学习研究的是智能体（Agent）如何在环境中通过试错学习来最大化累积奖励的问题。在这个过程中，智能体通过与环境的交互来学习最优策略。

### 核心要素

1. **智能体（Agent）**：学习和决策的主体
2. **环境（Environment）**：智能体交互的外部世界
3. **状态（State）**：环境的描述信息
4. **动作（Action）**：智能体可以执行的操作
5. **奖励（Reward）**：环境对智能体动作的反馈
6. **策略（Policy）**：智能体选择动作的规则
7. **价值函数（Value Function）**：衡量状态或状态-动作对的长期价值

### 交互过程

强化学习的交互过程可以描述为：
```
智能体 → 动作 → 环境 → 状态/奖励 → 智能体
```

在每个时间步t，智能体观察环境状态s_t，根据策略选择动作a_t，环境返回下一个状态s_{t+1}和即时奖励r_{t+1}。

## 10.1.2 马尔可夫决策过程定义

MDP是一个五元组M = (S, A, P, R, γ)，其中：

- **S**：状态空间（State Space），所有可能状态的集合
- **A**：动作空间（Action Space），所有可能动作的集合
- **P**：状态转移概率函数（Transition Probability Function），P(s'|s,a)表示在状态s执行动作a转移到状态s'的概率
- **R**：奖励函数（Reward Function），R(s,a)表示在状态s执行动作a获得的期望奖励
- **γ**：折扣因子（Discount Factor），γ ∈ [0,1]，用于计算未来奖励的现值

### 离散MDP

对于离散状态和动作空间的MDP，各要素可以表示为：

```java
/**
 * 离散马尔可夫决策过程
 */
public class DiscreteMDP {
    private int numStates;           // 状态数量
    private int numActions;          // 动作数量
    private double[][][] transition; // 转移概率 P[s][a][s']
    private double[][] reward;       // 奖励函数 R[s][a]
    private double discountFactor;   // 折扣因子 γ
    
    /**
     * 构造函数
     */
    public DiscreteMDP(int numStates, int numActions, double discountFactor) {
        this.numStates = numStates;
        this.numActions = numActions;
        this.discountFactor = discountFactor;
        
        // 初始化转移概率和奖励函数
        this.transition = new double[numStates][numActions][numStates];
        this.reward = new double[numStates][numActions];
    }
    
    /**
     * 设置状态转移概率
     */
    public void setTransitionProbability(int state, int action, int nextState, double probability) {
        if (state >= 0 && state < numStates && 
            action >= 0 && action < numActions && 
            nextState >= 0 && nextState < numStates) {
            this.transition[state][action][nextState] = probability;
        }
    }
    
    /**
     * 获取状态转移概率
     */
    public double getTransitionProbability(int state, int action, int nextState) {
        if (state >= 0 && state < numStates && 
            action >= 0 && action < numActions && 
            nextState >= 0 && nextState < numStates) {
            return this.transition[state][action][nextState];
        }
        return 0.0;
    }
    
    /**
     * 设置奖励函数
     */
    public void setReward(int state, int action, double reward) {
        if (state >= 0 && state < numStates && 
            action >= 0 && action < numActions) {
            this.reward[state][action] = reward;
        }
    }
    
    /**
     * 获取奖励
     */
    public double getReward(int state, int action) {
        if (state >= 0 && state < numStates && 
            action >= 0 && action < numActions) {
            return this.reward[state][action];
        }
        return 0.0;
    }
    
    /**
     * 验证转移概率的正确性（每行概率和为1）
     */
    public boolean validateTransitionProbabilities() {
        for (int s = 0; s < numStates; s++) {
            for (int a = 0; a < numActions; a++) {
                double sum = 0.0;
                for (int sNext = 0; sNext < numStates; sNext++) {
                    sum += transition[s][a][sNext];
                }
                if (Math.abs(sum - 1.0) > 1e-6) {
                    System.out.printf("状态 %d 动作 %d 的转移概率和为 %.6f，不等于1%n", s, a, sum);
                    return false;
                }
            }
        }
        return true;
    }
    
    // Getter方法
    public int getNumStates() { return numStates; }
    public int getNumActions() { return numActions; }
    public double getDiscountFactor() { return discountFactor; }
    public double[][][] getTransition() { return transition; }
    public double[][] getReward() { return reward; }
}

/**
 * MDP状态类
 */
class MDPState {
    private int stateId;
    private String description;
    
    public MDPState(int stateId, String description) {
        this.stateId = stateId;
        this.description = description;
    }
    
    // Getter方法
    public int getStateId() { return stateId; }
    public String getDescription() { return description; }
}

/**
 * MDP动作类
 */
class MDPAction {
    private int actionId;
    private String description;
    
    public MDPAction(int actionId, String description) {
        this.actionId = actionId;
        this.description = description;
    }
    
    // Getter方法
    public int getActionId() { return actionId; }
    public String getDescription() { return description; }
}
```

### 连续MDP

对于连续状态和动作空间的MDP，通常使用函数近似来表示转移概率和奖励函数：

```java
/**
 * 连续马尔可夫决策过程（函数近似版本）
 */
public class ContinuousMDP {
    private double discountFactor;
    private StateTransitionFunction transitionFunction;
    private RewardFunction rewardFunction;
    
    /**
     * 构造函数
     */
    public ContinuousMDP(double discountFactor, 
                        StateTransitionFunction transitionFunction,
                        RewardFunction rewardFunction) {
        this.discountFactor = discountFactor;
        this.transitionFunction = transitionFunction;
        this.rewardFunction = rewardFunction;
    }
    
    /**
     * 获取下一个状态（随机采样）
     */
    public double[] getNextState(double[] state, double[] action) {
        return transitionFunction.sampleNextState(state, action);
    }
    
    /**
     * 获取期望奖励
     */
    public double getExpectedReward(double[] state, double[] action) {
        return rewardFunction.computeReward(state, action);
    }
    
    /**
     * 获取折扣因子
     */
    public double getDiscountFactor() {
        return discountFactor;
    }
}

/**
 * 状态转移函数接口
 */
interface StateTransitionFunction {
    /**
     * 计算下一个状态的期望值
     */
    double[] computeExpectedNextState(double[] state, double[] action);
    
    /**
     * 采样下一个状态（考虑随机性）
     */
    double[] sampleNextState(double[] state, double[] action);
    
    /**
     * 计算转移概率密度
     */
    double computeTransitionDensity(double[] state, double[] action, double[] nextState);
}

/**
 * 奖励函数接口
 */
interface RewardFunction {
    /**
     * 计算奖励
     */
    double computeReward(double[] state, double[] action);
    
    /**
     * 计算奖励梯度
     */
    double[] computeRewardGradient(double[] state, double[] action);
}
```

## 10.1.3 马尔可夫性质

马尔可夫性质是MDP的核心假设，它表明系统的下一个状态只依赖于当前状态和动作，而与历史状态无关。

### 数学表达

马尔可夫性质可以表示为：
$$P(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},...,s_0,a_0) = P(s_{t+1}|s_t,a_t)$$

这意味着给定当前状态和动作，未来状态的分布与过去的状态和动作无关。

### 实际意义

马尔可夫性质简化了问题建模，使得我们可以用有限的当前信息来描述系统的完整状态。在实际应用中，如果系统不满足严格的马尔可夫性质，可以通过扩展状态表示来近似满足。

## 10.1.4 策略定义

策略（Policy）定义了智能体在给定状态下选择动作的规则。策略可以是确定性的或随机性的。

### 确定性策略

确定性策略π是一个从状态到动作的映射：
$$\pi: S \rightarrow A$$
$$a_t = \pi(s_t)$$

```java
/**
 * 确定性策略
 */
public class DeterministicPolicy {
    private int[] policy; // policy[s] = a，表示状态s下选择动作a
    
    /**
     * 构造函数
     */
    public DeterministicPolicy(int numStates, int numActions) {
        this.policy = new int[numStates];
        // 初始化为随机策略
        Random random = new Random(42);
        for (int s = 0; s < numStates; s++) {
            this.policy[s] = random.nextInt(numActions);
        }
    }
    
    /**
     * 获取指定状态下的动作
     */
    public int getAction(int state) {
        if (state >= 0 && state < policy.length) {
            return policy[state];
        }
        return 0; // 默认动作
    }
    
    /**
     * 设置策略
     */
    public void setPolicy(int state, int action) {
        if (state >= 0 && state < policy.length) {
            policy[state] = action;
        }
    }
    
    /**
     * 获取完整策略数组
     */
    public int[] getPolicy() {
        return policy.clone();
    }
}
```

### 随机性策略

随机性策略π是一个条件概率分布：
$$\pi(a|s) = P(a_t = a|s_t = s)$$

```java
/**
 * 随机性策略
 */
public class StochasticPolicy {
    private double[][] policy; // policy[s][a] = P(a|s)
    
    /**
     * 构造函数
     */
    public StochasticPolicy(int numStates, int numActions) {
        this.policy = new double[numStates][numActions];
        // 初始化为均匀随机策略
        Random random = new Random(42);
        for (int s = 0; s < numStates; s++) {
            double sum = 0;
            for (int a = 0; a < numActions; a++) {
                policy[s][a] = random.nextDouble();
                sum += policy[s][a];
            }
            // 归一化
            for (int a = 0; a < numActions; a++) {
                policy[s][a] /= sum;
            }
        }
    }
    
    /**
     * 获取指定状态下各动作的概率分布
     */
    public double[] getActionProbabilities(int state) {
        if (state >= 0 && state < policy.length) {
            return policy[state].clone();
        }
        return new double[policy[0].length];
    }
    
    /**
     * 根据策略采样动作
     */
    public int sampleAction(int state) {
        if (state >= 0 && state < policy.length) {
            double[] probabilities = policy[state];
            double randomValue = Math.random();
            double cumulative = 0;
            
            for (int a = 0; a < probabilities.length; a++) {
                cumulative += probabilities[a];
                if (randomValue <= cumulative) {
                    return a;
                }
            }
        }
        return 0; // 默认动作
    }
    
    /**
     * 设置策略概率
     */
    public void setActionProbability(int state, int action, double probability) {
        if (state >= 0 && state < policy.length && 
            action >= 0 && action < policy[state].length) {
            policy[state][action] = probability;
        }
    }
    
    /**
     * 归一化策略
     */
    public void normalizePolicy(int state) {
        if (state >= 0 && state < policy.length) {
            double sum = Arrays.stream(policy[state]).sum();
            if (sum > 0) {
                for (int a = 0; a < policy[state].length; a++) {
                    policy[state][a] /= sum;
                }
            }
        }
    }
}
```

## 10.1.5 价值函数

价值函数用于衡量状态或状态-动作对的长期价值，是强化学习的核心概念。

### 状态价值函数

状态价值函数V_π(s)表示在策略π下从状态s开始的期望累积折扣奖励：
$$V_\pi(s) = \mathbb{E}_\pi[G_t|S_t = s]$$

其中G_t是累积折扣奖励：
$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

```java
/**
 * 状态价值函数计算器
 */
public class StateValueFunction {
    private double[] values; // values[s] = V(s)
    private DiscreteMDP mdp;
    private StochasticPolicy policy;
    
    /**
     * 构造函数
     */
    public StateValueFunction(DiscreteMDP mdp, StochasticPolicy policy) {
        this.mdp = mdp;
        this.policy = policy;
        this.values = new double[mdp.getNumStates()];
    }
    
    /**
     * 计算状态价值（单次迭代）
     */
    public double computeStateValue(int state) {
        double value = 0.0;
        
        // 对所有可能的动作求和
        for (int a = 0; a < mdp.getNumActions(); a++) {
            double actionProb = policy.getActionProbabilities(state)[a];
            if (actionProb > 0) {
                double actionValue = 0.0;
                
                // 对所有可能的下一个状态求和
                for (int sNext = 0; sNext < mdp.getNumStates(); sNext++) {
                    double transProb = mdp.getTransitionProbability(state, a, sNext);
                    if (transProb > 0) {
                        double reward = mdp.getReward(state, a);
                        double nextValue = values[sNext];
                        actionValue += transProb * (reward + mdp.getDiscountFactor() * nextValue);
                    }
                }
                
                value += actionProb * actionValue;
            }
        }
        
        return value;
    }
    
    /**
     * 策略评估：迭代计算所有状态的价值
     */
    public void policyEvaluation(int maxIterations, double threshold) {
        double[] newValues = new double[mdp.getNumStates()];
        
        for (int iter = 0; iter < maxIterations; iter++) {
            double maxDelta = 0.0;
            
            // 更新所有状态的价值
            for (int s = 0; s < mdp.getNumStates(); s++) {
                double oldValue = values[s];
                newValues[s] = computeStateValue(s);
                maxDelta = Math.max(maxDelta, Math.abs(newValues[s] - oldValue));
            }
            
            // 更新价值函数
            System.arraycopy(newValues, 0, values, 0, values.length);
            
            // 检查收敛
            if (maxDelta < threshold) {
                System.out.printf("策略评估在第 %d 次迭代后收敛，最大变化: %.6f%n", iter + 1, maxDelta);
                break;
            }
            
            if (iter % 100 == 0) {
                System.out.printf("策略评估迭代 %d，最大变化: %.6f%n", iter + 1, maxDelta);
            }
        }
    }
    
    /**
     * 获取状态价值
     */
    public double getStateValue(int state) {
        if (state >= 0 && state < values.length) {
            return values[state];
        }
        return 0.0;
    }
    
    /**
     * 设置状态价值
     */
    public void setStateValue(int state, double value) {
        if (state >= 0 && state < values.length) {
            values[state] = value;
        }
    }
    
    /**
     * 获取所有状态价值
     */
    public double[] getAllStateValues() {
        return values.clone();
    }
}
```

### 动作价值函数

动作价值函数Q_π(s,a)表示在策略π下从状态s执行动作a开始的期望累积折扣奖励：
$$Q_\pi(s,a) = \mathbb{E}_\pi[G_t|S_t = s, A_t = a]$$

```java
/**
 * 动作价值函数计算器
 */
public class ActionValueFunction {
    private double[][] values; // values[s][a] = Q(s,a)
    private DiscreteMDP mdp;
    private StochasticPolicy policy;
    
    /**
     * 构造函数
     */
    public ActionValueFunction(DiscreteMDP mdp, StochasticPolicy policy) {
        this.mdp = mdp;
        this.policy = policy;
        this.values = new double[mdp.getNumStates()][mdp.getNumActions()];
    }
    
    /**
     * 计算动作价值（单次迭代）
     */
    public double computeActionValue(int state, int action) {
        double value = 0.0;
        
        // 对所有可能的下一个状态求和
        for (int sNext = 0; sNext < mdp.getNumStates(); sNext++) {
            double transProb = mdp.getTransitionProbability(state, action, sNext);
            if (transProb > 0) {
                double reward = mdp.getReward(state, action);
                
                // 计算下一个状态的价值（基于策略）
                double nextValue = 0.0;
                double[] nextActionProbs = policy.getActionProbabilities(sNext);
                for (int aNext = 0; aNext < mdp.getNumActions(); aNext++) {
                    nextValue += nextActionProbs[aNext] * values[sNext][aNext];
                }
                
                value += transProb * (reward + mdp.getDiscountFactor() * nextValue);
            }
        }
        
        return value;
    }
    
    /**
     * 策略评估：迭代计算所有状态-动作对的价值
     */
    public void policyEvaluation(int maxIterations, double threshold) {
        double[][] newValues = new double[mdp.getNumStates()][mdp.getNumActions()];
        
        for (int iter = 0; iter < maxIterations; iter++) {
            double maxDelta = 0.0;
            
            // 更新所有状态-动作对的价值
            for (int s = 0; s < mdp.getNumStates(); s++) {
                for (int a = 0; a < mdp.getNumActions(); a++) {
                    double oldValue = values[s][a];
                    newValues[s][a] = computeActionValue(s, a);
                    maxDelta = Math.max(maxDelta, Math.abs(newValues[s][a] - oldValue));
                }
            }
            
            // 更新价值函数
            for (int s = 0; s < mdp.getNumStates(); s++) {
                System.arraycopy(newValues[s], 0, values[s], 0, values[s].length);
            }
            
            // 检查收敛
            if (maxDelta < threshold) {
                System.out.printf("动作价值函数评估在第 %d 次迭代后收敛，最大变化: %.6f%n", iter + 1, maxDelta);
                break;
            }
            
            if (iter % 100 == 0) {
                System.out.printf("动作价值函数评估迭代 %d，最大变化: %.6f%n", iter + 1, maxDelta);
            }
        }
    }
    
    /**
     * 获取动作价值
     */
    public double getActionValue(int state, int action) {
        if (state >= 0 && state < values.length && 
            action >= 0 && action < values[state].length) {
            return values[state][action];
        }
        return 0.0;
    }
    
    /**
     * 设置动作价值
     */
    public void setActionValue(int state, int action, double value) {
        if (state >= 0 && state < values.length && 
            action >= 0 && action < values[state].length) {
            values[state][action] = value;
        }
    }
    
    /**
     * 获取所有动作价值
     */
    public double[][] getAllActionValues() {
        double[][] result = new double[values.length][];
        for (int i = 0; i < values.length; i++) {
            result[i] = values[i].clone();
        }
        return result;
    }
}
```

## 10.1.6 贝尔曼方程

贝尔曼方程是动态规划和强化学习的基础，它建立了当前状态价值与后续状态价值之间的关系。

### 状态价值函数的贝尔曼方程

$$V_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V_\pi(s')]$$

### 动作价值函数的贝尔曼方程

$$Q_\pi(s,a) = \sum_{s',r} p(s',r|s,a)[r + \gamma \sum_{a'} \pi(a'|s') Q_\pi(s',a')]$$

### 贝尔曼最优方程

对于最优价值函数，贝尔曼方程变为：

$$V_*(s) = \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V_*(s')]$$

$$Q_*(s,a) = \sum_{s',r} p(s',r|s,a)[r + \gamma \max_{a'} Q_*(s',a')]$$

```java
/**
 * 贝尔曼方程求解器
 */
public class BellmanSolver {
    private DiscreteMDP mdp;
    
    /**
     * 构造函数
     */
    public BellmanSolver(DiscreteMDP mdp) {
        this.mdp = mdp;
    }
    
    /**
     * 求解状态价值函数的贝尔曼方程（策略评估）
     */
    public double[] solveStateValueBellman(StochasticPolicy policy, int maxIterations, double threshold) {
        double[] values = new double[mdp.getNumStates()];
        double[] newValues = new double[mdp.getNumStates()];
        
        for (int iter = 0; iter < maxIterations; iter++) {
            double maxDelta = 0.0;
            
            // 更新所有状态的价值
            for (int s = 0; s < mdp.getNumStates(); s++) {
                newValues[s] = 0.0;
                
                // 对所有可能的动作求和
                double[] actionProbs = policy.getActionProbabilities(s);
                for (int a = 0; a < mdp.getNumActions(); a++) {
                    double actionProb = actionProbs[a];
                    if (actionProb > 0) {
                        double actionValue = 0.0;
                        
                        // 对所有可能的下一个状态求和
                        for (int sNext = 0; sNext < mdp.getNumStates(); sNext++) {
                            double transProb = mdp.getTransitionProbability(s, a, sNext);
                            if (transProb > 0) {
                                double reward = mdp.getReward(s, a);
                                actionValue += transProb * (reward + mdp.getDiscountFactor() * values[sNext]);
                            }
                        }
                        
                        newValues[s] += actionProb * actionValue;
                    }
                }
                
                maxDelta = Math.max(maxDelta, Math.abs(newValues[s] - values[s]));
            }
            
            // 更新价值函数
            System.arraycopy(newValues, 0, values, 0, values.length);
            
            // 检查收敛
            if (maxDelta < threshold) {
                System.out.printf("贝尔曼方程求解在第 %d 次迭代后收敛%n", iter + 1);
                break;
            }
        }
        
        return values;
    }
    
    /**
     * 求解贝尔曼最优方程（价值迭代）
     */
    public double[] solveOptimalStateValue(int maxIterations, double threshold) {
        double[] values = new double[mdp.getNumStates()];
        double[] newValues = new double[mdp.getNumStates()];
        
        for (int iter = 0; iter < maxIterations; iter++) {
            double maxDelta = 0.0;
            
            // 更新所有状态的价值
            for (int s = 0; s < mdp.getNumStates(); s++) {
                double maxValue = Double.NEGATIVE_INFINITY;
                
                // 对所有可能的动作求最大值
                for (int a = 0; a < mdp.getNumActions(); a++) {
                    double actionValue = 0.0;
                    
                    // 对所有可能的下一个状态求和
                    for (int sNext = 0; sNext < mdp.getNumStates(); sNext++) {
                        double transProb = mdp.getTransitionProbability(s, a, sNext);
                        if (transProb > 0) {
                            double reward = mdp.getReward(s, a);
                            actionValue += transProb * (reward + mdp.getDiscountFactor() * values[sNext]);
                        }
                    }
                    
                    maxValue = Math.max(maxValue, actionValue);
                }
                
                newValues[s] = maxValue;
                maxDelta = Math.max(maxDelta, Math.abs(newValues[s] - values[s]));
            }
            
            // 更新价值函数
            System.arraycopy(newValues, 0, values, 0, values.length);
            
            // 检查收敛
            if (maxDelta < threshold) {
                System.out.printf("最优贝尔曼方程求解在第 %d 次迭代后收敛%n", iter + 1);
                break;
            }
        }
        
        return values;
    }
    
    /**
     * 从最优价值函数推导最优策略
     */
    public DeterministicPolicy deriveOptimalPolicy(double[] optimalValues) {
        DeterministicPolicy policy = new DeterministicPolicy(mdp.getNumStates(), mdp.getNumActions());
        
        for (int s = 0; s < mdp.getNumStates(); s++) {
            int bestAction = 0;
            double bestValue = Double.NEGATIVE_INFINITY;
            
            // 寻找最优动作
            for (int a = 0; a < mdp.getNumActions(); a++) {
                double actionValue = 0.0;
                
                // 计算动作价值
                for (int sNext = 0; sNext < mdp.getNumStates(); sNext++) {
                    double transProb = mdp.getTransitionProbability(s, a, sNext);
                    if (transProb > 0) {
                        double reward = mdp.getReward(s, a);
                        actionValue += transProb * (reward + mdp.getDiscountFactor() * optimalValues[sNext]);
                    }
                }
                
                if (actionValue > bestValue) {
                    bestValue = actionValue;
                    bestAction = a;
                }
            }
            
            policy.setPolicy(s, bestAction);
        }
        
        return policy;
    }
}
```

## 10.1.7 探索与利用的权衡

在强化学习中，智能体需要在探索（尝试新动作以发现更好的策略）和利用（使用已知的最佳动作）之间进行权衡。

### ε-贪心策略

ε-贪心策略是一种常用的平衡探索与利用的方法：

```java
/**
 * ε-贪心策略
 */
public class EpsilonGreedyPolicy {
    private double[][] qValues; // Q值表
    private double epsilon;     // 探索概率
    private Random random;
    
    /**
     * 构造函数
     */
    public EpsilonGreedyPolicy(double[][] qValues, double epsilon) {
        this.qValues = qValues;
        this.epsilon = epsilon;
        this.random = new Random(42);
    }
    
    /**
     * 根据ε-贪心策略选择动作
     */
    public int selectAction(int state) {
        if (state < 0 || state >= qValues.length) {
            return 0;
        }
        
        // ε概率随机探索
        if (random.nextDouble() < epsilon) {
            return random.nextInt(qValues[state].length);
        }
        
        // 1-ε概率贪婪利用
        int bestAction = 0;
        double bestValue = qValues[state][0];
        
        for (int a = 1; a < qValues[state].length; a++) {
            if (qValues[state][a] > bestValue) {
                bestValue = qValues[state][a];
                bestAction = a;
            }
        }
        
        return bestAction;
    }
    
    /**
     * 衰减ε值（随时间减少探索）
     */
    public void decayEpsilon(double decayRate) {
        this.epsilon = Math.max(0.01, this.epsilon * decayRate); // 最小探索概率为0.01
    }
    
    /**
     * 获取当前ε值
     */
    public double getEpsilon() {
        return epsilon;
    }
    
    /**
     * 设置ε值
     */
    public void setEpsilon(double epsilon) {
        this.epsilon = Math.max(0.0, Math.min(1.0, epsilon)); // 限制在[0,1]范围内
    }
}
```

## 10.1.8 完整示例：简单的网格世界

让我们通过一个具体的例子来演示MDP的实现和求解：

```java
/**
 * 网格世界环境示例
 */
public class GridWorldExample {
    public static void main(String[] args) {
        System.out.println("=== 网格世界MDP示例 ===");
        
        // 创建4x4的网格世界
        int gridSize = 4;
        int numStates = gridSize * gridSize;
        int numActions = 4; // 上、下、左、右
        
        // 创建MDP
        DiscreteMDP mdp = new DiscreteMDP(numStates, numActions, 0.9);
        
        // 定义特殊状态
        int goalState = 15; // 右下角为目标
        int obstacleState = 10; // 障碍物
        
        // 设置转移概率和奖励
        setupGridWorld(mdp, gridSize, goalState, obstacleState);
        
        // 验证转移概率
        if (mdp.validateTransitionProbabilities()) {
            System.out.println("转移概率验证通过");
        }
        
        // 创建初始策略（随机策略）
        StochasticPolicy initialPolicy = new StochasticPolicy(numStates, numActions);
        System.out.println("初始策略创建完成");
        
        // 策略评估
        System.out.println("\n=== 策略评估 ===");
        StateValueFunction stateValueFunction = new StateValueFunction(mdp, initialPolicy);
        stateValueFunction.policyEvaluation(1000, 1e-6);
        
        // 显示价值函数
        displayValueFunction(stateValueFunction, gridSize);
        
        // 贝尔曼方程求解
        System.out.println("\n=== 贝尔曼方程求解 ===");
        BellmanSolver solver = new BellmanSolver(mdp);
        double[] optimalValues = solver.solveOptimalStateValue(1000, 1e-6);
        
        // 从最优价值函数推导最优策略
        DeterministicPolicy optimalPolicy = solver.deriveOptimalPolicy(optimalValues);
        
        // 显示最优策略
        displayOptimalPolicy(optimalPolicy, gridSize);
        
        // 比较初始策略和最优策略的价值
        System.out.println("\n=== 策略比较 ===");
        comparePolicies(mdp, initialPolicy, optimalPolicy, gridSize);
    }
    
    /**
     * 设置网格世界的转移概率和奖励
     */
    private static void setupGridWorld(DiscreteMDP mdp, int gridSize, int goalState, int obstacleState) {
        int numStates = gridSize * gridSize;
        int numActions = 4;
        
        // 动作定义：0-上，1-下，2-左，3-右
        int[][] actions = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}};
        
        for (int s = 0; s < numStates; s++) {
            int row = s / gridSize;
            int col = s % gridSize;
            
            // 目标状态和障碍物状态的特殊处理
            if (s == goalState) {
                // 目标状态：任何动作都保持在目标状态，获得+10奖励
                for (int a = 0; a < numActions; a++) {
                    mdp.setTransitionProbability(s, a, s, 1.0);
                    mdp.setReward(s, a, 10.0);
                }
                continue;
            }
            
            if (s == obstacleState) {
                // 障碍物状态：任何动作都保持在障碍物状态，获得-10奖励
                for (int a = 0; a < numActions; a++) {
                    mdp.setTransitionProbability(s, a, s, 1.0);
                    mdp.setReward(s, a, -10.0);
                }
                continue;
            }
            
            // 普通状态
            for (int a = 0; a < numActions; a++) {
                int newRow = row + actions[a][0];
                int newCol = col + actions[a][1];
                
                // 检查边界
                if (newRow < 0 || newRow >= gridSize || newCol < 0 || newCol >= gridSize) {
                    // 边界外：保持原位置，获得-1奖励
                    mdp.setTransitionProbability(s, a, s, 1.0);
                    mdp.setReward(s, a, -1.0);
                } else {
                    // 有效移动：移动到新位置
                    int newState = newRow * gridSize + newCol;
                    mdp.setTransitionProbability(s, a, newState, 1.0);
                    
                    // 根据新位置设置奖励
                    if (newState == goalState) {
                        mdp.setReward(s, a, 10.0); // 到达目标
                    } else if (newState == obstacleState) {
                        mdp.setReward(s, a, -10.0); // 撞到障碍物
                    } else {
                        mdp.setReward(s, a, -1.0); // 普通移动
                    }
                }
            }
        }
    }
    
    /**
     * 显示价值函数
     */
    private static void displayValueFunction(StateValueFunction valueFunction, int gridSize) {
        System.out.println("状态价值函数:");
        for (int row = 0; row < gridSize; row++) {
            for (int col = 0; col < gridSize; col++) {
                int state = row * gridSize + col;
                System.out.printf("%7.2f ", valueFunction.getStateValue(state));
            }
            System.out.println();
        }
    }
    
    /**
     * 显示最优策略
     */
    private static void displayOptimalPolicy(DeterministicPolicy policy, int gridSize) {
        String[] actionSymbols = {"↑", "↓", "←", "→"};
        System.out.println("最优策略:");
        for (int row = 0; row < gridSize; row++) {
            for (int col = 0; col < gridSize; col++) {
                int state = row * gridSize + col;
                int action = policy.getAction(state);
                System.out.printf("%2s ", actionSymbols[action]);
            }
            System.out.println();
        }
    }
    
    /**
     * 比较策略
     */
    private static void comparePolicies(DiscreteMDP mdp, StochasticPolicy initialPolicy, 
                                      DeterministicPolicy optimalPolicy, int gridSize) {
        BellmanSolver solver = new BellmanSolver(mdp);
        
        // 计算初始策略的价值
        double[] initialValues = solver.solveStateValueBellman(initialPolicy, 1000, 1e-6);
        
        // 计算最优策略的价值
        double[] optimalValues = solver.solveOptimalStateValue(1000, 1e-6);
        
        System.out.println("状态\t初始策略价值\t最优策略价值\t改进");
        for (int s = 0; s < initialValues.length; s++) {
            double improvement = optimalValues[s] - initialValues[s];
            System.out.printf("%d\t%7.2f\t\t%7.2f\t\t%7.2f%n", 
                s, initialValues[s], optimalValues[s], improvement);
        }
    }
}
```

## 本节小结

在本节中，我们深入学习了马尔可夫决策过程这一强化学习的数学基础：

1. **MDP定义**：理解了MDP的五个基本要素及其数学表示
2. **马尔可夫性质**：掌握了马尔可夫性质的意义和应用
3. **策略概念**：学习了确定性和随机性策略的定义和实现
4. **价值函数**：掌握了状态价值函数和动作价值函数的概念和计算方法
5. **贝尔曼方程**：理解了贝尔曼方程的数学原理和求解方法
6. **探索与利用**：学习了ε-贪心策略在平衡探索与利用中的应用

通过Java代码实现，我们不仅掌握了理论知识，还获得了实际的编程经验。这些基础概念是后续学习强化学习算法的重要前提。

## 下一步计划

在下一节中，我们将学习价值函数的计算方法和策略梯度技术，进一步深入强化学习的核心算法。