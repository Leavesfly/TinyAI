# 10.2 价值函数与策略梯度

> **本节学习目标**：掌握价值函数计算和策略优化方法，理解蒙特卡洛方法、时序差分学习和策略梯度定理

## 内容概览

在上一节中，我们学习了马尔可夫决策过程的数学基础。本节将深入探讨价值函数的计算方法和策略优化技术。我们将学习蒙特卡洛方法和时序差分学习这两种重要的价值函数估计方法，以及策略梯度这一直接优化策略的技术。

## 10.2.1 价值函数估计方法概述

价值函数估计是强化学习的核心问题之一。由于环境的转移概率和奖励函数通常是未知的，我们需要通过与环境的交互来估计价值函数。

### 价值函数估计的挑战

1. **模型未知**：环境的转移概率和奖励函数未知
2. **样本有限**：只能通过有限的交互样本进行估计
3. **方差问题**：不同估计方法具有不同的方差特性
4. **偏差问题**：估计方法可能引入偏差

### 估计方法分类

价值函数估计方法主要分为两类：

1. **基于模型的方法**：先学习环境模型，再计算价值函数
2. **无模型的方法**：直接从经验数据估计价值函数

## 10.2.2 蒙特卡洛方法

蒙特卡洛方法通过完整的轨迹样本来估计价值函数，是一种无模型的估计方法。

### 基本原理

蒙特卡洛方法基于大数定律，通过多次采样来估计期望值：
$$V_\pi(s) = \mathbb{E}_\pi[G_t|S_t = s] \approx \frac{1}{N} \sum_{i=1}^{N} G_t^i$$

其中G_t^i是第i次访问状态s时的回报。

### 首次访问蒙特卡洛方法

```java
/**
 * 首次访问蒙特卡洛价值函数估计器
 */
public class FirstVisitMonteCarlo {
    private int numStates;
    private double discountFactor;
    private Map<Integer, List<Double>> returns; // 每个状态的回报列表
    private Map<Integer, Integer> visitCounts;  // 每个状态的访问次数
    
    /**
     * 构造函数
     */
    public FirstVisitMonteCarlo(int numStates, double discountFactor) {
        this.numStates = numStates;
        this.discountFactor = discountFactor;
        this.returns = new HashMap<>();
        this.visitCounts = new HashMap<>();
        
        // 初始化数据结构
        for (int s = 0; s < numStates; s++) {
            returns.put(s, new ArrayList<>());
            visitCounts.put(s, 0);
        }
    }
    
    /**
     * 更新价值函数估计（首次访问）
     */
    public void updateValueFunction(List<EpisodeStep> episode) {
        // 记录每个状态是否在本轮中已访问
        Set<Integer> visitedStates = new HashSet<>();
        
        // 从后往前计算回报
        double G = 0.0;
        for (int t = episode.size() - 1; t >= 0; t--) {
            EpisodeStep step = episode.get(t);
            int state = step.getState();
            double reward = step.getReward();
            
            // 计算回报
            G = reward + discountFactor * G;
            
            // 如果是首次访问该状态
            if (!visitedStates.contains(state)) {
                visitedStates.add(state);
                
                // 记录回报
                returns.get(state).add(G);
                visitCounts.put(state, visitCounts.get(state) + 1);
            }
        }
    }
    
    /**
     * 获取状态价值估计
     */
    public double getValueEstimate(int state) {
        List<Double> stateReturns = returns.get(state);
        if (stateReturns.isEmpty()) {
            return 0.0;
        }
        
        // 计算平均回报
        double sum = 0.0;
        for (double ret : stateReturns) {
            sum += ret;
        }
        return sum / stateReturns.size();
    }
    
    /**
     * 获取所有状态的价值估计
     */
    public double[] getAllValueEstimates() {
        double[] values = new double[numStates];
        for (int s = 0; s < numStates; s++) {
            values[s] = getValueEstimate(s);
        }
        return values;
    }
    
    /**
     * 获取访问次数
     */
    public int getVisitCount(int state) {
        return visitCounts.getOrDefault(state, 0);
    }
    
    /**
     * 重置估计器
     */
    public void reset() {
        for (int s = 0; s < numStates; s++) {
            returns.get(s).clear();
            visitCounts.put(s, 0);
        }
    }
}

/**
 * 轨迹步骤
 */
class EpisodeStep {
    private int state;
    private int action;
    private double reward;
    
    public EpisodeStep(int state, int action, double reward) {
        this.state = state;
        this.action = action;
        this.reward = reward;
    }
    
    // Getter方法
    public int getState() { return state; }
    public int getAction() { return action; }
    public double getReward() { return reward; }
}
```

### 每次访问蒙特卡洛方法

```java
/**
 * 每次访问蒙特卡洛价值函数估计器
 */
public class EveryVisitMonteCarlo {
    private int numStates;
    private double discountFactor;
    private Map<Integer, List<Double>> returns; // 每个状态的回报列表
    private Map<Integer, Integer> visitCounts;  // 每个状态的访问次数
    
    /**
     * 构造函数
     */
    public EveryVisitMonteCarlo(int numStates, double discountFactor) {
        this.numStates = numStates;
        this.discountFactor = discountFactor;
        this.returns = new HashMap<>();
        this.visitCounts = new HashMap<>();
        
        // 初始化数据结构
        for (int s = 0; s < numStates; s++) {
            returns.put(s, new ArrayList<>());
            visitCounts.put(s, 0);
        }
    }
    
    /**
     * 更新价值函数估计（每次访问）
     */
    public void updateValueFunction(List<EpisodeStep> episode) {
        // 从后往前计算回报
        double[] G = new double[episode.size()];
        G[episode.size() - 1] = episode.get(episode.size() - 1).getReward();
        
        // 计算每个时间步的回报
        for (int t = episode.size() - 2; t >= 0; t--) {
            EpisodeStep step = episode.get(t);
            EpisodeStep nextStep = episode.get(t + 1);
            G[t] = step.getReward() + discountFactor * G[t + 1];
        }
        
        // 记录所有状态的回报
        for (int t = 0; t < episode.size(); t++) {
            int state = episode.get(t).getState();
            returns.get(state).add(G[t]);
            visitCounts.put(state, visitCounts.get(state) + 1);
        }
    }
    
    /**
     * 获取状态价值估计
     */
    public double getValueEstimate(int state) {
        List<Double> stateReturns = returns.get(state);
        if (stateReturns.isEmpty()) {
            return 0.0;
        }
        
        // 计算平均回报
        double sum = 0.0;
        for (double ret : stateReturns) {
            sum += ret;
        }
        return sum / stateReturns.size();
    }
    
    /**
     * 获取所有状态的价值估计
     */
    public double[] getAllValueEstimates() {
        double[] values = new double[numStates];
        for (int s = 0; s < numStates; s++) {
            values[s] = getValueEstimate(s);
        }
        return values;
    }
    
    /**
     * 获取访问次数
     */
    public int getVisitCount(int state) {
        return visitCounts.getOrDefault(state, 0);
    }
    
    /**
     * 重置估计器
     */
    public void reset() {
        for (int s = 0; s < numStates; s++) {
            returns.get(s).clear();
            visitCounts.put(s, 0);
        }
    }
}
```

### 增量蒙特卡洛方法

```java
/**
 * 增量蒙特卡洛价值函数估计器
 */
public class IncrementalMonteCarlo {
    private int numStates;
    private double discountFactor;
    private double[] valueEstimates;  // 价值估计
    private int[] visitCounts;        // 访问次数
    
    /**
     * 构造函数
     */
    public IncrementalMonteCarlo(int numStates, double discountFactor) {
        this.numStates = numStates;
        this.discountFactor = discountFactor;
        this.valueEstimates = new double[numStates];
        this.visitCounts = new int[numStates];
    }
    
    /**
     * 增量更新价值函数估计
     */
    public void updateValueFunction(List<EpisodeStep> episode) {
        // 记录每个状态是否在本轮中已访问（用于首次访问）
        Set<Integer> visitedStates = new HashSet<>();
        
        // 从后往前计算回报
        double G = 0.0;
        for (int t = episode.size() - 1; t >= 0; t--) {
            EpisodeStep step = episode.get(t);
            int state = step.getState();
            double reward = step.getReward();
            
            // 计算回报
            G = reward + discountFactor * G;
            
            // 如果是首次访问该状态
            if (!visitedStates.contains(state)) {
                visitedStates.add(state);
                
                // 增量更新价值估计
                visitCounts[state]++;
                double oldEstimate = valueEstimates[state];
                valueEstimates[state] = oldEstimate + (G - oldEstimate) / visitCounts[state];
            }
        }
    }
    
    /**
     * 获取状态价值估计
     */
    public double getValueEstimate(int state) {
        return valueEstimates[state];
    }
    
    /**
     * 获取所有状态的价值估计
     */
    public double[] getAllValueEstimates() {
        return valueEstimates.clone();
    }
    
    /**
     * 获取访问次数
     */
    public int getVisitCount(int state) {
        return visitCounts[state];
    }
    
    /**
     * 重置估计器
     */
    public void reset() {
        Arrays.fill(valueEstimates, 0.0);
        Arrays.fill(visitCounts, 0);
    }
}
```

## 10.2.3 时序差分学习

时序差分（Temporal Difference, TD）学习是一种结合了动态规划和蒙特卡洛方法的在线学习方法。

### TD(0)算法

TD(0)是最简单的时序差分算法，它使用下一个状态的价值估计来更新当前状态的价值：

$$V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$$

```java
/**
 * TD(0)价值函数估计器
 */
public class TD0 {
    private int numStates;
    private double discountFactor;
    private double learningRate;
    private double[] valueEstimates;
    
    /**
     * 构造函数
     */
    public TD0(int numStates, double discountFactor, double learningRate) {
        this.numStates = numStates;
        this.discountFactor = discountFactor;
        this.learningRate = learningRate;
        this.valueEstimates = new double[numStates];
    }
    
    /**
     * 更新价值函数估计（单步更新）
     */
    public void updateValueFunction(int state, double reward, int nextState, boolean isTerminal) {
        double currentEstimate = valueEstimates[state];
        double nextEstimate = isTerminal ? 0.0 : valueEstimates[nextState];
        
        // TD误差
        double tdError = reward + discountFactor * nextEstimate - currentEstimate;
        
        // 更新价值估计
        valueEstimates[state] = currentEstimate + learningRate * tdError;
    }
    
    /**
     * 批量更新价值函数估计
     */
    public void updateValueFunction(List<TDStep> steps) {
        for (TDStep step : steps) {
            updateValueFunction(step.getState(), step.getReward(), 
                              step.getNextState(), step.isTerminal());
        }
    }
    
    /**
     * 获取状态价值估计
     */
    public double getValueEstimate(int state) {
        return valueEstimates[state];
    }
    
    /**
     * 获取所有状态的价值估计
     */
    public double[] getAllValueEstimates() {
        return valueEstimates.clone();
    }
    
    /**
     * 设置学习率
     */
    public void setLearningRate(double learningRate) {
        this.learningRate = learningRate;
    }
    
    /**
     * 获取学习率
     */
    public double getLearningRate() {
        return learningRate;
    }
    
    /**
     * 重置估计器
     */
    public void reset() {
        Arrays.fill(valueEstimates, 0.0);
    }
}

/**
 * TD学习步骤
 */
class TDStep {
    private int state;
    private double reward;
    private int nextState;
    private boolean terminal;
    
    public TDStep(int state, double reward, int nextState, boolean terminal) {
        this.state = state;
        this.reward = reward;
        this.nextState = nextState;
        this.terminal = terminal;
    }
    
    // Getter方法
    public int getState() { return state; }
    public double getReward() { return reward; }
    public int getNextState() { return nextState; }
    public boolean isTerminal() { return terminal; }
}
```

### TD(λ)算法

TD(λ)通过引入资格迹（Eligibility Traces）来结合多步TD方法的优点：

```java
/**
 * TD(λ)价值函数估计器
 */
public class TDLambda {
    private int numStates;
    private double discountFactor;
    private double learningRate;
    private double lambda;
    private double[] valueEstimates;
    private double[] eligibilityTraces;
    
    /**
     * 构造函数
     */
    public TDLambda(int numStates, double discountFactor, double learningRate, double lambda) {
        this.numStates = numStates;
        this.discountFactor = discountFactor;
        this.learningRate = learningRate;
        this.lambda = lambda;
        this.valueEstimates = new double[numStates];
        this.eligibilityTraces = new double[numStates];
    }
    
    /**
     * 更新价值函数估计（带资格迹）
     */
    public void updateValueFunction(int state, double reward, int nextState, boolean isTerminal) {
        double currentEstimate = valueEstimates[state];
        double nextEstimate = isTerminal ? 0.0 : valueEstimates[nextState];
        
        // TD误差
        double tdError = reward + discountFactor * nextEstimate - currentEstimate;
        
        // 更新资格迹
        eligibilityTraces[state] += 1.0;
        
        // 更新所有状态的价值估计
        for (int s = 0; s < numStates; s++) {
            if (eligibilityTraces[s] > 0) {
                valueEstimates[s] += learningRate * tdError * eligibilityTraces[s];
                eligibilityTraces[s] *= discountFactor * lambda;
            }
        }
    }
    
    /**
     * 获取状态价值估计
     */
    public double getValueEstimate(int state) {
        return valueEstimates[state];
    }
    
    /**
     * 获取所有状态的价值估计
     */
    public double[] getAllValueEstimates() {
        return valueEstimates.clone();
    }
    
    /**
     * 获取资格迹
     */
    public double[] getEligibilityTraces() {
        return eligibilityTraces.clone();
    }
    
    /**
     * 重置资格迹
     */
    public void resetEligibilityTraces() {
        Arrays.fill(eligibilityTraces, 0.0);
    }
    
    /**
     * 重置估计器
     */
    public void reset() {
        Arrays.fill(valueEstimates, 0.0);
        Arrays.fill(eligibilityTraces, 0.0);
    }
}
```

## 10.2.4 动作价值函数估计

动作价值函数Q(s,a)表示在状态s执行动作a的长期价值，是许多强化学习算法的基础。

### Q函数的蒙特卡洛估计

```java
/**
 * 动作价值函数的蒙特卡洛估计器
 */
public class MonteCarloQFunction {
    private int numStates;
    private int numActions;
    private double discountFactor;
    private Map<StateActionPair, List<Double>> returns; // 状态-动作对的回报列表
    private Map<StateActionPair, Integer> visitCounts;  // 状态-动作对的访问次数
    
    /**
     * 构造函数
     */
    public MonteCarloQFunction(int numStates, int numActions, double discountFactor) {
        this.numStates = numStates;
        this.numActions = numActions;
        this.discountFactor = discountFactor;
        this.returns = new HashMap<>();
        this.visitCounts = new HashMap<>();
    }
    
    /**
     * 更新动作价值函数估计（首次访问）
     */
    public void updateQFunction(List<EpisodeStepWithAction> episode) {
        // 记录每个状态-动作对是否在本轮中已访问
        Set<StateActionPair> visitedPairs = new HashSet<>();
        
        // 从后往前计算回报
        double G = 0.0;
        for (int t = episode.size() - 1; t >= 0; t--) {
            EpisodeStepWithAction step = episode.get(t);
            int state = step.getState();
            int action = step.getAction();
            double reward = step.getReward();
            
            // 计算回报
            G = reward + discountFactor * G;
            
            // 如果是首次访问该状态-动作对
            StateActionPair pair = new StateActionPair(state, action);
            if (!visitedPairs.contains(pair)) {
                visitedPairs.add(pair);
                
                // 记录回报
                returns.computeIfAbsent(pair, k -> new ArrayList<>()).add(G);
                visitCounts.put(pair, visitCounts.getOrDefault(pair, 0) + 1);
            }
        }
    }
    
    /**
     * 获取动作价值估计
     */
    public double getQValue(int state, int action) {
        StateActionPair pair = new StateActionPair(state, action);
        List<Double> pairReturns = returns.get(pair);
        if (pairReturns == null || pairReturns.isEmpty()) {
            return 0.0;
        }
        
        // 计算平均回报
        double sum = 0.0;
        for (double ret : pairReturns) {
            sum += ret;
        }
        return sum / pairReturns.size();
    }
    
    /**
     * 获取状态的所有动作价值
     */
    public double[] getQValues(int state) {
        double[] qValues = new double[numActions];
        for (int a = 0; a < numActions; a++) {
            qValues[a] = getQValue(state, a);
        }
        return qValues;
    }
    
    /**
     * 获取访问次数
     */
    public int getVisitCount(int state, int action) {
        StateActionPair pair = new StateActionPair(state, action);
        return visitCounts.getOrDefault(pair, 0);
    }
    
    /**
     * 重置估计器
     */
    public void reset() {
        returns.clear();
        visitCounts.clear();
    }
}

/**
 * 状态-动作对
 */
class StateActionPair {
    private int state;
    private int action;
    
    public StateActionPair(int state, int action) {
        this.state = state;
        this.action = action;
    }
    
    @Override
    public boolean equals(Object obj) {
        if (this == obj) return true;
        if (obj == null || getClass() != obj.getClass()) return false;
        StateActionPair that = (StateActionPair) obj;
        return state == that.state && action == that.action;
    }
    
    @Override
    public int hashCode() {
        return Objects.hash(state, action);
    }
    
    // Getter方法
    public int getState() { return state; }
    public int getAction() { return action; }
}

/**
 * 带动作的轨迹步骤
 */
class EpisodeStepWithAction {
    private int state;
    private int action;
    private double reward;
    
    public EpisodeStepWithAction(int state, int action, double reward) {
        this.state = state;
        this.action = action;
        this.reward = reward;
    }
    
    // Getter方法
    public int getState() { return state; }
    public int getAction() { return action; }
    public double getReward() { return reward; }
}
```

### Q函数的TD学习

```java
/**
 * Q函数的TD学习器（Q-Learning基础）
 */
public class QFunctionTD {
    private int numStates;
    private int numActions;
    private double discountFactor;
    private double learningRate;
    private double[][] qValues;
    
    /**
     * 构造函数
     */
    public QFunctionTD(int numStates, int numActions, double discountFactor, double learningRate) {
        this.numStates = numStates;
        this.numActions = numActions;
        this.discountFactor = discountFactor;
        this.learningRate = learningRate;
        this.qValues = new double[numStates][numActions];
    }
    
    /**
     * 更新Q函数（Q-Learning更新规则）
     */
    public void updateQFunction(int state, int action, double reward, int nextState, boolean isTerminal) {
        double currentQ = qValues[state][action];
        double nextMaxQ = isTerminal ? 0.0 : Arrays.stream(qValues[nextState]).max().orElse(0.0);
        
        // Q-Learning更新规则
        double tdError = reward + discountFactor * nextMaxQ - currentQ;
        qValues[state][action] = currentQ + learningRate * tdError;
    }
    
    /**
     * 更新Q函数（SARSA更新规则）
     */
    public void updateQFunctionSARSA(int state, int action, double reward, int nextState, int nextAction, boolean isTerminal) {
        double currentQ = qValues[state][action];
        double nextQ = isTerminal ? 0.0 : qValues[nextState][nextAction];
        
        // SARSA更新规则
        double tdError = reward + discountFactor * nextQ - currentQ;
        qValues[state][action] = currentQ + learningRate * tdError;
    }
    
    /**
     * 获取Q值
     */
    public double getQValue(int state, int action) {
        return qValues[state][action];
    }
    
    /**
     * 获取状态的所有Q值
     */
    public double[] getQValues(int state) {
        return qValues[state].clone();
    }
    
    /**
     * 获取最优动作
     */
    public int getOptimalAction(int state) {
        return argmax(qValues[state]);
    }
    
    /**
     * 获取最大Q值
     */
    public double getMaxQValue(int state) {
        return Arrays.stream(qValues[state]).max().orElse(0.0);
    }
    
    /**
     * 获取所有Q值
     */
    public double[][] getAllQValues() {
        double[][] copy = new double[qValues.length][];
        for (int i = 0; i < qValues.length; i++) {
            copy[i] = qValues[i].clone();
        }
        return copy;
    }
    
    /**
     * 获取最大值索引
     */
    private int argmax(double[] array) {
        int maxIndex = 0;
        for (int i = 1; i < array.length; i++) {
            if (array[i] > array[maxIndex]) {
                maxIndex = i;
            }
        }
        return maxIndex;
    }
    
    /**
     * 设置学习率
     */
    public void setLearningRate(double learningRate) {
        this.learningRate = learningRate;
    }
    
    /**
     * 获取学习率
     */
    public double getLearningRate() {
        return learningRate;
    }
    
    /**
     * 重置Q函数
     */
    public void reset() {
        for (int s = 0; s < numStates; s++) {
            Arrays.fill(qValues[s], 0.0);
        }
    }
}
```

## 10.2.5 策略梯度方法

策略梯度方法直接优化策略参数，是一种基于策略的强化学习方法。

### 策略梯度定理

策略梯度定理表明策略性能的梯度可以表示为：

$$\nabla_\theta J(\theta) = \mathbb{E}_\pi[\nabla_\theta \log \pi_\theta(a|s) Q^\pi(s,a)]$$

### 简单策略梯度实现

```java
/**
 * 简单策略梯度实现
 */
public class SimplePolicyGradient {
    private int numStates;
    private int numActions;
    private double learningRate;
    private double[][] policyParams; // 策略参数（logits）
    private Random random;
    
    /**
     * 构造函数
     */
    public SimplePolicyGradient(int numStates, int numActions, double learningRate) {
        this.numStates = numStates;
        this.numActions = numActions;
        this.learningRate = learningRate;
        this.policyParams = new double[numStates][numActions];
        this.random = new Random(42);
        
        // 初始化策略参数
        initializeParameters();
    }
    
    /**
     * 初始化策略参数
     */
    private void initializeParameters() {
        for (int s = 0; s < numStates; s++) {
            for (int a = 0; a < numActions; a++) {
                policyParams[s][a] = random.nextGaussian() * 0.1;
            }
        }
    }
    
    /**
     * 计算策略概率（softmax）
     */
    public double[] getActionProbabilities(int state) {
        double[] logits = policyParams[state];
        double[] probs = new double[numActions];
        
        // 数值稳定性处理
        double maxLogit = Arrays.stream(logits).max().orElse(0);
        double sumExp = 0;
        for (int a = 0; a < numActions; a++) {
            probs[a] = Math.exp(logits[a] - maxLogit);
            sumExp += probs[a];
        }
        
        // 归一化
        for (int a = 0; a < numActions; a++) {
            probs[a] /= sumExp;
        }
        
        return probs;
    }
    
    /**
     * 根据策略采样动作
     */
    public int sampleAction(int state) {
        double[] probabilities = getActionProbabilities(state);
        double randomValue = random.nextDouble();
        double cumulative = 0;
        
        for (int a = 0; a < numActions; a++) {
            cumulative += probabilities[a];
            if (randomValue <= cumulative) {
                return a;
            }
        }
        return 0; // 默认动作
    }
    
    /**
     * 更新策略参数（REINFORCE算法）
     */
    public void updatePolicy(List<PolicyGradientStep> trajectory) {
        // 计算回报
        double[] returns = computeReturns(trajectory);
        
        // 基线（平均回报）
        double baseline = Arrays.stream(returns).average().orElse(0);
        
        // 更新每个步骤的策略参数
        for (int t = 0; t < trajectory.size(); t++) {
            PolicyGradientStep step = trajectory.get(t);
            int state = step.getState();
            int action = step.getAction();
            double advantage = returns[t] - baseline;
            
            // 计算策略梯度
            double[] probs = getActionProbabilities(state);
            for (int a = 0; a < numActions; a++) {
                double gradient = (a == action) ? (1 - probs[a]) : -probs[a];
                policyParams[state][a] += learningRate * advantage * gradient;
            }
        }
    }
    
    /**
     * 计算回报（从每个时间步开始的累积折扣奖励）
     */
    private double[] computeReturns(List<PolicyGradientStep> trajectory) {
        double[] returns = new double[trajectory.size()];
        double G = 0.0;
        
        // 从后往前计算回报
        for (int t = trajectory.size() - 1; t >= 0; t--) {
            G = trajectory.get(t).getReward() + 0.99 * G; // 假设折扣因子为0.99
            returns[t] = G;
        }
        
        return returns;
    }
    
    /**
     * 获取策略参数
     */
    public double[][] getPolicyParameters() {
        double[][] copy = new double[policyParams.length][];
        for (int i = 0; i < policyParams.length; i++) {
            copy[i] = policyParams[i].clone();
        }
        return copy;
    }
    
    /**
     * 设置策略参数
     */
    public void setPolicyParameters(double[][] params) {
        for (int s = 0; s < numStates && s < params.length; s++) {
            System.arraycopy(params[s], 0, policyParams[s], 0, 
                           Math.min(numActions, params[s].length));
        }
    }
    
    /**
     * 重置策略参数
     */
    public void reset() {
        initializeParameters();
    }
}

/**
 * 策略梯度步骤
 */
class PolicyGradientStep {
    private int state;
    private int action;
    private double reward;
    
    public PolicyGradientStep(int state, int action, double reward) {
        this.state = state;
        this.action = action;
        this.reward = reward;
    }
    
    // Getter方法
    public int getState() { return state; }
    public int getAction() { return action; }
    public double getReward() { return reward; }
}
```

### Actor-Critic方法

Actor-Critic方法结合了价值函数方法和策略梯度方法的优点：

```java
/**
 * Actor-Critic方法实现
 */
public class ActorCritic {
    private int numStates;
    private int numActions;
    private double actorLearningRate;
    private double criticLearningRate;
    private double[][] policyParams; // Actor：策略参数
    private double[] valueParams;    // Critic：价值参数
    private Random random;
    
    /**
     * 构造函数
     */
    public ActorCritic(int numStates, int numActions, double actorLearningRate, double criticLearningRate) {
        this.numStates = numStates;
        this.numActions = numActions;
        this.actorLearningRate = actorLearningRate;
        this.criticLearningRate = criticLearningRate;
        this.policyParams = new double[numStates][numActions];
        this.valueParams = new double[numStates];
        this.random = new Random(42);
        
        // 初始化参数
        initializeParameters();
    }
    
    /**
     * 初始化参数
     */
    private void initializeParameters() {
        for (int s = 0; s < numStates; s++) {
            for (int a = 0; a < numActions; a++) {
                policyParams[s][a] = random.nextGaussian() * 0.1;
            }
        }
    }
    
    /**
     * 计算策略概率（softmax）
     */
    public double[] getActionProbabilities(int state) {
        double[] logits = policyParams[state];
        double[] probs = new double[numActions];
        
        // 数值稳定性处理
        double maxLogit = Arrays.stream(logits).max().orElse(0);
        double sumExp = 0;
        for (int a = 0; a < numActions; a++) {
            probs[a] = Math.exp(logits[a] - maxLogit);
            sumExp += probs[a];
        }
        
        // 归一化
        for (int a = 0; a < numActions; a++) {
            probs[a] /= sumExp;
        }
        
        return probs;
    }
    
    /**
     * 根据策略采样动作
     */
    public int sampleAction(int state) {
        double[] probabilities = getActionProbabilities(state);
        double randomValue = random.nextDouble();
        double cumulative = 0;
        
        for (int a = 0; a < numActions; a++) {
            cumulative += probabilities[a];
            if (randomValue <= cumulative) {
                return a;
            }
        }
        return 0; // 默认动作
    }
    
    /**
     * 更新Actor和Critic
     */
    public void updateActorCritic(int state, int action, double reward, int nextState, boolean isTerminal) {
        // Critic更新：TD学习更新价值函数
        double currentValue = valueParams[state];
        double nextValue = isTerminal ? 0.0 : valueParams[nextState];
        double tdError = reward + 0.99 * nextValue - currentValue; // 假设折扣因子为0.99
        
        // 更新价值参数（Critic）
        valueParams[state] += criticLearningRate * tdError;
        
        // Actor更新：策略梯度更新策略参数
        double[] probs = getActionProbabilities(state);
        for (int a = 0; a < numActions; a++) {
            double gradient = (a == action) ? (1 - probs[a]) : -probs[a];
            policyParams[state][a] += actorLearningRate * tdError * gradient;
        }
    }
    
    /**
     * 获取状态价值
     */
    public double getStateValue(int state) {
        return valueParams[state];
    }
    
    /**
     * 获取所有状态价值
     */
    public double[] getAllStateValues() {
        return valueParams.clone();
    }
    
    /**
     * 获取策略参数
     */
    public double[][] getPolicyParameters() {
        double[][] copy = new double[policyParams.length][];
        for (int i = 0; i < policyParams.length; i++) {
            copy[i] = policyParams[i].clone();
        }
        return copy;
    }
    
    /**
     * 重置参数
     */
    public void reset() {
        initializeParameters();
        Arrays.fill(valueParams, 0.0);
    }
}
```

## 10.2.6 完整示例：比较不同价值函数估计方法

```java
/**
 * 价值函数估计方法比较示例
 */
public class ValueFunctionComparison {
    public static void main(String[] args) {
        System.out.println("=== 价值函数估计方法比较 ===");
        
        // 创建简单的环境（4x4网格世界）
        int gridSize = 4;
        int numStates = gridSize * gridSize;
        int numActions = 4;
        double discountFactor = 0.9;
        
        // 创建MDP环境
        DiscreteMDP mdp = createGridWorldMDP(gridSize, numStates, numActions, discountFactor);
        
        // 创建不同的价值函数估计器
        FirstVisitMonteCarlo mcFirstVisit = new FirstVisitMonteCarlo(numStates, discountFactor);
        EveryVisitMonteCarlo mcEveryVisit = new EveryVisitMonteCarlo(numStates, discountFactor);
        IncrementalMonteCarlo mcIncremental = new IncrementalMonteCarlo(numStates, discountFactor);
        TD0 td0 = new TD0(numStates, discountFactor, 0.1);
        TDLambda tdLambda = new TDLambda(numStates, discountFactor, 0.1, 0.5);
        
        // 生成示例轨迹进行测试
        System.out.println("生成示例轨迹...");
        List<List<EpisodeStep>> episodes = generateSampleEpisodes(mdp, 1000);
        
        // 使用蒙特卡洛方法更新
        System.out.println("使用蒙特卡洛方法更新价值函数...");
        for (List<EpisodeStep> episode : episodes) {
            mcFirstVisit.updateValueFunction(episode);
            mcEveryVisit.updateValueFunction(episode);
            mcIncremental.updateValueFunction(episode);
        }
        
        // 使用TD方法更新
        System.out.println("使用TD方法更新价值函数...");
        for (List<EpisodeStep> episode : episodes) {
            for (int t = 0; t < episode.size() - 1; t++) {
                EpisodeStep current = episode.get(t);
                EpisodeStep next = episode.get(t + 1);
                td0.updateValueFunction(current.getState(), current.getReward(), 
                                      next.getState(), false);
                tdLambda.updateValueFunction(current.getState(), current.getReward(), 
                                           next.getState(), false);
            }
            // 处理最后一个步骤
            if (!episode.isEmpty()) {
                EpisodeStep last = episode.get(episode.size() - 1);
                td0.updateValueFunction(last.getState(), last.getReward(), 0, true);
                tdLambda.updateValueFunction(last.getState(), last.getReward(), 0, true);
                tdLambda.resetEligibilityTraces();
            }
        }
        
        // 比较结果
        System.out.println("\n=== 价值函数估计结果比较 ===");
        System.out.println("状态\t首次访问MC\t每次访问MC\t增量MC\tTD(0)\tTD(λ)");
        for (int s = 0; s < Math.min(10, numStates); s++) { // 只显示前10个状态
            System.out.printf("%d\t%.3f\t\t%.3f\t\t%.3f\t%.3f\t%.3f%n",
                s,
                mcFirstVisit.getValueEstimate(s),
                mcEveryVisit.getValueEstimate(s),
                mcIncremental.getValueEstimate(s),
                td0.getValueEstimate(s),
                tdLambda.getValueEstimate(s));
        }
        
        // 测试策略梯度方法
        System.out.println("\n=== 策略梯度方法测试 ===");
        testPolicyGradientMethods(numStates, numActions);
    }
    
    /**
     * 创建网格世界MDP
     */
    private static DiscreteMDP createGridWorldMDP(int gridSize, int numStates, int numActions, double discountFactor) {
        DiscreteMDP mdp = new DiscreteMDP(numStates, numActions, discountFactor);
        
        // 简化的转移概率和奖励设置
        for (int s = 0; s < numStates; s++) {
            for (int a = 0; a < numActions; a++) {
                // 简单的转移：80%概率成功，20%概率失败（保持原位）
                mdp.setTransitionProbability(s, a, s, 0.2); // 20%保持原位
                
                // 80%概率移动到相邻状态（简化处理）
                int nextState = Math.min(s + 1, numStates - 1);
                mdp.setTransitionProbability(s, a, nextState, 0.8);
                
                // 奖励设置
                mdp.setReward(s, a, -1.0); // 每步-1奖励
            }
        }
        
        // 目标状态奖励
        int goalState = numStates - 1;
        for (int a = 0; a < numActions; a++) {
            mdp.setReward(goalState, a, 10.0);
        }
        
        return mdp;
    }
    
    /**
     * 生成示例轨迹
     */
    private static List<List<EpisodeStep>> generateSampleEpisodes(DiscreteMDP mdp, int numEpisodes) {
        List<List<EpisodeStep>> episodes = new ArrayList<>();
        Random random = new Random(42);
        
        for (int i = 0; i < numEpisodes; i++) {
            List<EpisodeStep> episode = new ArrayList<>();
            int currentState = 0; // 从状态0开始
            int maxSteps = 20; // 最大步数
            
            for (int step = 0; step < maxSteps; step++) {
                // 随机选择动作
                int action = random.nextInt(mdp.getNumActions());
                
                // 计算奖励
                double reward = mdp.getReward(currentState, action);
                
                // 添加到轨迹
                episode.add(new EpisodeStep(currentState, action, reward));
                
                // 检查是否到达目标状态
                if (currentState == mdp.getNumStates() - 1) {
                    break;
                }
                
                // 转移到下一个状态（简化处理）
                currentState = Math.min(currentState + 1, mdp.getNumStates() - 1);
            }
            
            episodes.add(episode);
        }
        
        return episodes;
    }
    
    /**
     * 测试策略梯度方法
     */
    private static void testPolicyGradientMethods(int numStates, int numActions) {
        // 创建策略梯度实例
        SimplePolicyGradient policyGradient = new SimplePolicyGradient(numStates, numActions, 0.01);
        ActorCritic actorCritic = new ActorCritic(numStates, numActions, 0.01, 0.1);
        
        // 生成示例轨迹
        List<PolicyGradientStep> trajectory = new ArrayList<>();
        Random random = new Random(42);
        
        for (int t = 0; t < 10; t++) {
            int state = random.nextInt(numStates);
            int action = random.nextInt(numActions);
            double reward = random.nextGaussian(); // 随机奖励
            trajectory.add(new PolicyGradientStep(state, action, reward));
        }
        
        // 更新策略
        System.out.println("更新策略梯度方法...");
        policyGradient.updatePolicy(trajectory);
        
        // 显示更新前后的策略变化
        System.out.println("策略梯度方法测试完成");
    }
}
```

## 本节小结

在本节中，我们深入学习了价值函数估计和策略优化的核心方法：

1. **蒙特卡洛方法**：通过完整轨迹样本来估计价值函数，包括首次访问、每次访问和增量更新方法
2. **时序差分学习**：结合动态规划和蒙特卡洛方法的在线学习方法，包括TD(0)和TD(λ)
3. **动作价值函数估计**：扩展到Q函数的估计方法
4. **策略梯度方法**：直接优化策略参数的方法，包括REINFORCE算法和Actor-Critic方法

通过Java代码实现，我们不仅掌握了理论知识，还获得了实际的编程经验。这些方法是强化学习算法的重要基础。

## 下一步计划

在下一节中，我们将学习经典的Q-Learning算法，深入理解其原理和实现方法。