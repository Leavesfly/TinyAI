# 5.5 Block组合模式：构建复杂网络架构

Block组合模式是深度学习框架中构建复杂网络架构的核心设计模式。它通过将基础Layer组件组合成更高级的Block单元，实现了代码复用、模块化设计和灵活的架构构建。

## 5.5.1 组合模式的设计理念

Block组合模式基于以下核心理念：
- **分层抽象**：将复杂网络分解为多个层次的抽象
- **组件复用**：基础Layer可以在不同Block中重复使用  
- **模块化设计**：每个Block封装特定的功能逻辑
- **灵活组合**：Block可以进一步组合成更复杂的架构

## 5.5.2 Sequential容器实现

Sequential是最基础的Block容器，按顺序执行其包含的层：

```java
package com.tinyai.blocks;

import com.tinyai.core.Variable;
import com.tinyai.layers.Layer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

/**
 * Sequential容器实现
 */
public class Sequential extends Layer {
    private final List<Layer> layers;
    private boolean parametersInitialized = false;

    public Sequential() {
        super("Sequential");
        this.layers = new ArrayList<>();
    }

    public Sequential(Layer... layers) {
        this();
        this.layers.addAll(Arrays.asList(layers));
    }

    public Sequential add(Layer layer) {
        if (layer == null) {
            throw new IllegalArgumentException("Layer cannot be null");
        }
        layers.add(layer);
        return this;
    }

    @Override
    public void initializeParameters(int[] inputShape) {
        if (layers.isEmpty()) return;
        
        int[] currentShape = inputShape.clone();
        for (Layer layer : layers) {
            layer.initializeParameters(currentShape);
            registerLayerParameters(layer);
            currentShape = layer.computeOutputShape(currentShape);
        }
        parametersInitialized = true;
    }

    @Override
    public Variable forward(Variable input) {
        if (layers.isEmpty()) return input;
        
        Variable current = input;
        for (Layer layer : layers) {
            current = layer.forward(current);
        }
        return current;
    }

    @Override
    public int[] computeOutputShape(int[] inputShape) {
        if (layers.isEmpty()) return inputShape.clone();
        
        int[] currentShape = inputShape.clone();
        for (Layer layer : layers) {
            currentShape = layer.computeOutputShape(currentShape);
        }
        return currentShape;
    }

    private void registerLayerParameters(Layer layer) {
        layer.getParameters().forEach((name, param) -> {
            String fullName = layer.getName() + "." + name;
            addParameter(fullName, param);
        });
    }

    @Override
    public void setTraining(boolean training) {
        super.setTraining(training);
        for (Layer layer : layers) {
            layer.setTraining(training);
        }
    }

    public Layer get(int index) { return layers.get(index); }
    public int size() { return layers.size(); }
    public boolean isEmpty() { return layers.isEmpty(); }
}
```

## 5.5.3 残差Block实现

```java
/**
 * 残差Block实现
 */
public class ResidualBlock extends Layer {
    private final Sequential mainPath;
    private final Layer shortcutPath;
    private final boolean useShortcut;

    public ResidualBlock(Sequential mainPath, Layer shortcutPath) {
        super("ResidualBlock");
        this.mainPath = mainPath;
        this.shortcutPath = shortcutPath;
        this.useShortcut = shortcutPath != null;
    }

    @Override
    public void initializeParameters(int[] inputShape) {
        mainPath.initializeParameters(inputShape);
        registerSubModuleParameters("main", mainPath);
        
        if (useShortcut) {
            shortcutPath.initializeParameters(inputShape);
            registerSubModuleParameters("shortcut", shortcutPath);
        }
    }

    @Override
    public Variable forward(Variable input) {
        Variable mainOutput = mainPath.forward(input);
        Variable shortcutOutput = useShortcut ? shortcutPath.forward(input) : input;
        return mainOutput.add(shortcutOutput);
    }

    @Override
    public int[] computeOutputShape(int[] inputShape) {
        return mainPath.computeOutputShape(inputShape);
    }

    private void registerSubModuleParameters(String prefix, Layer subModule) {
        subModule.getParameters().forEach((name, param) -> {
            String fullName = prefix + "." + name;
            addParameter(fullName, param);
        });
    }
}
```

## 5.5.4 多分支Block实现

```java
/**
 * 多分支Block实现
 */
public class MultiBranchBlock extends Layer {
    private final List<Layer> branches;
    private final BranchFusionStrategy fusionStrategy;

    public MultiBranchBlock(BranchFusionStrategy fusionStrategy, Layer... branches) {
        super("MultiBranchBlock");
        this.branches = new ArrayList<>(Arrays.asList(branches));
        this.fusionStrategy = fusionStrategy;
    }

    @Override
    public Variable forward(Variable input) {
        if (branches.isEmpty()) return input;
        
        List<Variable> branchOutputs = new ArrayList<>();
        for (Layer branch : branches) {
            branchOutputs.add(branch.forward(input));
        }
        
        return fusionStrategy.fuse(branchOutputs);
    }

    // ... 其他方法实现
}

/**
 * 分支融合策略接口
 */
public interface BranchFusionStrategy {
    Variable fuse(List<Variable> branchOutputs);
    int[] computeFusedShape(List<int[]> branchShapes);
}

/**
 * 相加融合策略
 */
public class AdditionFusion implements BranchFusionStrategy {
    @Override
    public Variable fuse(List<Variable> branchOutputs) {
        Variable result = branchOutputs.get(0);
        for (int i = 1; i < branchOutputs.size(); i++) {
            result = result.add(branchOutputs.get(i));
        }
        return result;
    }

    @Override
    public int[] computeFusedShape(List<int[]> branchShapes) {
        return branchShapes.get(0).clone();
    }
}
```

## 5.5.5 常见网络Block实现

### ResNet基本Block

```java
/**
 * ResNet基本块实现
 */
public class BasicResBlock extends ResidualBlock {
    
    public static BasicResBlock create(int inChannels, int outChannels, int stride, boolean downsample) {
        Sequential mainPath = new Sequential()
            .add(new Conv2D(inChannels, outChannels, new int[]{3, 3}, 
                          new int[]{stride, stride}, new int[]{1, 1}, false, "zeros"))
            .add(new BatchNorm(outChannels))
            .add(new ReLU())
            .add(new Conv2D(outChannels, outChannels, new int[]{3, 3}, 
                          new int[]{1, 1}, new int[]{1, 1}, false, "zeros"))
            .add(new BatchNorm(outChannels));
        
        Layer shortcutPath = null;
        if (downsample || inChannels != outChannels) {
            shortcutPath = new Sequential()
                .add(new Conv2D(inChannels, outChannels, new int[]{1, 1}, 
                              new int[]{stride, stride}, new int[]{0, 0}, false, "zeros"))
                .add(new BatchNorm(outChannels));
        }
        
        return new BasicResBlock(mainPath, shortcutPath);
    }
    
    private BasicResBlock(Sequential mainPath, Layer shortcutPath) {
        super(mainPath, shortcutPath);
    }
    
    @Override
    public Variable forward(Variable input) {
        Variable output = super.forward(input);
        return new ReLU().forward(output);
    }
}
```

### Inception Block

```java
/**
 * Inception Block实现
 */
public class InceptionBlock extends MultiBranchBlock {
    
    public static InceptionBlock create(int inChannels, int[] config) {
        // 1x1卷积分支
        Layer branch1x1 = new Sequential()
            .add(new Conv2D(inChannels, config[0], new int[]{1, 1}, 
                          new int[]{1, 1}, new int[]{0, 0}, true, "zeros"))
            .add(new ReLU());
        
        // 1x1 -> 3x3卷积分支
        Layer branch3x3 = new Sequential()
            .add(new Conv2D(inChannels, config[1], new int[]{1, 1}, 
                          new int[]{1, 1}, new int[]{0, 0}, true, "zeros"))
            .add(new ReLU())
            .add(new Conv2D(config[1], config[2], new int[]{3, 3}, 
                          new int[]{1, 1}, new int[]{1, 1}, true, "zeros"))
            .add(new ReLU());
        
        // 池化分支
        Layer branchPool = new Sequential()
            .add(new MaxPool2D(new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1}))
            .add(new Conv2D(inChannels, config[3], new int[]{1, 1}, 
                          new int[]{1, 1}, new int[]{0, 0}, true, "zeros"))
            .add(new ReLU());
        
        return new InceptionBlock(new ConcatenationFusion(1), 
                                 branch1x1, branch3x3, branchPool);
    }
    
    private InceptionBlock(BranchFusionStrategy fusionStrategy, Layer... branches) {
        super(fusionStrategy, branches);
    }
}
```

## 5.5.6 动态网络构建器

```java
/**
 * 动态网络构建器
 */
public class NetworkBuilder {
    private Sequential network;
    private int[] currentShape;

    public NetworkBuilder(int[] inputShape) {
        this.network = new Sequential();
        this.currentShape = inputShape.clone();
    }

    public NetworkBuilder addConvBlock(int outChannels, int[] kernelSize, 
                                     int[] stride, boolean useBatchNorm, String activation) {
        int inChannels = currentShape[1];
        
        Sequential block = new Sequential()
            .add(new Conv2D(inChannels, outChannels, kernelSize, stride, 
                          new int[]{kernelSize[0]/2, kernelSize[1]/2}, true, "zeros"));
        
        if (useBatchNorm) {
            block.add(new BatchNorm(outChannels));
        }
        
        block.add(createActivation(activation));
        
        network.add(block);
        updateCurrentShape(block);
        
        return this;
    }

    public NetworkBuilder addResidualBlock(int outChannels, int stride) {
        int inChannels = currentShape[1];
        boolean needsDownsample = stride > 1 || inChannels != outChannels;
        
        BasicResBlock resBlock = BasicResBlock.create(inChannels, outChannels, stride, needsDownsample);
        network.add(resBlock);
        updateCurrentShape(resBlock);
        
        return this;
    }

    public Sequential build() {
        network.initializeParameters(currentShape);
        return network;
    }

    private Layer createActivation(String activation) {
        switch (activation.toLowerCase()) {
            case "relu": return new ReLU();
            case "gelu": return new GELU();
            case "swish": return new Swish();
            default: throw new IllegalArgumentException("Unknown activation: " + activation);
        }
    }

    private void updateCurrentShape(Layer layer) {
        currentShape = layer.computeOutputShape(currentShape);
    }
}
```

## 5.5.7 实际应用示例

### 构建ResNet-18

```java
public class ResNet18 extends Sequential {
    
    public ResNet18(int numClasses) {
        super();
        
        // 初始卷积层
        add(new Conv2D(3, 64, new int[]{7, 7}, new int[]{2, 2}, new int[]{3, 3}, false, "zeros"));
        add(new BatchNorm(64));
        add(new ReLU());
        add(new MaxPool2D(new int[]{3, 3}, new int[]{2, 2}, new int[]{1, 1}));
        
        // 残差层组
        add(createResidualLayer(64, 64, 2, 1));
        add(createResidualLayer(64, 128, 2, 2));
        add(createResidualLayer(128, 256, 2, 2));
        add(createResidualLayer(256, 512, 2, 2));
        
        // 分类头
        add(new AdaptiveAvgPool2D(new int[]{1, 1}));
        add(new Flatten());
        add(new Dense(512, numClasses));
    }
    
    private Sequential createResidualLayer(int inChannels, int outChannels, int numBlocks, int stride) {
        Sequential layer = new Sequential();
        layer.add(BasicResBlock.create(inChannels, outChannels, stride, stride > 1));
        
        for (int i = 1; i < numBlocks; i++) {
            layer.add(BasicResBlock.create(outChannels, outChannels, 1, false));
        }
        
        return layer;
    }
}
```

### 使用NetworkBuilder构建自定义网络

```java
public class CustomNetworkExample {
    
    public static Sequential buildCustomCNN() {
        return new NetworkBuilder(new int[]{1, 3, 224, 224})
            .addConvBlock(32, new int[]{3, 3}, new int[]{1, 1}, true, "relu")
            .addConvBlock(32, new int[]{3, 3}, new int[]{1, 1}, true, "relu")
            .addResidualBlock(64, 2)
            .addResidualBlock(128, 2)
            .addResidualBlock(128, 1)
            .build();
    }
}
```

Block组合模式作为深度学习框架的核心设计模式，提供了构建复杂网络架构的强大工具。通过Sequential容器、残差连接、多分支Block等组合模式，我们可以灵活地构建从简单的CNN到复杂的ResNet、Inception等现代网络架构。

下一章我们将探讨模型封装与管理，学习如何将这些Block组合成完整的可训练模型。