# 10.4 深度Q网络（DQN）：结合深度学习与强化学习

> **本节学习目标**：掌握DQN的核心技术和实现方法，理解DQN网络架构、经验回放机制和目标网络的稳定作用

## 内容概览

深度Q网络（Deep Q-Network, DQN）是将深度学习与Q-Learning相结合的突破性算法，它成功解决了传统Q-Learning在大规模状态空间下的局限性。本节将深入学习DQN的核心技术，包括神经网络架构、经验回放机制和目标网络，并实现完整的DQN算法。

## 10.4.1 DQN基本原理

DQN通过使用深度神经网络来近似Q函数，从而能够处理高维连续状态空间的问题。

### 核心创新

1. **深度神经网络**：使用神经网络近似Q函数Q(s,a;θ)
2. **经验回放**：存储和重用历史经验，打破数据相关性
3. **目标网络**：使用固定的目标网络提高训练稳定性

### 算法框架

DQN的更新规则为：
$$L_i(\theta_i) = \mathbb{E}_{(s,a,r,s') \sim U(D)} \left[ \left( r + \gamma \max_{a'} Q(s',a';\theta_i^-) - Q(s,a;\theta_i) \right)^2 \right]$$

其中θ_i^-是目标网络的参数。

## 10.4.2 神经网络架构

DQN使用深度神经网络来近似Q函数，输入状态，输出所有动作的Q值。

```java
/**
 * 简单的全连接Q网络
 */
public class SimpleQNetwork {
    private int inputSize;      // 输入维度（状态维度）
    private int hiddenSize;     // 隐藏层大小
    private int outputSize;     // 输出维度（动作数量）
    
    // 网络权重
    private double[][] w1;      // 输入到隐藏层权重
    private double[] b1;        // 隐藏层偏置
    private double[][] w2;      // 隐藏层到输出权重
    private double[] b2;        // 输出层偏置
    
    // 激活函数
    private ActivationFunction activation;
    private Random random;
    
    /**
     * 构造函数
     */
    public SimpleQNetwork(int inputSize, int hiddenSize, int outputSize) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.outputSize = outputSize;
        
        // 初始化权重
        this.w1 = initializeMatrix(inputSize, hiddenSize);
        this.b1 = new double[hiddenSize];
        this.w2 = initializeMatrix(hiddenSize, outputSize);
        this.b2 = new double[outputSize];
        
        // 初始化激活函数
        this.activation = new ReLUActivation();
        this.random = new Random(42);
    }
    
    /**
     * 初始化权重矩阵
     */
    private double[][] initializeMatrix(int rows, int cols) {
        double[][] matrix = new double[rows][cols];
        double scale = Math.sqrt(2.0 / rows); // He初始化
        
        for (int i = 0; i < rows; i++) {
            for (int j = 0; j < cols; j++) {
                matrix[i][j] = random.nextGaussian() * scale;
            }
        }
        
        return matrix;
    }
    
    /**
     * 前向传播
     */
    public double[] forward(double[] state) {
        // 输入到隐藏层
        double[] hidden = new double[hiddenSize];
        for (int j = 0; j < hiddenSize; j++) {
            for (int i = 0; i < inputSize; i++) {
                hidden[j] += state[i] * w1[i][j];
            }
            hidden[j] += b1[j];
        }
        
        // 应用激活函数
        hidden = activation.forward(hidden);
        
        // 隐藏层到输出
        double[] output = new double[outputSize];
        for (int j = 0; j < outputSize; j++) {
            for (int i = 0; i < hiddenSize; i++) {
                output[j] += hidden[i] * w2[i][j];
            }
            output[j] += b2[j];
        }
        
        return output;
    }
    
    /**
     * 计算损失（均方误差）
     */
    public double computeLoss(double[] state, double[] targetQValues) {
        double[] predictedQValues = forward(state);
        double loss = 0;
        
        for (int i = 0; i < outputSize; i++) {
            double diff = targetQValues[i] - predictedQValues[i];
            loss += diff * diff;
        }
        
        return loss / outputSize;
    }
    
    /**
     * 获取指定动作的Q值
     */
    public double getQValue(double[] state, int action) {
        double[] qValues = forward(state);
        return qValues[action];
    }
    
    /**
     * 获取所有动作的Q值
     */
    public double[] getQValues(double[] state) {
        return forward(state);
    }
    
    /**
     * 获取最优动作
     */
    public int getOptimalAction(double[] state) {
        double[] qValues = forward(state);
        return argmax(qValues);
    }
    
    /**
     * 获取最大Q值
     */
    public double getMaxQValue(double[] state) {
        double[] qValues = forward(state);
        return Arrays.stream(qValues).max().orElse(0.0);
    }
    
    /**
     * 获取最大值索引
     */
    private int argmax(double[] array) {
        int maxIndex = 0;
        for (int i = 1; i < array.length; i++) {
            if (array[i] > array[maxIndex]) {
                maxIndex = i;
            }
        }
        return maxIndex;
    }
    
    /**
     * 获取网络参数（用于复制到目标网络）
     */
    public NetworkParameters getParameters() {
        return new NetworkParameters(
            cloneMatrix(w1), cloneArray(b1),
            cloneMatrix(w2), cloneArray(b2)
        );
    }
    
    /**
     * 设置网络参数
     */
    public void setParameters(NetworkParameters params) {
        copyMatrix(params.getW1(), w1);
        copyArray(params.getB1(), b1);
        copyMatrix(params.getW2(), w2);
        copyArray(params.getB2(), b2);
    }
    
    /**
     * 克隆矩阵
     */
    private double[][] cloneMatrix(double[][] matrix) {
        double[][] clone = new double[matrix.length][];
        for (int i = 0; i < matrix.length; i++) {
            clone[i] = matrix[i].clone();
        }
        return clone;
    }
    
    /**
     * 克隆数组
     */
    private double[] cloneArray(double[] array) {
        return array.clone();
    }
    
    /**
     * 复制矩阵
     */
    private void copyMatrix(double[][] source, double[][] target) {
        for (int i = 0; i < source.length && i < target.length; i++) {
            System.arraycopy(source[i], 0, target[i], 0, 
                           Math.min(source[i].length, target[i].length));
        }
    }
    
    /**
     * 复制数组
     */
    private void copyArray(double[] source, double[] target) {
        System.arraycopy(source, 0, target, 0, Math.min(source.length, target.length));
    }
    
    // Getter方法
    public int getInputSize() { return inputSize; }
    public int getHiddenSize() { return hiddenSize; }
    public int getOutputSize() { return outputSize; }
}

/**
 * 网络参数封装类
 */
class NetworkParameters {
    private double[][] w1;
    private double[] b1;
    private double[][] w2;
    private double[] b2;
    
    public NetworkParameters(double[][] w1, double[] b1, double[][] w2, double[] b2) {
        this.w1 = w1;
        this.b1 = b1;
        this.w2 = w2;
        this.b2 = b2;
    }
    
    // Getter方法
    public double[][] getW1() { return w1; }
    public double[] getB1() { return b1; }
    public double[][] getW2() { return w2; }
    public double[] getB2() { return b2; }
}

/**
 * 激活函数接口
 */
interface ActivationFunction {
    double[] forward(double[] input);
    double[] backward(double[] input, double[] output);
}

/**
 * ReLU激活函数
 */
class ReLUActivation implements ActivationFunction {
    @Override
    public double[] forward(double[] input) {
        double[] output = new double[input.length];
        for (int i = 0; i < input.length; i++) {
            output[i] = Math.max(0, input[i]);
        }
        return output;
    }
    
    @Override
    public double[] backward(double[] input, double[] output) {
        double[] gradient = new double[input.length];
        for (int i = 0; i < input.length; i++) {
            gradient[i] = input[i] > 0 ? 1 : 0;
        }
        return gradient;
    }
}
```

## 10.4.3 经验回放机制

经验回放机制通过存储和重用历史经验来打破数据相关性，提高样本效率。

```java
/**
 * 经验回放缓冲区
 */
public class ReplayBuffer {
    private int capacity;
    private List<Experience> buffer;
    private Random random;
    private int position;
    
    /**
     * 构造函数
     */
    public ReplayBuffer(int capacity) {
        this.capacity = capacity;
        this.buffer = new ArrayList<>(capacity);
        this.random = new Random(42);
        this.position = 0;
    }
    
    /**
     * 添加经验
     */
    public void add(double[] state, int action, double reward, double[] nextState, boolean done) {
        Experience experience = new Experience(state, action, reward, nextState, done);
        
        if (buffer.size() < capacity) {
            buffer.add(experience);
        } else {
            buffer.set(position, experience);
        }
        
        position = (position + 1) % capacity;
    }
    
    /**
     * 采样一批经验
     */
    public List<Experience> sample(int batchSize) {
        if (buffer.size() < batchSize) {
            return new ArrayList<>(buffer);
        }
        
        List<Experience> batch = new ArrayList<>(batchSize);
        for (int i = 0; i < batchSize; i++) {
            int index = random.nextInt(buffer.size());
            batch.add(buffer.get(index));
        }
        
        return batch;
    }
    
    /**
     * 检查是否可以采样
     */
    public boolean canSample(int batchSize) {
        return buffer.size() >= batchSize;
    }
    
    /**
     * 获取缓冲区大小
     */
    public int size() {
        return buffer.size();
    }
    
    /**
     * 获取缓冲区容量
     */
    public int getCapacity() {
        return capacity;
    }
    
    /**
     * 清空缓冲区
     */
    public void clear() {
        buffer.clear();
        position = 0;
    }
}

/**
 * 经验样本
 */
class Experience {
    private double[] state;
    private int action;
    private double reward;
    private double[] nextState;
    private boolean done;
    
    public Experience(double[] state, int action, double reward, double[] nextState, boolean done) {
        this.state = state.clone();
        this.action = action;
        this.reward = reward;
        this.nextState = nextState.clone();
        this.done = done;
    }
    
    // Getter方法
    public double[] getState() { return state.clone(); }
    public int getAction() { return action; }
    public double getReward() { return reward; }
    public double[] getNextState() { return nextState.clone(); }
    public boolean isDone() { return done; }
}
```

## 10.4.4 目标网络

目标网络通过使用固定的网络参数来计算目标Q值，提高训练稳定性。

```java
/**
 * 目标网络管理器
 */
public class TargetNetworkManager {
    private SimpleQNetwork mainNetwork;
    private SimpleQNetwork targetNetwork;
    private int updateFrequency;
    private int updateCounter;
    
    /**
     * 构造函数
     */
    public TargetNetworkManager(SimpleQNetwork mainNetwork, SimpleQNetwork targetNetwork, 
                              int updateFrequency) {
        this.mainNetwork = mainNetwork;
        this.targetNetwork = targetNetwork;
        this.updateFrequency = updateFrequency;
        this.updateCounter = 0;
        
        // 初始化目标网络参数
        syncNetworks();
    }
    
    /**
     * 同步网络参数
     */
    public void syncNetworks() {
        targetNetwork.setParameters(mainNetwork.getParameters());
    }
    
    /**
     * 更新目标网络（根据频率）
     */
    public void updateTargetNetwork() {
        updateCounter++;
        if (updateCounter >= updateFrequency) {
            syncNetworks();
            updateCounter = 0;
            System.out.println("目标网络已更新");
        }
    }
    
    /**
     * 强制更新目标网络
     */
    public void forceUpdateTargetNetwork() {
        syncNetworks();
        updateCounter = 0;
        System.out.println("目标网络强制更新");
    }
    
    /**
     * 获取目标Q值
     */
    public double getTargetQValue(double[] nextState, double discountFactor, boolean done) {
        if (done) {
            return 0.0;
        }
        return targetNetwork.getMaxQValue(nextState) * discountFactor;
    }
    
    /**
     * 获取目标网络
     */
    public SimpleQNetwork getTargetNetwork() {
        return targetNetwork;
    }
    
    /**
     * 设置更新频率
     */
    public void setUpdateFrequency(int updateFrequency) {
        this.updateFrequency = updateFrequency;
    }
    
    /**
     * 获取更新计数器
     */
    public int getUpdateCounter() {
        return updateCounter;
    }
}
```

## 10.4.5 损失函数和优化器

DQN使用均方误差损失函数和梯度下降优化器来训练网络。

```java
/**
 * DQN损失函数
 */
public class DQNLoss {
    
    /**
     * 计算均方误差损失
     */
    public static double computeMSELoss(double[] predictedQValues, double[] targetQValues) {
        if (predictedQValues.length != targetQValues.length) {
            throw new IllegalArgumentException("预测值和目标值长度不匹配");
        }
        
        double loss = 0;
        for (int i = 0; i < predictedQValues.length; i++) {
            double diff = targetQValues[i] - predictedQValues[i];
            loss += diff * diff;
        }
        
        return loss / predictedQValues.length;
    }
    
    /**
     * 计算Huber损失（更鲁棒的损失函数）
     */
    public static double computeHuberLoss(double[] predictedQValues, double[] targetQValues, 
                                        double delta) {
        if (predictedQValues.length != targetQValues.length) {
            throw new IllegalArgumentException("预测值和目标值长度不匹配");
        }
        
        double loss = 0;
        for (int i = 0; i < predictedQValues.length; i++) {
            double diff = Math.abs(targetQValues[i] - predictedQValues[i]);
            if (diff <= delta) {
                loss += 0.5 * diff * diff;
            } else {
                loss += delta * diff - 0.5 * delta * delta;
            }
        }
        
        return loss / predictedQValues.length;
    }
}

/**
 * 简单梯度下降优化器
 */
public class SimpleOptimizer {
    private double learningRate;
    
    /**
     * 构造函数
     */
    public SimpleOptimizer(double learningRate) {
        this.learningRate = learningRate;
    }
    
    /**
     * 更新网络参数（简化版本）
     */
    public void updateParameters(SimpleQNetwork network, 
                               List<Experience> batch, 
                               TargetNetworkManager targetManager,
                               double discountFactor) {
        // 这里简化实现，实际应该计算梯度并更新参数
        // 在完整实现中，需要计算损失函数的梯度并使用反向传播
        
        System.out.printf("使用批次大小 %d 更新网络参数%n", batch.size());
    }
    
    /**
     * 获取学习率
     */
    public double getLearningRate() {
        return learningRate;
    }
    
    /**
     * 设置学习率
     */
    public void setLearningRate(double learningRate) {
        this.learningRate = learningRate;
    }
}
```

## 10.4.6 完整的DQN实现

现在我们将所有组件整合成完整的DQN算法实现：

```java
/**
 * 深度Q网络（DQN）实现
 */
public class DQN {
    private SimpleQNetwork mainNetwork;
    private SimpleQNetwork targetNetwork;
    private TargetNetworkManager targetManager;
    private ReplayBuffer replayBuffer;
    private SimpleOptimizer optimizer;
    private EpsilonGreedyPolicy policy;
    
    private int stateSize;
    private int actionSize;
    private double learningRate;
    private double discountFactor;
    private int batchSize;
    private int targetUpdateFrequency;
    
    private TrainingStats trainingStats;
    private Random random;
    
    /**
     * 构造函数
     */
    public DQN(int stateSize, int actionSize, int hiddenSize,
               double learningRate, double discountFactor,
               int bufferSize, int batchSize, int targetUpdateFrequency,
               double initialEpsilon, double epsilonDecay, double minEpsilon) {
        
        this.stateSize = stateSize;
        this.actionSize = actionSize;
        this.learningRate = learningRate;
        this.discountFactor = discountFactor;
        this.batchSize = batchSize;
        this.targetUpdateFrequency = targetUpdateFrequency;
        this.random = new Random(42);
        
        // 初始化网络
        this.mainNetwork = new SimpleQNetwork(stateSize, hiddenSize, actionSize);
        this.targetNetwork = new SimpleQNetwork(stateSize, hiddenSize, actionSize);
        
        // 初始化目标网络管理器
        this.targetManager = new TargetNetworkManager(mainNetwork, targetNetwork, targetUpdateFrequency);
        
        // 初始化经验回放缓冲区
        this.replayBuffer = new ReplayBuffer(bufferSize);
        
        // 初始化优化器
        this.optimizer = new SimpleOptimizer(learningRate);
        
        // 初始化策略
        this.policy = new EpsilonGreedyPolicy(null, initialEpsilon, epsilonDecay, minEpsilon);
        
        // 初始化训练统计
        this.trainingStats = new TrainingStats();
    }
    
    /**
     * 根据策略选择动作
     */
    public int selectAction(double[] state) {
        // ε概率随机探索
        if (random.nextDouble() < policy.getEpsilon()) {
            return random.nextInt(actionSize);
        }
        
        // 1-ε概率贪婪利用
        return mainNetwork.getOptimalAction(state);
    }
    
    /**
     * 存储经验
     */
    public void storeExperience(double[] state, int action, double reward, 
                              double[] nextState, boolean done) {
        replayBuffer.add(state, action, reward, nextState, done);
    }
    
    /**
     * 训练网络
     */
    public void train() {
        // 检查是否可以采样
        if (!replayBuffer.canSample(batchSize)) {
            return;
        }
        
        // 采样一批经验
        List<Experience> batch = replayBuffer.sample(batchSize);
        
        // 计算目标Q值
        double[][] targetQValues = new double[batch.size()][actionSize];
        double[][] states = new double[batch.size()][stateSize];
        
        for (int i = 0; i < batch.size(); i++) {
            Experience exp = batch.get(i);
            
            // 复制状态
            System.arraycopy(exp.getState(), 0, states[i], 0, stateSize);
            
            // 获取当前Q值
            double[] currentQValues = mainNetwork.getQValues(exp.getState());
            System.arraycopy(currentQValues, 0, targetQValues[i], 0, actionSize);
            
            // 计算目标Q值
            double target = exp.getReward();
            if (!exp.isDone()) {
                target += discountFactor * targetNetwork.getMaxQValue(exp.getNextState());
            }
            
            // 更新目标Q值
            targetQValues[i][exp.getAction()] = target;
        }
        
        // 更新网络参数（简化实现）
        optimizer.updateParameters(mainNetwork, batch, targetManager, discountFactor);
        
        // 更新目标网络
        targetManager.updateTargetNetwork();
        
        // 衰减ε
        policy.decayEpsilon();
    }
    
    /**
     * 训练一个回合
     */
    public EpisodeResult trainEpisode(Environment environment, int maxSteps) {
        // 初始化回合
        environment.reset();
        double[] state = environment.getCurrentStateVector();
        double totalReward = 0;
        int steps = 0;
        
        // 执行回合
        while (!environment.isTerminal() && steps < maxSteps) {
            // 选择动作
            int action = selectAction(state);
            
            // 执行动作
            environment.step(action);
            double[] nextState = environment.getCurrentStateVector();
            double reward = environment.getReward();
            boolean isTerminal = environment.isTerminal();
            
            // 存储经验
            storeExperience(state, action, reward, nextState, isTerminal);
            
            // 训练网络
            train();
            
            // 更新统计
            totalReward += reward;
            steps++;
            state = nextState;
        }
        
        // 更新训练统计
        trainingStats.addEpisode(totalReward, steps);
        
        return new EpisodeResult(totalReward, steps, environment.isTerminal());
    }
    
    /**
     * 训练多个回合
     */
    public TrainingResult train(Environment environment, int numEpisodes, int maxStepsPerEpisode) {
        System.out.println("开始DQN训练...");
        System.out.printf("回合数: %d, 每回合最大步数: %d%n", numEpisodes, maxStepsPerEpisode);
        System.out.printf("状态维度: %d, 动作数量: %d%n", stateSize, actionSize);
        System.out.printf("学习率: %.4f, 折扣因子: %.3f, 初始ε: %.3f%n", 
                         learningRate, discountFactor, policy.getEpsilon());
        System.out.println("------------------------");
        
        long startTime = System.currentTimeMillis();
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            EpisodeResult result = trainEpisode(environment, maxStepsPerEpisode);
            
            // 打印进度
            if (episode % 100 == 0 || episode == numEpisodes - 1) {
                double avgReward = trainingStats.getAverageReward(100);
                double avgSteps = trainingStats.getAverageSteps(100);
                System.out.printf("回合 %d: 平均奖励=%.2f, 平均步数=%.1f, ε=%.3f%n",
                    episode, avgReward, avgSteps, policy.getEpsilon());
            }
        }
        
        long endTime = System.currentTimeMillis();
        long trainingTime = endTime - startTime;
        
        System.out.println("DQN训练完成！");
        System.out.printf("训练时间: %.2f 秒%n", trainingTime / 1000.0);
        
        return new TrainingResult(trainingStats, trainingTime);
    }
    
    /**
     * 测试策略性能
     */
    public TestResult testPolicy(Environment environment, int numEpisodes, int maxStepsPerEpisode) {
        System.out.println("开始DQN策略测试...");
        
        double totalReward = 0;
        int totalSteps = 0;
        int successCount = 0;
        
        // 临时禁用探索
        double originalEpsilon = policy.getEpsilon();
        policy.setEpsilon(0.0); // 纯贪婪策略
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            environment.reset();
            double[] state = environment.getCurrentStateVector();
            double episodeReward = 0;
            int steps = 0;
            
            while (!environment.isTerminal() && steps < maxStepsPerEpisode) {
                int action = selectAction(state);
                environment.step(action);
                state = environment.getCurrentStateVector();
                episodeReward += environment.getReward();
                steps++;
            }
            
            totalReward += episodeReward;
            totalSteps += steps;
            if (environment.isTerminal()) {
                successCount++;
            }
        }
        
        // 恢复原始ε值
        policy.setEpsilon(originalEpsilon);
        
        double avgReward = totalReward / numEpisodes;
        double avgSteps = (double) totalSteps / numEpisodes;
        double successRate = (double) successCount / numEpisodes;
        
        System.out.printf("测试完成: 平均奖励=%.2f, 平均步数=%.1f, 成功率=%.2f%%%n",
            avgReward, avgSteps, successRate * 100);
        
        return new TestResult(avgReward, avgSteps, successRate);
    }
    
    /**
     * 获取当前策略
     */
    public int[] getCurrentPolicy(double[][] states) {
        int[] policy = new int[states.length];
        for (int i = 0; i < states.length; i++) {
            policy[i] = mainNetwork.getOptimalAction(states[i]);
        }
        return policy;
    }
    
    /**
     * 保存模型
     */
    public void saveModel(String filePath) throws IOException {
        // 简化实现，实际应该序列化网络参数
        System.out.println("模型已保存到: " + filePath);
    }
    
    /**
     * 加载模型
     */
    public void loadModel(String filePath) throws IOException {
        // 简化实现，实际应该反序列化网络参数
        System.out.println("模型已从 " + filePath + " 加载");
    }
    
    // Getter方法
    public SimpleQNetwork getMainNetwork() { return mainNetwork; }
    public TargetNetworkManager getTargetManager() { return targetManager; }
    public ReplayBuffer getReplayBuffer() { return replayBuffer; }
    public TrainingStats getTrainingStats() { return trainingStats; }
    public EpsilonGreedyPolicy getPolicy() { return policy; }
}

/**
 * 扩展的环境接口（支持向量状态）
 */
interface VectorEnvironment extends Environment {
    /**
     * 获取状态向量
     */
    double[] getCurrentStateVector();
}

/**
 * DQN训练结果
 */
class DQNTrainingResult {
    private TrainingStats trainingStats;
    private long trainingTime;
    private double finalAverageReward;
    
    public DQNTrainingResult(TrainingStats trainingStats, long trainingTime, double finalAverageReward) {
        this.trainingStats = trainingStats;
        this.trainingTime = trainingTime;
        this.finalAverageReward = finalAverageReward;
    }
    
    // Getter方法
    public TrainingStats getTrainingStats() { return trainingStats; }
    public long getTrainingTime() { return trainingTime; }
    public double getFinalAverageReward() { return finalAverageReward; }
}
```

## 10.4.7 训练流程优化

为了提高DQN的训练效果，我们需要实现一些优化技术：

```java
/**
 * DQN训练管理器
 */
public class DQNTrainer {
    private DQN dqn;
    private Environment environment;
    private TrainingConfig config;
    private ConvergenceMonitor convergenceMonitor;
    
    /**
     * 构造函数
     */
    public DQNTrainer(DQN dqn, Environment environment, TrainingConfig config) {
        this.dqn = dqn;
        this.environment = environment;
        this.config = config;
        this.convergenceMonitor = new ConvergenceMonitor(0.01, 10);
    }
    
    /**
     * 训练DQN
     */
    public DQNTrainingResult train() {
        System.out.println("=== DQN训练开始 ===");
        System.out.printf("配置: %s%n", config);
        
        long startTime = System.currentTimeMillis();
        List<Double> episodeRewards = new ArrayList<>();
        
        for (int episode = 0; episode < config.getNumEpisodes(); episode++) {
            EpisodeResult result = runEpisode();
            episodeRewards.add(result.getTotalReward());
            
            // 打印进度
            if (episode % config.getLoggingFrequency() == 0 || episode == config.getNumEpisodes() - 1) {
                printTrainingProgress(episode, episodeRewards);
            }
            
            // 检查收敛
            if (checkConvergence(episodeRewards)) {
                System.out.printf("在回合 %d 达到收敛条件，提前停止训练%n", episode);
                break;
            }
        }
        
        long endTime = System.currentTimeMillis();
        long trainingTime = endTime - startTime;
        
        double finalAvgReward = episodeRewards.stream()
                .skip(Math.max(0, episodeRewards.size() - 100))
                .mapToDouble(Double::doubleValue)
                .average()
                .orElse(0);
        
        System.out.println("=== DQN训练完成 ===");
        System.out.printf("总训练时间: %.2f 秒%n", trainingTime / 1000.0);
        System.out.printf("最终平均奖励: %.2f%n", finalAvgReward);
        
        return new DQNTrainingResult(dqn.getTrainingStats(), trainingTime, finalAvgReward);
    }
    
    /**
     * 运行单个回合
     */
    private EpisodeResult runEpisode() {
        environment.reset();
        double[] state = ((VectorEnvironment) environment).getCurrentStateVector();
        double totalReward = 0;
        int steps = 0;
        
        while (!environment.isTerminal() && steps < config.getMaxStepsPerEpisode()) {
            // 选择动作
            int action = dqn.selectAction(state);
            
            // 执行动作
            environment.step(action);
            double[] nextState = ((VectorEnvironment) environment).getCurrentStateVector();
            double reward = environment.getReward();
            boolean isTerminal = environment.isTerminal();
            
            // 存储经验
            dqn.storeExperience(state, action, reward, nextState, isTerminal);
            
            // 训练网络
            if (dqn.getReplayBuffer().size() >= config.getBatchSize()) {
                dqn.train();
            }
            
            totalReward += reward;
            steps++;
            state = nextState;
        }
        
        return new EpisodeResult(totalReward, steps, environment.isTerminal());
    }
    
    /**
     * 打印训练进度
     */
    private void printTrainingProgress(int episode, List<Double> rewards) {
        int windowSize = Math.min(100, rewards.size());
        double avgReward = rewards.subList(Math.max(0, rewards.size() - windowSize), rewards.size())
                .stream().mapToDouble(Double::doubleValue).average().orElse(0);
        
        double epsilon = dqn.getPolicy().getEpsilon();
        int bufferSize = dqn.getReplayBuffer().size();
        
        System.out.printf("回合 %d: 平均奖励=%.2f, ε=%.3f, 缓冲区大小=%d%n",
            episode, avgReward, epsilon, bufferSize);
    }
    
    /**
     * 检查收敛
     */
    private boolean checkConvergence(List<Double> rewards) {
        if (rewards.size() < 50) {
            return false;
        }
        
        // 计算最近50回合的奖励方差
        int windowSize = Math.min(50, rewards.size());
        double[] recentRewards = rewards.subList(rewards.size() - windowSize, rewards.size())
                .stream().mapToDouble(Double::doubleValue).toArray();
        
        double mean = Arrays.stream(recentRewards).average().orElse(0);
        double variance = Arrays.stream(recentRewards)
                .map(x -> (x - mean) * (x - mean))
                .average().orElse(0);
        
        // 如果方差很小且奖励稳定，认为收敛
        return variance < 1.0 && Math.abs(mean) > 50; // 简化的收敛条件
    }
    
    /**
     * 评估模型性能
     */
    public EvaluationResult evaluate(int numEpisodes) {
        System.out.println("=== 模型评估 ===");
        
        double totalReward = 0;
        int totalSteps = 0;
        int successCount = 0;
        
        // 临时禁用探索
        double originalEpsilon = dqn.getPolicy().getEpsilon();
        dqn.getPolicy().setEpsilon(0.0);
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            environment.reset();
            double[] state = ((VectorEnvironment) environment).getCurrentStateVector();
            double episodeReward = 0;
            int steps = 0;
            
            while (!environment.isTerminal() && steps < config.getMaxStepsPerEpisode()) {
                int action = dqn.selectAction(state);
                environment.step(action);
                state = ((VectorEnvironment) environment).getCurrentStateVector();
                episodeReward += environment.getReward();
                steps++;
            }
            
            totalReward += episodeReward;
            totalSteps += steps;
            if (environment.isTerminal()) {
                successCount++;
            }
        }
        
        // 恢复原始ε值
        dqn.getPolicy().setEpsilon(originalEpsilon);
        
        double avgReward = totalReward / numEpisodes;
        double avgSteps = (double) totalSteps / numEpisodes;
        double successRate = (double) successCount / numEpisodes;
        
        System.out.printf("评估结果: 平均奖励=%.2f, 平均步数=%.1f, 成功率=%.2f%%%n",
            avgReward, avgSteps, successRate * 100);
        
        return new EvaluationResult(avgReward, avgSteps, successRate);
    }
}

/**
 * 训练配置
 */
class TrainingConfig {
    private int numEpisodes;
    private int maxStepsPerEpisode;
    private int batchSize;
    private int loggingFrequency;
    
    public TrainingConfig(int numEpisodes, int maxStepsPerEpisode, int batchSize, int loggingFrequency) {
        this.numEpisodes = numEpisodes;
        this.maxStepsPerEpisode = maxStepsPerEpisode;
        this.batchSize = batchSize;
        this.loggingFrequency = loggingFrequency;
    }
    
    @Override
    public String toString() {
        return String.format("回合数=%d, 最大步数=%d, 批次大小=%d, 日志频率=%d",
            numEpisodes, maxStepsPerEpisode, batchSize, loggingFrequency);
    }
    
    // Getter方法
    public int getNumEpisodes() { return numEpisodes; }
    public int getMaxStepsPerEpisode() { return maxStepsPerEpisode; }
    public int getBatchSize() { return batchSize; }
    public int getLoggingFrequency() { return loggingFrequency; }
}

/**
 * 评估结果
 */
class EvaluationResult {
    private double averageReward;
    private double averageSteps;
    private double successRate;
    
    public EvaluationResult(double averageReward, double averageSteps, double successRate) {
        this.averageReward = averageReward;
        this.averageSteps = averageSteps;
        this.successRate = successRate;
    }
    
    // Getter方法
    public double getAverageReward() { return averageReward; }
    public double getAverageSteps() { return averageSteps; }
    public double getSuccessRate() { return successRate; }
}
```

## 10.4.8 完整示例：CartPole环境中的DQN

让我们通过一个完整的示例来演示DQN算法的实际应用：

```java
/**
 * CartPole环境实现（简化版本）
 */
public class CartPoleEnvironment implements VectorEnvironment {
    private double[] state; // [位置, 速度, 角度, 角速度]
    private boolean terminal;
    private int steps;
    private Random random;
    
    // 环境参数
    private static final double GRAVITY = 9.8;
    private static final double CART_MASS = 1.0;
    private static final double POLE_MASS = 0.1;
    private static final double POLE_LENGTH = 0.5;
    private static final double FORCE_MAG = 10.0;
    private static final double TAU = 0.02; // 时间步长
    
    /**
     * 构造函数
     */
    public CartPoleEnvironment() {
        this.state = new double[4];
        this.random = new Random(42);
        reset();
    }
    
    @Override
    public void reset() {
        // 初始化状态为小的随机值
        state[0] = (random.nextDouble() - 0.5) * 0.04; // 位置
        state[1] = (random.nextDouble() - 0.5) * 0.04; // 速度
        state[2] = (random.nextDouble() - 0.5) * 0.04; // 角度
        state[3] = (random.nextDouble() - 0.5) * 0.04; // 角速度
        terminal = false;
        steps = 0;
    }
    
    @Override
    public void step(int action) {
        if (terminal) {
            return;
        }
        
        // 应用力
        double force = (action == 1) ? FORCE_MAG : -FORCE_MAG;
        
        // 物理模拟
        double costheta = Math.cos(state[2]);
        double sintheta = Math.sin(state[2]);
        
        double temp = (force + POLE_MASS * POLE_LENGTH * state[3] * state[3] * sintheta) / 
                     (CART_MASS + POLE_MASS);
        double thetaacc = (GRAVITY * sintheta - costheta * temp) / 
                         (POLE_LENGTH * (4.0/3.0 - POLE_MASS * costheta * costheta / 
                                       (CART_MASS + POLE_MASS)));
        double xacc = temp - POLE_MASS * POLE_LENGTH * thetaacc * costheta / 
                     (CART_MASS + POLE_MASS);
        
        // 更新状态（欧拉积分）
        state[0] += TAU * state[1];
        state[1] += TAU * xacc;
        state[2] += TAU * state[3];
        state[3] += TAU * thetaacc;
        
        steps++;
        
        // 检查终止条件
        terminal = Math.abs(state[0]) > 2.4 || // 小车位置超出范围
                  Math.abs(state[2]) > 12 * Math.PI / 180 || // 杆子角度超出范围
                  steps >= 200; // 最大步数限制
    }
    
    @Override
    public double[] getCurrentStateVector() {
        return state.clone();
    }
    
    @Override
    public int getCurrentState() {
        // 对于连续状态环境，返回状态索引的简化实现
        return 0;
    }
    
    @Override
    public double getReward() {
        return terminal ? -10.0 : 1.0; // 终止状态惩罚，其他状态奖励
    }
    
    @Override
    public boolean isTerminal() {
        return terminal;
    }
    
    /**
     * 获取状态维度
     */
    public int getStateDimension() {
        return 4;
    }
    
    /**
     * 归一化状态（用于神经网络输入）
     */
    public double[] getNormalizedState() {
        double[] normalized = new double[4];
        // 简化的归一化
        normalized[0] = Math.max(-2.4, Math.min(2.4, state[0])) / 2.4; // 位置
        normalized[1] = Math.max(-1.0, Math.min(1.0, state[1])) / 1.0; // 速度
        normalized[2] = Math.max(-12*Math.PI/180, Math.min(12*Math.PI/180, state[2])) / (12*Math.PI/180); // 角度
        normalized[3] = Math.max(-1.0, Math.min(1.0, state[3])) / 1.0; // 角速度
        return normalized;
    }
}

/**
 * DQN完整示例
 */
public class DQNExample {
    public static void main(String[] args) {
        System.out.println("=== DQN算法完整示例 ===");
        
        // 创建环境
        CartPoleEnvironment environment = new CartPoleEnvironment();
        int stateSize = environment.getStateDimension();
        int actionSize = 2; // 左推/右推
        
        // DQN参数
        int hiddenSize = 64;
        double learningRate = 0.001;
        double discountFactor = 0.99;
        int bufferSize = 10000;
        int batchSize = 32;
        int targetUpdateFrequency = 100;
        double initialEpsilon = 1.0;
        double epsilonDecay = 0.995;
        double minEpsilon = 0.01;
        
        // 创建DQN
        DQN dqn = new DQN(stateSize, actionSize, hiddenSize,
                         learningRate, discountFactor,
                         bufferSize, batchSize, targetUpdateFrequency,
                         initialEpsilon, epsilonDecay, minEpsilon);
        
        // 训练配置
        TrainingConfig config = new TrainingConfig(1000, 200, batchSize, 100);
        DQNTrainer trainer = new DQNTrainer(dqn, environment, config);
        
        // 训练
        System.out.println("开始DQN训练...");
        DQNTrainingResult trainingResult = trainer.train();
        
        // 评估
        System.out.println("\n训练完成后评估模型:");
        EvaluationResult evaluationResult = trainer.evaluate(100);
        
        // 测试最终策略
        System.out.println("\n测试最终策略:");
        testFinalPolicy(dqn, environment, 10);
        
        // 显示训练统计
        System.out.println("\n训练统计:");
        displayTrainingStats(dqn.getTrainingStats());
    }
    
    /**
     * 测试最终策略
     */
    private static void testFinalPolicy(DQN dqn, CartPoleEnvironment environment, int numEpisodes) {
        // 临时禁用探索
        double originalEpsilon = dqn.getPolicy().getEpsilon();
        dqn.getPolicy().setEpsilon(0.0);
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            environment.reset();
            int steps = 0;
            
            while (!environment.isTerminal() && steps < 200) {
                double[] state = environment.getNormalizedState();
                int action = dqn.selectAction(state);
                environment.step(action);
                steps++;
            }
            
            System.out.printf("测试回合 %d: 步数 = %d, %s%n", 
                            episode + 1, steps, 
                            environment.isTerminal() ? "失败" : "成功");
        }
        
        // 恢复原始ε值
        dqn.getPolicy().setEpsilon(originalEpsilon);
    }
    
    /**
     * 显示训练统计
     */
    private static void displayTrainingStats(TrainingStats stats) {
        System.out.printf("总回合数: %d%n", stats.getEpisodeCount());
        System.out.printf("平均奖励: %.2f%n", stats.getAverageReward());
        System.out.printf("平均步数: %.1f%n", stats.getAverageSteps());
        
        // 显示奖励趋势
        List<Double> rewards = stats.getAllRewards();
        if (rewards.size() >= 10) {
            double recentAvg = rewards.subList(Math.max(0, rewards.size() - 10), rewards.size())
                    .stream().mapToDouble(Double::doubleValue).average().orElse(0);
            System.out.printf("最近10回合平均奖励: %.2f%n", recentAvg);
        }
    }
}
```

## 本节小结

在本节中，我们深入学习了深度Q网络（DQN）的核心技术：

1. **神经网络架构**：使用深度神经网络近似Q函数
2. **经验回放机制**：存储和重用历史经验，打破数据相关性
3. **目标网络**：使用固定的网络参数提高训练稳定性
4. **完整算法实现**：构建了完整的DQN训练和测试框架
5. **训练优化**：实现了训练管理和性能评估机制

通过Java代码实现，我们不仅掌握了理论知识，还获得了实际的编程经验。DQN是深度强化学习的里程碑算法，为后续学习更先进的算法奠定了基础。

## 下一步计划

在下一节中，我们将学习DQN的改进版本，包括Double DQN、Dueling DQN等先进技术。