# 6.5 推理引擎：高效模型部署

推理引擎是深度学习模型从训练转向生产部署的关键组件。它专注于优化推理性能，提供高吞吐量、低延迟的模型服务。本节将深入探讨推理引擎的设计原理和实现方案。

## 6.5.1 推理引擎的核心特性

推理引擎与训练引擎的关键差异：
- **性能优先**：追求最高的推理速度和吞吐量
- **内存优化**：最小化内存占用和分配
- **批处理支持**：动态批处理以提高效率
- **模型优化**：图优化、量化、剪枝等技术

### 推理引擎基类设计

```java
package com.tinyai.inference;

import com.tinyai.core.Variable;
import com.tinyai.models.Model;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.List;
import java.util.ArrayList;
import java.util.Queue;
import java.util.concurrent.ConcurrentLinkedQueue;

/**
 * 推理引擎基类
 */
public class InferenceEngine {
    private final Model model;
    private final InferenceConfig config;
    private final ExecutorService executorService;
    
    // 批处理队列
    private final Queue<InferenceRequest> requestQueue = new ConcurrentLinkedQueue<>();
    private final BatchProcessor batchProcessor;
    
    // 性能统计
    private volatile long totalRequests = 0;
    private volatile long totalLatency = 0;
    private volatile long maxLatency = 0;
    private volatile long minLatency = Long.MAX_VALUE;

    public InferenceEngine(Model model, InferenceConfig config) {
        this.model = model;
        this.config = config;
        this.executorService = Executors.newFixedThreadPool(config.numThreads);
        this.batchProcessor = new BatchProcessor(this, config);
        
        // 设置为评估模式
        model.eval();
        
        // 启动批处理器
        if (config.enableBatching) {
            batchProcessor.start();
        }
    }

    /**
     * 同步推理
     */
    public Variable predict(Variable input) {
        long startTime = System.nanoTime();
        
        try (NoGradContext noGrad = new NoGradContext()) {
            Variable output = model.forward(input);
            recordLatency(System.nanoTime() - startTime);
            return output;
        }
    }

    /**
     * 异步推理
     */
    public CompletableFuture<Variable> predictAsync(Variable input) {
        return CompletableFuture.supplyAsync(() -> predict(input), executorService);
    }

    /**
     * 批量推理
     */
    public List<Variable> predictBatch(List<Variable> inputs) {
        if (inputs.isEmpty()) {
            return new ArrayList<>();
        }
        
        long startTime = System.nanoTime();
        
        try (NoGradContext noGrad = new NoGradContext()) {
            // 将输入拼接成批次
            Variable batchInput = Variable.stack(inputs, 0);
            Variable batchOutput = model.forward(batchInput);
            
            // 分解批次输出
            List<Variable> outputs = unbatchOutput(batchOutput, inputs.size());
            
            recordLatency(System.nanoTime() - startTime);
            return outputs;
        }
    }

    /**
     * 动态批处理推理
     */
    public CompletableFuture<Variable> predictWithBatching(Variable input) {
        if (!config.enableBatching) {
            return predictAsync(input);
        }
        
        InferenceRequest request = new InferenceRequest(input);
        requestQueue.offer(request);
        
        return request.getFuture();
    }

    /**
     * 预热模型
     */
    public void warmup() {
        System.out.println("Warming up inference engine...");
        
        // 使用随机输入进行预热
        int[] inputShape = config.inputShape;
        for (int i = 0; i < config.warmupIterations; i++) {
            Variable warmupInput = Variable.randn(inputShape);
            predict(warmupInput);
        }
        
        System.out.println("Warmup completed.");
    }

    /**
     * 获取性能统计
     */
    public InferenceStats getStats() {
        return new InferenceStats(
            totalRequests,
            totalRequests > 0 ? totalLatency / totalRequests : 0,
            maxLatency,
            minLatency == Long.MAX_VALUE ? 0 : minLatency
        );
    }

    /**
     * 关闭推理引擎
     */
    public void shutdown() {
        if (config.enableBatching) {
            batchProcessor.stop();
        }
        executorService.shutdown();
    }

    // 私有辅助方法

    private List<Variable> unbatchOutput(Variable batchOutput, int batchSize) {
        List<Variable> outputs = new ArrayList<>();
        int[] outputShape = batchOutput.getShape();
        
        for (int i = 0; i < batchSize; i++) {
            // 简化实现：假设批次维度在第0维
            Variable singleOutput = batchOutput.slice(0, i, i + 1).squeeze(0);
            outputs.add(singleOutput);
        }
        
        return outputs;
    }

    private void recordLatency(long latencyNanos) {
        totalRequests++;
        totalLatency += latencyNanos;
        
        if (latencyNanos > maxLatency) {
            maxLatency = latencyNanos;
        }
        
        if (latencyNanos < minLatency) {
            minLatency = latencyNanos;
        }
    }

    // 批处理器
    private static class BatchProcessor {
        private final InferenceEngine engine;
        private final InferenceConfig config;
        private volatile boolean running = false;
        private Thread processingThread;

        public BatchProcessor(InferenceEngine engine, InferenceConfig config) {
            this.engine = engine;
            this.config = config;
        }

        public void start() {
            running = true;
            processingThread = new Thread(this::processRequests);
            processingThread.setDaemon(true);
            processingThread.start();
        }

        public void stop() {
            running = false;
            if (processingThread != null) {
                processingThread.interrupt();
            }
        }

        private void processRequests() {
            while (running) {
                try {
                    List<InferenceRequest> batch = collectBatch();
                    if (!batch.isEmpty()) {
                        processBatch(batch);
                    } else {
                        Thread.sleep(1); // 短暂等待
                    }
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    break;
                } catch (Exception e) {
                    System.err.println("Batch processing error: " + e.getMessage());
                }
            }
        }

        private List<InferenceRequest> collectBatch() {
            List<InferenceRequest> batch = new ArrayList<>();
            long deadline = System.currentTimeMillis() + config.batchTimeoutMs;
            
            // 收集请求直到达到批次大小或超时
            while (batch.size() < config.maxBatchSize && System.currentTimeMillis() < deadline) {
                InferenceRequest request = engine.requestQueue.poll();
                if (request != null) {
                    batch.add(request);
                } else if (batch.isEmpty()) {
                    try {
                        Thread.sleep(1);
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                        break;
                    }
                } else {
                    break; // 有部分请求，不再等待
                }
            }
            
            return batch;
        }

        private void processBatch(List<InferenceRequest> batch) {
            try {
                // 提取输入
                List<Variable> inputs = new ArrayList<>();
                for (InferenceRequest request : batch) {
                    inputs.add(request.getInput());
                }
                
                // 批量推理
                List<Variable> outputs = engine.predictBatch(inputs);
                
                // 返回结果
                for (int i = 0; i < batch.size(); i++) {
                    batch.get(i).complete(outputs.get(i));
                }
                
            } catch (Exception e) {
                // 异常处理：为所有请求设置异常
                for (InferenceRequest request : batch) {
                    request.completeExceptionally(e);
                }
            }
        }
    }

    // 推理请求类
    private static class InferenceRequest {
        private final Variable input;
        private final CompletableFuture<Variable> future;

        public InferenceRequest(Variable input) {
            this.input = input;
            this.future = new CompletableFuture<>();
        }

        public Variable getInput() { return input; }
        public CompletableFuture<Variable> getFuture() { return future; }
        
        public void complete(Variable output) {
            future.complete(output);
        }
        
        public void completeExceptionally(Throwable ex) {
            future.completeExceptionally(ex);
        }
    }
}
```

## 6.5.2 模型优化器

```java
/**
 * 模型优化器
 * 提供图优化、量化等功能
 */
public class ModelOptimizer {
    
    /**
     * 融合卷积和批归一化
     */
    public static Model fuseBatchNorm(Model model) {
        // 遍历模型层，查找Conv+BN组合
        if (model instanceof SequentialModel) {
            SequentialModel seqModel = (SequentialModel) model;
            Sequential network = seqModel.getNetwork();
            
            List<Layer> optimizedLayers = new ArrayList<>();
            
            for (int i = 0; i < network.size(); i++) {
                Layer currentLayer = network.get(i);
                
                if (currentLayer instanceof Conv2D && i + 1 < network.size()) {
                    Layer nextLayer = network.get(i + 1);
                    
                    if (nextLayer instanceof BatchNorm) {
                        // 融合Conv2D和BatchNorm
                        Conv2D conv = (Conv2D) currentLayer;
                        BatchNorm bn = (BatchNorm) nextLayer;
                        
                        Conv2D fusedConv = fuseConvBN(conv, bn);
                        optimizedLayers.add(fusedConv);
                        i++; // 跳过BatchNorm层
                    } else {
                        optimizedLayers.add(currentLayer);
                    }
                } else {
                    optimizedLayers.add(currentLayer);
                }
            }
            
            // 构建优化后的模型
            SequentialModel optimizedModel = new SequentialModel(model.getModelName() + "_optimized");
            for (Layer layer : optimizedLayers) {
                optimizedModel.add(layer);
            }
            
            return optimizedModel;
        }
        
        return model; // 不支持的模型类型，返回原模型
    }

    /**
     * 量化模型
     */
    public static Model quantizeModel(Model model, QuantizationConfig config) {
        // 简化的量化实现
        System.out.println("Applying " + config.quantizationType + " quantization...");
        
        // 遍历模型参数进行量化
        for (Map.Entry<String, Variable> entry : model.getParameters().entrySet()) {
            Variable param = entry.getValue();
            Variable quantizedParam = quantizeParameter(param, config);
            param.setData(quantizedParam.getData());
        }
        
        System.out.println("Model quantization completed.");
        return model;
    }

    /**
     * 剪枝模型
     */
    public static Model pruneModel(Model model, PruningConfig config) {
        System.out.println("Applying structured pruning...");
        
        // 计算参数重要性
        Map<String, Double> importance = computeParameterImportance(model);
        
        // 根据重要性进行剪枝
        for (Map.Entry<String, Variable> entry : model.getParameters().entrySet()) {
            String paramName = entry.getKey();
            Variable param = entry.getValue();
            
            if (importance.containsKey(paramName)) {
                double importanceScore = importance.get(paramName);
                if (importanceScore < config.pruningThreshold) {
                    // 将不重要的参数设为零
                    param.setData(Variable.zeros(param.getShape()).getData());
                }
            }
        }
        
        System.out.println("Model pruning completed.");
        return model;
    }

    // 私有辅助方法

    private static Conv2D fuseConvBN(Conv2D conv, BatchNorm bn) {
        // 简化的Conv+BN融合实现
        System.out.println("Fusing Conv2D + BatchNorm layers");
        
        // 创建融合后的卷积层
        Conv2D fusedConv = new Conv2D(
            conv.getInChannels(),
            conv.getOutChannels(),
            conv.getKernelSize(),
            conv.getStride(),
            conv.getPadding(),
            true, // 融合后总是有偏置
            conv.getPaddingMode()
        );
        
        // 实际实现需要根据BN参数调整Conv权重和偏置
        
        return fusedConv;
    }

    private static Variable quantizeParameter(Variable param, QuantizationConfig config) {
        // 简化的量化实现
        switch (config.quantizationType) {
            case INT8:
                return quantizeToInt8(param);
            case INT16:
                return quantizeToInt16(param);
            default:
                return param;
        }
    }

    private static Variable quantizeToInt8(Variable param) {
        // 8位量化实现
        double[] data = (double[]) param.getData();
        double max = Arrays.stream(data).max().orElse(1.0);
        double min = Arrays.stream(data).min().orElse(-1.0);
        
        double scale = 255.0 / (max - min);
        double[] quantized = new double[data.length];
        
        for (int i = 0; i < data.length; i++) {
            int quantizedValue = (int) Math.round((data[i] - min) * scale);
            quantizedValue = Math.max(0, Math.min(255, quantizedValue));
            quantized[i] = quantizedValue / scale + min;
        }
        
        return new Variable(quantized, param.requiresGrad());
    }

    private static Variable quantizeToInt16(Variable param) {
        // 16位量化实现（简化）
        return param; // 占位符实现
    }

    private static Map<String, Double> computeParameterImportance(Model model) {
        Map<String, Double> importance = new HashMap<>();
        
        // 简化的重要性计算：基于参数的L1范数
        for (Map.Entry<String, Variable> entry : model.getParameters().entrySet()) {
            String name = entry.getKey();
            Variable param = entry.getValue();
            
            double l1Norm = computeL1Norm(param);
            importance.put(name, l1Norm);
        }
        
        return importance;
    }

    private static double computeL1Norm(Variable param) {
        double[] data = (double[]) param.getData();
        double sum = 0.0;
        for (double value : data) {
            sum += Math.abs(value);
        }
        return sum;
    }

    // 配置类
    public static class QuantizationConfig {
        public QuantizationType quantizationType = QuantizationType.INT8;
        public boolean calibrationRequired = true;
        public int calibrationSamples = 100;
    }

    public static class PruningConfig {
        public double pruningThreshold = 0.01;
        public PruningType pruningType = PruningType.MAGNITUDE;
    }

    public enum QuantizationType {
        INT8, INT16, FP16
    }

    public enum PruningType {
        MAGNITUDE, GRADIENT_BASED, STRUCTURED
    }
}
```

## 6.5.3 推理配置

```java
/**
 * 推理引擎配置
 */
public class InferenceConfig {
    // 基本配置
    public int[] inputShape = {1, 3, 224, 224};
    public int numThreads = Runtime.getRuntime().availableProcessors();
    public boolean enableBatching = true;
    
    // 批处理配置
    public int maxBatchSize = 32;
    public long batchTimeoutMs = 10;
    public int batchQueueSize = 1000;
    
    // 性能配置
    public int warmupIterations = 10;
    public boolean enableOptimization = true;
    public boolean enableQuantization = false;
    public boolean enablePruning = false;
    
    // 内存配置
    public long maxMemoryMB = 1024;
    public boolean enableMemoryPool = true;
    
    public static InferenceConfig defaultConfig() {
        return new InferenceConfig();
    }
    
    public static InferenceConfig highThroughputConfig() {
        InferenceConfig config = new InferenceConfig();
        config.enableBatching = true;
        config.maxBatchSize = 64;
        config.batchTimeoutMs = 5;
        config.numThreads = Runtime.getRuntime().availableProcessors() * 2;
        return config;
    }
    
    public static InferenceConfig lowLatencyConfig() {
        InferenceConfig config = new InferenceConfig();
        config.enableBatching = false;
        config.numThreads = 1;
        config.warmupIterations = 50;
        return config;
    }
}
```

## 6.5.4 性能监控

```java
/**
 * 推理性能统计
 */
public class InferenceStats {
    private final long totalRequests;
    private final long averageLatencyNanos;
    private final long maxLatencyNanos;
    private final long minLatencyNanos;

    public InferenceStats(long totalRequests, long averageLatencyNanos, 
                         long maxLatencyNanos, long minLatencyNanos) {
        this.totalRequests = totalRequests;
        this.averageLatencyNanos = averageLatencyNanos;
        this.maxLatencyNanos = maxLatencyNanos;
        this.minLatencyNanos = minLatencyNanos;
    }

    public double getThroughputQPS() {
        if (averageLatencyNanos == 0) return 0.0;
        return 1_000_000_000.0 / averageLatencyNanos;
    }

    public double getAverageLatencyMs() {
        return averageLatencyNanos / 1_000_000.0;
    }

    public double getMaxLatencyMs() {
        return maxLatencyNanos / 1_000_000.0;
    }

    public double getMinLatencyMs() {
        return minLatencyNanos / 1_000_000.0;
    }

    @Override
    public String toString() {
        return String.format(
            "InferenceStats{requests=%d, avgLatency=%.2fms, maxLatency=%.2fms, " +
            "minLatency=%.2fms, throughput=%.2f QPS}",
            totalRequests, getAverageLatencyMs(), getMaxLatencyMs(), 
            getMinLatencyMs(), getThroughputQPS()
        );
    }
}

/**
 * 性能监控器
 */
public class PerformanceMonitor {
    private final InferenceEngine engine;
    private final long monitoringIntervalMs;
    private volatile boolean monitoring = false;
    private Thread monitoringThread;

    public PerformanceMonitor(InferenceEngine engine, long monitoringIntervalMs) {
        this.engine = engine;
        this.monitoringIntervalMs = monitoringIntervalMs;
    }

    public void startMonitoring() {
        monitoring = true;
        monitoringThread = new Thread(this::monitorPerformance);
        monitoringThread.setDaemon(true);
        monitoringThread.start();
    }

    public void stopMonitoring() {
        monitoring = false;
        if (monitoringThread != null) {
            monitoringThread.interrupt();
        }
    }

    private void monitorPerformance() {
        while (monitoring) {
            try {
                InferenceStats stats = engine.getStats();
                logPerformanceMetrics(stats);
                
                Thread.sleep(monitoringIntervalMs);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            }
        }
    }

    private void logPerformanceMetrics(InferenceStats stats) {
        System.out.printf("[PERF] %s\n", stats.toString());
        
        // 可以添加更多的监控逻辑，如内存使用情况
        Runtime runtime = Runtime.getRuntime();
        long usedMemory = runtime.totalMemory() - runtime.freeMemory();
        System.out.printf("[MEMORY] Used: %.2f MB\n", usedMemory / (1024.0 * 1024.0));
    }
}
```

## 6.5.5 使用示例

```java
/**
 * 推理引擎使用示例
 */
public class InferenceExample {
    
    public static void demonstrateBasicInference() {
        // 创建和训练模型
        SequentialModel model = createTrainedModel();
        
        // 创建推理引擎
        InferenceConfig config = InferenceConfig.defaultConfig();
        InferenceEngine engine = new InferenceEngine(model, config);
        
        // 预热
        engine.warmup();
        
        // 单次推理
        Variable input = Variable.randn(new int[]{1, 3, 224, 224});
        Variable output = engine.predict(input);
        System.out.println("Prediction shape: " + Arrays.toString(output.getShape()));
        
        // 批量推理
        List<Variable> inputs = Arrays.asList(
            Variable.randn(new int[]{1, 3, 224, 224}),
            Variable.randn(new int[]{1, 3, 224, 224}),
            Variable.randn(new int[]{1, 3, 224, 224})
        );
        
        List<Variable> outputs = engine.predictBatch(inputs);
        System.out.println("Batch prediction completed: " + outputs.size() + " results");
        
        // 查看性能统计
        InferenceStats stats = engine.getStats();
        System.out.println("Performance: " + stats);
        
        engine.shutdown();
    }
    
    public static void demonstrateAsyncInference() {
        SequentialModel model = createTrainedModel();
        InferenceConfig config = InferenceConfig.highThroughputConfig();
        InferenceEngine engine = new InferenceEngine(model, config);
        
        // 启动性能监控
        PerformanceMonitor monitor = new PerformanceMonitor(engine, 5000);
        monitor.startMonitoring();
        
        // 异步推理
        List<CompletableFuture<Variable>> futures = new ArrayList<>();
        
        for (int i = 0; i < 100; i++) {
            Variable input = Variable.randn(new int[]{1, 3, 224, 224});
            CompletableFuture<Variable> future = engine.predictWithBatching(input);
            futures.add(future);
        }
        
        // 等待所有推理完成
        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
                .thenRun(() -> {
                    System.out.println("All async predictions completed");
                    System.out.println("Final stats: " + engine.getStats());
                })
                .join();
        
        monitor.stopMonitoring();
        engine.shutdown();
    }
    
    public static void demonstrateModelOptimization() {
        SequentialModel model = createTrainedModel();
        
        // 模型优化
        System.out.println("Original model parameters: " + model.getTotalParameters());
        
        // 融合BatchNorm
        Model optimizedModel = ModelOptimizer.fuseBatchNorm(model);
        
        // 量化
        ModelOptimizer.QuantizationConfig quantConfig = new ModelOptimizer.QuantizationConfig();
        quantConfig.quantizationType = ModelOptimizer.QuantizationType.INT8;
        optimizedModel = ModelOptimizer.quantizeModel(optimizedModel, quantConfig);
        
        // 剪枝
        ModelOptimizer.PruningConfig pruneConfig = new ModelOptimizer.PruningConfig();
        pruneConfig.pruningThreshold = 0.01;
        optimizedModel = ModelOptimizer.pruneModel(optimizedModel, pruneConfig);
        
        // 使用优化后的模型创建推理引擎
        InferenceConfig config = InferenceConfig.lowLatencyConfig();
        InferenceEngine engine = new InferenceEngine(optimizedModel, config);
        
        // 性能测试
        benchmarkInference(engine);
        
        engine.shutdown();
    }
    
    private static void benchmarkInference(InferenceEngine engine) {
        System.out.println("Starting inference benchmark...");
        
        int numIterations = 1000;
        Variable input = Variable.randn(new int[]{1, 3, 224, 224});
        
        // 预热
        for (int i = 0; i < 100; i++) {
            engine.predict(input);
        }
        
        // 基准测试
        long startTime = System.nanoTime();
        for (int i = 0; i < numIterations; i++) {
            engine.predict(input);
        }
        long endTime = System.nanoTime();
        
        double avgLatencyMs = (endTime - startTime) / (numIterations * 1_000_000.0);
        double throughputQPS = 1000.0 / avgLatencyMs;
        
        System.out.printf("Benchmark Results:\n");
        System.out.printf("Iterations: %d\n", numIterations);
        System.out.printf("Average Latency: %.2f ms\n", avgLatencyMs);
        System.out.printf("Throughput: %.2f QPS\n", throughputQPS);
        System.out.printf("Final Stats: %s\n", engine.getStats());
    }
    
    private static SequentialModel createTrainedModel() {
        SequentialModel model = new SequentialModel("ImageClassifier");
        model.add(new Conv2D(3, 32, new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1}, true, "zeros"))
             .add(new BatchNorm(32))
             .add(new ReLU())
             .add(new MaxPool2D(new int[]{2, 2}, new int[]{2, 2}))
             .add(new Conv2D(32, 64, new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1}, true, "zeros"))
             .add(new BatchNorm(64))
             .add(new ReLU())
             .add(new AdaptiveAvgPool2D(new int[]{1, 1}))
             .add(new Flatten())
             .add(new Dense(64, 10));
        
        model.initializeParameters(new int[]{1, 3, 224, 224});
        return model;
    }
}
```

推理引擎作为深度学习模型部署的关键组件，提供了高性能、可扩展的推理服务。通过合理的架构设计、模型优化和性能监控，我们构建了一个完整的推理解决方案，能够满足从低延迟到高吞吐量的各种部署需求。

至此，我们完成了TinyAI深度学习框架的核心组件设计与实现，从底层的自动微分引擎到高层的模型管理，构建了一个功能完备、设计优雅的深度学习框架。