# 自然语言处理示例

<cite>
**本文档引用的文件**
- [GPT2Example.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Demo.java)
- [GPT2Model.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Model.java)
- [GPT2Block.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Block.java)
- [SimpleTokenizer.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/nlp/SimpleTokenizer.java)
- [EmbeddingFullExample.java](file://tinyai-deeplearning-case/src/main/java/io/leavesfly/tinyai/example/embedd/EmbeddingFullExample.java)
- [CompleteRnnExample.java](file://tinyai-deeplearning-case/src/main/java/io/leavesfly/tinyai/example/rnn/CompleteRnnExample.java)
- [Word2VecExample.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/nlp/Word2VecExample.java)
- [GPT3Demo.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt3/GPT3Demo.java)
</cite>

## 更新摘要
**变更内容**
- 根据代码变更，更新了文档中引用的文件路径，将旧的`GPT2Example.java`替换为新的`GPT2Demo.java`
- 新增了对`GPT3Demo.java`文件的引用，以反映GPT-3模型演示代码的新增
- 移除了已删除的测试文件引用，保持文档与代码库同步
- 更新了所有受影响的章节来源和图表来源，确保引用的准确性

## 目录
1. [简介](#简介)
2. [项目结构概览](#项目结构概览)
3. [核心组件分析](#核心组件分析)
4. [GPT-2模型架构详解](#gpt-2模型架构详解)
5. [文本编码与分词系统](#文本编码与分词系统)
6. [词向量训练与应用](#词向量训练与应用)
7. [RNN序列建模对比](#rnn序列建模对比)
8. [输入掩码与序列处理](#输入掩码与序列处理)
9. [模型微调与部署](#模型微调与部署)
10. [性能优化建议](#性能优化建议)
11. [故障排除指南](#故障排除指南)
12. [总结](#总结)

## 简介

本文档深入解析TinyAI框架中的自然语言处理示例，重点介绍GPT-2模型的文本生成实现、Word2Vec词嵌入技术、SimpleTokenizer文本编码系统，以及CompleteRnnExample中RNN在序列建模中的应用。通过结合EmbeddingFullExample说明词向量训练过程，帮助开发者全面理解现代NLP技术的核心原理和实践应用。

## 项目结构概览

TinyAI项目采用模块化设计，将自然语言处理功能分布在多个专门的模块中：

```mermaid
graph TB
subgraph "核心NLP模块"
GPT[gpt包]
Embedding[embedd包]
RNN[rnn包]
end
subgraph "案例示例模块"
Case[tinyai-dl-case]
Examples[示例代码]
end
subgraph "底层支持"
NDArray[ndarr包]
Network[nnet包]
ML[ml包]
end
GPT --> Network
Embedding --> Network
RNN --> Network
Case --> GPT
Case --> Embedding
Case --> RNN
Network --> NDArray
Network --> ML
```

**图表来源**
- [GPT2Demo.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Demo.java#L1-L30)
- [EmbeddingFullExample.java](file://tinyai-deeplearning-case/src/main/java/io/leavesfly/tinyai/example/embedd/EmbeddingFullExample.java#L1-L20)

## 核心组件分析

### GPT-2模型核心组件

GPT-2模型是基于Transformer解码器架构的自回归语言模型，其核心组件包括：

```mermaid
classDiagram
class GPT2Model {
-int vocabSize
-int dModel
-int numLayers
-int numHeads
-int dFF
-int maxSeqLength
-double dropoutRate
-GPT2TokenEmbedding tokenEmbedding
-GPT2Block[] transformerBlocks
-LayerNorm finalLayerNorm
-GPT2OutputHead outputHead
+createTinyModel(name, vocabSize) GPT2Model
+layerForward(inputs) Variable
+predictNextToken(input) int
+printModelInfo() void
}
class GPT2Block {
-LayerNorm layerNorm1
-MultiHeadAttention attention
-LayerNorm layerNorm2
-FeedForward feedForward
-double dropoutRate
+layerForward(inputs) Variable
+addResidualConnection(input, output) Variable
}
class SimpleTokenizer {
-Map~String,Integer~ vocab
-Map~Integer,String~ reverseVocab
-int vocabSize
-Pattern tokenPattern
+buildVocab(texts, minFreq, maxVocabSize) void
+encode(text, addSpecialTokens) int[]
+decode(tokens, removeSpecialTokens) String
+pad(tokens, maxLength, direction) int[]
}
class Word2Vec {
-int vocabSize
-int embedSize
-TrainingMode mode
-int windowSize
-boolean useNegativeSampling
+buildVocab(corpus) void
+generateTrainingSamples(corpus) TrainingSample[]
+train(samples, epochs) void
}
GPT2Model --> GPT2Block : "包含多个"
GPT2Model --> SimpleTokenizer : "使用"
GPT2Model --> Word2Vec : "可选集成"
GPT2Block --> MultiHeadAttention : "包含"
GPT2Block --> FeedForward : "包含"
```

**图表来源**
- [GPT2Model.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Model.java#L30-L80)
- [GPT2Block.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Block.java#L25-L50)
- [SimpleTokenizer.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/nlp/SimpleTokenizer.java#L15-L60)

**章节来源**
- [GPT2Model.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Model.java#L1-L100)
- [GPT2Block.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Block.java#L1-L80)

## GPT-2模型架构详解

### 多头注意力机制

GPT-2的核心是Transformer解码器架构，其中多头注意力机制负责捕捉序列内部的复杂依赖关系：

```mermaid
sequenceDiagram
participant Input as "输入序列"
participant LN1 as "LayerNorm1"
participant MHA as "多头注意力"
participant Res1 as "残差连接1"
participant LN2 as "LayerNorm2"
participant FFN as "前馈网络"
participant Res2 as "残差连接2"
participant Output as "输出"
Input->>LN1 : 输入序列
LN1->>MHA : 归一化后的输入
MHA->>MHA : 计算注意力权重
MHA->>Res1 : 注意力输出
Res1->>Res1 : 加上原始输入
Res1->>LN2 : 残差输出
LN2->>FFN : 归一化后的输入
FFN->>Res2 : 前馈网络输出
Res2->>Res2 : 加上原始输入
Res2->>Output : 最终输出
Note over MHA : 使用因果掩码防止未来信息泄露
Note over FFN : 使用GELU激活函数
```

**图表来源**
- [GPT2Block.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Block.java#L65-L85)

### 前馈网络结构

每个GPT-2Block包含两个主要子层：多头自注意力和前馈网络：

```mermaid
flowchart TD
Input["输入张量<br/>(batch_size, seq_len, d_model)"] --> LayerNorm1["层归一化1"]
LayerNorm1 --> MultiHeadAttn["多头自注意力<br/>带因果掩码"]
MultiHeadAttn --> Residual1["残差连接1"]
Input -.-> Residual1
Residual1 --> LayerNorm2["层归一化2"]
LayerNorm2 --> FeedForward["前馈网络<br/>线性→GELU→线性"]
FeedForward --> Residual2["残差连接2"]
Residual1 -.-> Residual2
Residual2 --> Output["输出张量<br/>(batch_size, seq_len, d_model)"]
style MultiHeadAttn fill:#e1f5fe
style FeedForward fill:#f3e5f5
style Residual1 fill:#fff3e0
style Residual2 fill:#fff3e0
```

**图表来源**
- [GPT2Block.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Block.java#L65-L85)

**章节来源**
- [GPT2Block.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Block.java#L65-L120)

## 文本编码与分词系统

### SimpleTokenizer实现原理

SimpleTokenizer提供了完整的文本预处理功能，支持词汇表构建、文本编码和解码：

```mermaid
flowchart LR
subgraph "文本预处理流程"
Text["原始文本"] --> Lower["转小写"]
Lower --> Split["正则分割"]
Split --> Filter["过滤空词"]
Filter --> TokenList["Token列表"]
end
subgraph "词汇表管理"
TokenList --> BuildVocab["构建词汇表"]
BuildVocab --> SortFreq["按频率排序"]
SortFreq --> LimitSize["限制大小"]
LimitSize --> VocabMap["词汇表映射"]
end
subgraph "编码解码"
VocabMap --> Encode["文本编码"]
VocabMap --> Decode["Token解码"]
Encode --> TokenIDs["Token ID数组"]
TokenIDs --> Decode
end
style Text fill:#e8f5e8
style VocabMap fill:#fff2cc
style TokenIDs fill:#ffebee
```

**图表来源**
- [SimpleTokenizer.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/nlp/SimpleTokenizer.java#L100-L150)

### 特殊Token处理

SimpleTokenizer定义了四种特殊Token，用于序列控制和边界标记：

- `<pad>` (ID: 0): 填充Token，用于序列对齐
- `<unk>` (ID: 1): 未知Token，用于未登录词
- `<bos>` (ID: 2): 序列开始Token
- `<eos>` (ID: 3): 序列结束Token

**章节来源**
- [SimpleTokenizer.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/nlp/SimpleTokenizer.java#L15-L80)

## 词向量训练与应用

### Word2Vec模型架构

Word2Vec是Google开发的词向量生成模型，支持两种训练模式：

```mermaid
graph TB
subgraph "Skip-gram模式"
Center1["中心词"] --> Context1["上下文词预测"]
Context1 --> Predict1["预测概率分布"]
end
subgraph "CBOW模式"
Context2["上下文词"] --> Center2["中心词预测"]
Center2 --> Predict2["预测概率分布"]
end
subgraph "训练过程"
Corpus["语料库"] --> BuildVocab["构建词汇表"]
BuildVocab --> GenSamples["生成训练样本"]
GenSamples --> TrainLoop["训练循环"]
TrainLoop --> UpdateWeights["更新词向量"]
end
style Center1 fill:#e3f2fd
style Context2 fill:#e8f5e8
style UpdateWeights fill:#fff3e0
```

**图表来源**
- [Word2VecExample.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/nlp/Word2VecExample.java#L40-L80)

### 词向量训练流程

Word2Vec训练分为三个主要阶段：

1. **词汇表构建**: 统计词频，过滤低频词，限制词汇表大小
2. **样本生成**: 根据选择的训练模式（Skip-gram或CBOW）生成训练样本
3. **模型训练**: 使用梯度下降优化词向量参数

**章节来源**
- [Word2VecExample.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/nlp/Word2VecExample.java#L40-L120)

## RNN序列建模对比

### 不同RNN架构比较

CompleteRnnExample展示了三种递归神经网络架构的性能对比：

```mermaid
graph LR
subgraph "SimpleRNN"
SR_Input["输入序列"] --> SR_Hidden["隐藏状态"]
SR_Hidden --> SR_Output["输出"]
SR_Hidden --> SR_Hidden
end
subgraph "LSTM"
LSTM_Input["输入序列"] --> LSTM_Cell["LSTM单元"]
LSTM_Cell --> LSTM_Output["输出"]
LSTM_Cell --> LSTM_CellState["细胞状态"]
LSTM_Cell --> LSTM_Hidden["隐藏状态"]
end
subgraph "GRU"
GRU_Input["输入序列"] --> GRU_Cell["GRU单元"]
GRU_Cell --> GRU_Output["输出"]
GRU_Cell --> GRU_Hidden["隐藏状态"]
end
style LSTM fill:#e8f5e8
style GRU fill:#e3f2fd
style SR_Input fill:#fff2cc
```

**图表来源**
- [CompleteRnnExample.java](file://tinyai-deeplearning-case/src/main/java/io/leavesfly/tinyai/example/rnn/CompleteRnnExample.java#L40-L100)

### RNN训练策略

RNN训练的关键在于状态管理和梯度处理：

```mermaid
sequenceDiagram
participant Data as "训练数据"
participant Model as "RNN模型"
participant Loss as "损失函数"
participant Optimizer as "优化器"
loop 每个训练批次
Data->>Model : 输入序列
Model->>Model : 前向传播
Model->>Loss : 计算预测误差
Loss->>Model : 反向传播
Model->>Optimizer : 更新参数
Model->>Model : 清理隐藏状态
end
Note over Model : 每个批次后重置状态
Note over Model : 切断计算图避免内存泄漏
```

**图表来源**
- [CompleteRnnExample.java](file://tinyai-deeplearning-case/src/main/java/io/leavesfly/tinyai/example/rnn/CompleteRnnExample.java#L140-L180)

**章节来源**
- [CompleteRnnExample.java](file://tinyai-deeplearning-case/src/main/java/io/leavesfly/tinyai/example/rnn/CompleteRnnExample.java#L1-L100)

## 输入掩码与序列处理

### 序列截断与填充机制

在GPT-2模型中，输入序列需要进行适当的截断和填充处理：

```mermaid
flowchart TD
InputSeq["输入序列<br/>长度: 15"] --> CheckLength{"检查长度"}
CheckLength --> |≤ max_seq_length| DirectPass["直接传递"]
CheckLength --> |> max_seq_length| Truncate["截断到最大长度"]
Truncate --> Pad["填充到固定长度"]
DirectPass --> Pad
Pad --> Mask["应用因果掩码"]
Mask --> ModelInput["模型输入"]
subgraph "填充策略"
Pad --> PrePad["前填充"]
Pad --> PostPad["后填充"]
end
style InputSeq fill:#ffebee
style ModelInput fill:#e8f5e8
style Mask fill:#e3f2fd
```

**图表来源**
- [GPT2Demo.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Demo.java#L200-L250)

### 自回归生成逻辑

GPT-2的文本生成采用自回归方式，逐步预测下一个Token：

```mermaid
sequenceDiagram
participant Prompt as "提示文本"
participant Tokenizer as "分词器"
participant Model as "GPT-2模型"
participant Sampler as "采样器"
participant Output as "生成文本"
Prompt->>Tokenizer : 编码提示
Tokenizer->>Model : 输入Token序列
loop 生成新Token
Model->>Model : 预测下一个Token概率
Model->>Sampler : 输出概率分布
Sampler->>Sampler : 采样下一个Token
Sampler->>Model : 添加到序列
alt 达到最大长度或EOS
Sampler->>Output : 结束生成
else 继续生成
Model->>Model : 循环预测
end
end
Note over Model : 使用因果掩码确保自回归
Note over Sampler : 可以使用贪婪采样或随机采样
```

**图表来源**
- [GPT2Demo.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Demo.java#L250-L300)

**章节来源**
- [GPT2Demo.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Demo.java#L200-L307)

## 模型微调与部署

### 预训练模型微调策略

GPT-2模型可以通过以下方式进行微调：

1. **全量微调**: 更新所有模型参数
2. **部分微调**: 冻结部分层，只更新顶层参数
3. **适配器微调**: 添加小型适配器层，保持原模型不变

### 模型序列化与加载

```mermaid
flowchart LR
subgraph "模型保存"
Model["训练好的模型"] --> Serialize["序列化参数"]
Serialize --> SaveFile["保存到文件"]
end
subgraph "模型加载"
LoadFile["加载文件"] --> Deserialize["反序列化参数"]
Deserialize --> ModelInstance["模型实例"]
end
subgraph "推理部署"
ModelInstance --> Inference["在线推理"]
ModelInstance --> BatchInference["批量推理"]
end
SaveFile -.-> LoadFile
style Serialize fill:#e8f5e8
style Deserialize fill:#e3f2fd
style Inference fill:#fff2cc
```

## 性能优化建议

### 训练优化策略

1. **学习率调度**: 使用学习率衰减策略
2. **梯度裁剪**: 防止梯度爆炸
3. **混合精度训练**: 使用FP16减少内存占用
4. **分布式训练**: 利用多GPU加速训练

### 推理优化技巧

1. **KV缓存**: 缓存注意力键值对，提高生成效率
2. **批处理**: 合理设置批大小平衡速度和内存
3. **量化**: 使用INT8量化减少推理延迟
4. **模型压缩**: 通过剪枝和蒸馏减少模型大小

## 故障排除指南

### 常见问题及解决方案

1. **内存不足错误**
   - 减少批次大小
   - 使用梯度累积
   - 启用混合精度训练

2. **训练不稳定**
   - 调整学习率
   - 使用梯度裁剪
   - 检查数据预处理

3. **生成质量差**
   - 增加训练轮数
   - 调整采样温度
   - 检查模型架构

**章节来源**
- [GPT2Demo.java](file://tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt2/GPT2Demo.java#L80-L150)

## 总结

本文档全面介绍了TinyAI框架中的自然语言处理示例，涵盖了从基础的词向量训练到复杂的GPT-2模型架构。通过深入分析各个组件的设计原理和实现细节，开发者可以更好地理解和应用这些先进的NLP技术。

主要收获包括：

1. **GPT-2模型架构**: 理解了多头注意力、前馈网络和残差连接的工作原理
2. **文本预处理**: 掌握了分词器的使用和特殊Token的处理方法
3. **词向量训练**: 学习了Word2Vec的不同训练模式和应用场景
4. **序列建模**: 对比了不同RNN架构的优缺点和适用场景
5. **性能优化**: 了解了训练和推理阶段的优化策略

这些知识为构建高质量的自然语言处理应用奠定了坚实的基础。