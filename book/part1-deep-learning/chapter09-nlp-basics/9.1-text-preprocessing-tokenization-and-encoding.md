# 9.1 文本预处理：分词与编码

> **本节学习目标**：掌握文本数据的预处理技术和编码方法，为后续的NLP模型训练做好数据准备

## 内容概览

在自然语言处理任务中，原始文本数据通常需要经过一系列预处理步骤才能被机器学习模型有效利用。这些预处理步骤包括文本清洗、分词、编码等，它们直接影响着模型的性能和效果。本节将详细介绍文本预处理的核心技术，并基于TinyAI框架实现一个完整的文本预处理流水线。

## 为什么需要文本预处理？

文本数据与图像或数值数据不同，它是一种非结构化的符号序列。为了让计算机能够处理和理解文本，我们需要将其转换为数值形式。这个转换过程中涉及多个关键步骤：

1. **文本清洗**：去除无关字符、标点符号、HTML标签等噪声数据
2. **分词处理**：将连续的文本切分为有意义的词汇单元
3. **编码转换**：将词汇映射为数值索引，形成模型可处理的数字序列

## 9.1.1 文本清洗和标准化

文本清洗是预处理的第一步，目的是去除噪声数据，保留有用信息。

### 常见的文本清洗操作

```java
/**
 * 文本清洗工具类
 */
public class TextCleaner {
    
    /**
     * 清洗文本，移除HTML标签、特殊字符等
     * @param text 原始文本
     * @return 清洗后的文本
     */
    public static String cleanText(String text) {
        if (text == null || text.isEmpty()) {
            return "";
        }
        
        // 移除HTML标签
        String cleaned = text.replaceAll("<[^>]*>", "");
        
        // 移除多余的空白字符
        cleaned = cleaned.replaceAll("\\s+", " ");
        
        // 移除特殊字符（保留字母、数字、中文和基本标点）
        cleaned = cleaned.replaceAll("[^\\w\\s\\u4e00-\\u9fa5\\p{P}]", "");
        
        // 去除首尾空格
        cleaned = cleaned.trim();
        
        return cleaned;
    }
    
    /**
     * 文本标准化处理
     * @param text 原始文本
     * @return 标准化后的文本
     */
    public static String normalizeText(String text) {
        if (text == null || text.isEmpty()) {
            return "";
        }
        
        // 转换为小写（英文）
        String normalized = text.toLowerCase();
        
        // 统一数字格式（如将"1,000"转换为"1000"）
        normalized = normalized.replaceAll(",", "");
        
        // 统一日期格式等
        // 这里可以根据具体需求添加更多标准化规则
        
        return normalized;
    }
}
```

### 使用示例

```java
public class TextCleaningExample {
    public static void main(String[] args) {
        String rawText = "<p>这是一个包含HTML标签的文本！价格是$1,234.56。</p>";
        System.out.println("原始文本: " + rawText);
        
        String cleaned = TextCleaner.cleanText(rawText);
        System.out.println("清洗后文本: " + cleaned);
        
        String normalized = TextCleaner.normalizeText(cleaned);
        System.out.println("标准化后文本: " + normalized);
    }
}
```

## 9.1.2 分词算法详解

分词是将连续的文本切分为有意义的词汇单元的过程。不同的语言有不同的分词需求，比如英文可以通过空格自然分词，而中文则需要专门的分词算法。

### 基于规则的分词

最简单的分词方式是基于分隔符的分词，适用于英文等以空格分隔的语言：

```java
/**
 * 基于分隔符的简单分词器
 */
public class SimpleTokenizer {
    
    /**
     * 基于空格的分词
     * @param text 输入文本
     * @return 词汇列表
     */
    public static List<String> tokenizeByWhitespace(String text) {
        if (text == null || text.isEmpty()) {
            return new ArrayList<>();
        }
        
        // 按空格分割并过滤空字符串
        return Arrays.stream(text.split("\\s+"))
                .filter(token -> !token.isEmpty())
                .collect(Collectors.toList());
    }
    
    /**
     * 基于标点符号的分词
     * @param text 输入文本
     * @return 词汇列表
     */
    public static List<String> tokenizeByPunctuation(String text) {
        if (text == null || text.isEmpty()) {
            return new ArrayList<>();
        }
        
        // 在标点符号前后添加分隔符
        String punctSeparated = text.replaceAll("([\\p{P}])", " $1 ");
        
        // 按空格分割并过滤
        return Arrays.stream(punctSeparated.split("\\s+"))
                .filter(token -> !token.isEmpty())
                .collect(Collectors.toList());
    }
}
```

### 中文分词实现

对于中文文本，我们需要更复杂的分词算法。这里我们实现一个基于词典的前向最大匹配分词器：

```java
/**
 * 基于词典的前向最大匹配中文分词器
 */
public class ChineseTokenizer {
    private Set<String> dictionary;
    private int maxWordLength;
    
    /**
     * 构造函数
     * @param dictionary 词典集合
     */
    public ChineseTokenizer(Set<String> dictionary) {
        this.dictionary = dictionary;
        this.maxWordLength = dictionary.stream()
                .mapToInt(String::length)
                .max()
                .orElse(0);
    }
    
    /**
     * 前向最大匹配分词
     * @param text 输入文本
     * @return 词汇列表
     */
    public List<String> forwardMaxMatch(String text) {
        List<String> result = new ArrayList<>();
        int position = 0;
        
        while (position < text.length()) {
            String matchedWord = null;
            int wordLength = Math.min(maxWordLength, text.length() - position);
            
            // 从最大长度开始尝试匹配
            for (int i = wordLength; i > 0; i--) {
                String candidate = text.substring(position, position + i);
                if (dictionary.contains(candidate)) {
                    matchedWord = candidate;
                    break;
                }
            }
            
            // 如果没有匹配到词典中的词，则取单个字符
            if (matchedWord == null) {
                matchedWord = String.valueOf(text.charAt(position));
            }
            
            result.add(matchedWord);
            position += matchedWord.length();
        }
        
        return result;
    }
}
```

### 使用示例

```java
public class TokenizerExample {
    public static void main(String[] args) {
        // 英文分词示例
        String englishText = "Hello, world! This is a sample sentence.";
        List<String> englishTokens = SimpleTokenizer.tokenizeByPunctuation(englishText);
        System.out.println("英文分词结果: " + englishTokens);
        
        // 中文分词示例
        Set<String> dict = new HashSet<>(Arrays.asList(
            "自然语言", "处理", "技术", "非常", "重要", "在", "现代", "人工智能", "中"
        ));
        
        ChineseTokenizer chineseTokenizer = new ChineseTokenizer(dict);
        String chineseText = "自然语言处理技术在现代人工智能中非常重要";
        List<String> chineseTokens = chineseTokenizer.forwardMaxMatch(chineseText);
        System.out.println("中文分词结果: " + chineseTokens);
    }
}
```

## 9.1.3 词汇表构建和管理

在将文本转换为数值序列之前，我们需要构建一个词汇表（Vocabulary），用于维护词汇与其索引之间的映射关系。

```java
/**
 * 词汇表管理类
 */
public class Vocabulary {
    private Map<String, Integer> wordToIndex;
    private Map<Integer, String> indexToWord;
    private int nextIndex;
    
    // 特殊标记
    public static final String PAD_TOKEN = "<PAD>";  // 填充标记
    public static final String UNK_TOKEN = "<UNK>";  // 未知词标记
    public static final String SOS_TOKEN = "<SOS>";  // 句子开始标记
    public static final String EOS_TOKEN = "<EOS>";  // 句子结束标记
    
    /**
     * 构造函数，初始化特殊标记
     */
    public Vocabulary() {
        this.wordToIndex = new HashMap<>();
        this.indexToWord = new HashMap<>();
        this.nextIndex = 0;
        
        // 添加特殊标记
        addToken(PAD_TOKEN);
        addToken(UNK_TOKEN);
        addToken(SOS_TOKEN);
        addToken(EOS_TOKEN);
    }
    
    /**
     * 添加词汇到词汇表
     * @param word 词汇
     * @return 词汇索引
     */
    public int addToken(String word) {
        if (!wordToIndex.containsKey(word)) {
            wordToIndex.put(word, nextIndex);
            indexToWord.put(nextIndex, word);
            nextIndex++;
        }
        return wordToIndex.get(word);
    }
    
    /**
     * 批量添加词汇
     * @param words 词汇列表
     */
    public void addTokens(List<String> words) {
        for (String word : words) {
            addToken(word);
        }
    }
    
    /**
     * 获取词汇索引
     * @param word 词汇
     * @return 索引，如果词汇不存在则返回UNK_TOKEN的索引
     */
    public int getIndex(String word) {
        return wordToIndex.getOrDefault(word, wordToIndex.get(UNK_TOKEN));
    }
    
    /**
     * 根据索引获取词汇
     * @param index 索引
     * @return 词汇，如果索引不存在则返回UNK_TOKEN
     */
    public String getWord(int index) {
        return indexToWord.getOrDefault(index, UNK_TOKEN);
    }
    
    /**
     * 获取词汇表大小
     * @return 词汇表大小
     */
    public int size() {
        return wordToIndex.size();
    }
    
    /**
     * 获取特殊标记索引
     */
    public int getPadIndex() { return getIndex(PAD_TOKEN); }
    public int getUnkIndex() { return getIndex(UNK_TOKEN); }
    public int getSosIndex() { return getIndex(SOS_TOKEN); }
    public int getEosIndex() { return getIndex(EOS_TOKEN); }
}
```

## 9.1.4 序列填充和截断

由于模型通常需要固定长度的输入序列，我们需要对文本序列进行填充或截断处理。

```java
/**
 * 序列处理工具类
 */
public class SequenceProcessor {
    
    /**
     * 对序列进行填充或截断
     * @param sequence 原始序列
     * @param maxLength 目标长度
     * @param padValue 填充值
     * @param truncateFromLeft 是否从左侧截断
     * @return 处理后的序列
     */
    public static List<Integer> padOrTruncate(List<Integer> sequence, 
                                            int maxLength, 
                                            int padValue, 
                                            boolean truncateFromLeft) {
        if (sequence.size() == maxLength) {
            return new ArrayList<>(sequence);
        }
        
        List<Integer> result = new ArrayList<>();
        
        if (sequence.size() < maxLength) {
            // 填充
            result.addAll(sequence);
            while (result.size() < maxLength) {
                result.add(padValue);
            }
        } else {
            // 截断
            if (truncateFromLeft) {
                result.addAll(sequence.subList(sequence.size() - maxLength, sequence.size()));
            } else {
                result.addAll(sequence.subList(0, maxLength));
            }
        }
        
        return result;
    }
    
    /**
     * 批量处理序列
     * @param sequences 序列列表
     * @param maxLength 目标长度
     * @param padValue 填充值
     * @return 处理后的序列数组
     */
    public static int[][] batchPadOrTruncate(List<List<Integer>> sequences, 
                                           int maxLength, 
                                           int padValue) {
        int batchSize = sequences.size();
        int[][] result = new int[batchSize][maxLength];
        
        for (int i = 0; i < batchSize; i++) {
            List<Integer> padded = padOrTruncate(sequences.get(i), maxLength, padValue, false);
            for (int j = 0; j < maxLength; j++) {
                result[i][j] = padded.get(j);
            }
        }
        
        return result;
    }
}
```

## 9.1.5 完整的文本预处理流水线

现在我们将所有组件组合成一个完整的文本预处理流水线：

```java
/**
 * 完整的文本预处理流水线
 */
public class TextPreprocessor {
    private Vocabulary vocabulary;
    private ChineseTokenizer tokenizer; // 也可以替换为其他分词器
    
    public TextPreprocessor(Vocabulary vocabulary, ChineseTokenizer tokenizer) {
        this.vocabulary = vocabulary;
        this.tokenizer = tokenizer;
    }
    
    /**
     * 预处理单个文本
     * @param text 原始文本
     * @param maxLength 序列最大长度
     * @return 处理后的数值序列
     */
    public int[] preprocessText(String text, int maxLength) {
        // 1. 文本清洗
        String cleaned = TextCleaner.cleanText(text);
        String normalized = TextCleaner.normalizeText(cleaned);
        
        // 2. 分词
        List<String> tokens = tokenizer.forwardMaxMatch(normalized);
        
        // 3. 添加句子边界标记
        tokens.add(0, Vocabulary.SOS_TOKEN);
        tokens.add(Vocabulary.EOS_TOKEN);
        
        // 4. 转换为索引序列
        List<Integer> indices = tokens.stream()
                .map(vocabulary::getIndex)
                .collect(Collectors.toList());
        
        // 5. 填充或截断
        List<Integer> processed = SequenceProcessor.padOrTruncate(
                indices, maxLength, vocabulary.getPadIndex(), false);
        
        // 6. 转换为数组
        return processed.stream().mapToInt(Integer::intValue).toArray();
    }
    
    /**
     * 批量预处理文本
     * @param texts 文本列表
     * @param maxLength 序列最大长度
     * @return 处理后的数值序列数组
     */
    public int[][] preprocessBatch(List<String> texts, int maxLength) {
        return texts.stream()
                .map(text -> preprocessText(text, maxLength))
                .toArray(int[][]::new);
    }
}
```

## 9.1.6 实际应用示例

让我们通过一个完整的情感分析数据预处理示例来演示如何使用我们的文本预处理流水线：

```java
/**
 * 情感分析数据预处理示例
 */
public class SentimentAnalysisPreprocessing {
    public static void main(String[] args) {
        // 1. 创建词典（实际应用中可能从文件加载）
        Set<String> dictionary = new HashSet<>(Arrays.asList(
            "这个", "电影", "非常", "好看", "不错", "喜欢", "推荐",
            "很", "不", "差", "糟糕", "讨厌", "浪费", "时间",
            "自然语言", "处理", "技术", "重要", "人工智能"
        ));
        
        // 2. 初始化分词器和词汇表
        ChineseTokenizer tokenizer = new ChineseTokenizer(dictionary);
        Vocabulary vocabulary = new Vocabulary();
        
        // 3. 构建词汇表
        List<String> sampleTexts = Arrays.asList(
            "这部电影非常好看，强烈推荐！",
            "很不错的电影，值得一看。",
            "这部电影太糟糕了，浪费时间。",
            "不推荐这部电影，很失望。"
        );
        
        // 从样本文本中提取词汇构建词汇表
        for (String text : sampleTexts) {
            String cleaned = TextCleaner.cleanText(text);
            List<String> tokens = tokenizer.forwardMaxMatch(cleaned);
            vocabulary.addTokens(tokens);
        }
        
        // 4. 创建预处理器
        TextPreprocessor preprocessor = new TextPreprocessor(vocabulary, tokenizer);
        
        // 5. 预处理文本
        String testText = "这部电影很不错，推荐大家观看。";
        int[] processed = preprocessor.preprocessText(testText, 20);
        
        System.out.println("原始文本: " + testText);
        System.out.println("处理后序列: " + Arrays.toString(processed));
        
        // 显示序列对应的词汇
        System.out.print("序列含义: ");
        for (int idx : processed) {
            if (idx != vocabulary.getPadIndex()) {
                System.out.print(vocabulary.getWord(idx) + " ");
            }
        }
        System.out.println();
        
        // 6. 批量预处理示例
        List<String> batchTexts = Arrays.asList(
            "这部电影很好看",
            "很糟糕的电影",
            "推荐观看"
        );
        
        int[][] batchProcessed = preprocessor.preprocessBatch(batchTexts, 15);
        System.out.println("\n批量处理结果:");
        for (int i = 0; i < batchProcessed.length; i++) {
            System.out.println("文本 " + (i+1) + ": " + Arrays.toString(batchProcessed[i]));
        }
    }
}
```

## 本节小结

在本节中，我们详细介绍了文本预处理的核心技术：

1. **文本清洗和标准化**：移除噪声数据，统一文本格式
2. **分词算法**：实现了基于规则的分词和中文分词算法
3. **词汇表管理**：构建词汇与索引的映射关系
4. **序列处理**：对变长序列进行填充或截断
5. **完整流水线**：整合所有步骤形成完整的预处理流程

通过TinyAI框架的实现，我们不仅掌握了理论知识，还获得了实际的编程经验。这些技术是NLP任务的基础，将在后续章节中发挥重要作用。

## 下一步计划

在下一节中，我们将学习词嵌入技术，了解如何将离散的词汇转换为连续的向量表示，这是现代NLP模型的核心技术之一。