# 5.4 归一化层：Batch Norm与Layer Norm

归一化技术是现代深度学习中最重要的突破之一，它通过标准化中间特征分布解决了深度网络训练中的内部协变量偏移问题。本节将深入探讨两种主要的归一化方法：Batch Normalization和Layer Normalization，并提供完整的Java实现。

## 5.4.1 归一化的数学原理

归一化的核心思想是将输入特征变换为均值为0、方差为1的标准正态分布，然后通过可学习的缩放和偏移参数恢复表示能力：

$$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}$$

$$y = \gamma \hat{x} + \beta$$

其中：
- $\mu$ 和 $\sigma^2$ 分别是均值和方差
- $\epsilon$ 是防止除零的小常数
- $\gamma$ 和 $\beta$ 是可学习的缩放和偏移参数

### 归一化的关键优势

1. **加速收敛**：减少内部协变量偏移，使训练更稳定
2. **降低对初始化的敏感性**：网络对参数初始化更加鲁棒
3. **正则化效果**：减少过拟合，提高泛化能力
4. **允许更高学习率**：加速训练过程

## 5.4.2 Batch Normalization实现

Batch Normalization在批次维度上计算统计信息：

```java
package com.tinyai.layers;

import com.tinyai.core.Variable;
import com.tinyai.initializers.OneInitializer;
import com.tinyai.initializers.ZeroInitializer;

/**
 * Batch Normalization层实现
 * 在批次维度上标准化特征，适用于CNN和DNN
 */
public class BatchNorm extends Layer {
    private final int numFeatures;
    private final double eps;
    private final double momentum;
    private final boolean affine;
    private final boolean trackRunningStats;
    
    // 可学习参数
    private Variable weight;  // gamma缩放参数
    private Variable bias;    // beta偏移参数
    
    // 运行时统计信息
    private Variable runningMean;
    private Variable runningVar;
    private long numBatchesTracked;
    
    // 训练模式标志
    private boolean training = true;

    /**
     * 构造函数
     * @param numFeatures 特征数量（通道数）
     * @param eps 数值稳定性常数
     * @param momentum 滑动平均的动量
     * @param affine 是否使用可学习的仿射变换
     * @param trackRunningStats 是否跟踪运行时统计信息
     */
    public BatchNorm(int numFeatures, double eps, double momentum, 
                     boolean affine, boolean trackRunningStats) {
        super("BatchNorm");
        this.numFeatures = numFeatures;
        this.eps = eps;
        this.momentum = momentum;
        this.affine = affine;
        this.trackRunningStats = trackRunningStats;
        this.numBatchesTracked = 0;
    }

    /**
     * 便利构造函数，使用默认参数
     */
    public BatchNorm(int numFeatures) {
        this(numFeatures, 1e-5, 0.1, true, true);
    }

    @Override
    public void initializeParameters(int[] inputShape) {
        // 初始化可学习参数
        if (affine) {
            // gamma初始化为1
            this.weight = new Variable(
                new OneInitializer().initialize(new int[]{numFeatures}), true);
            // beta初始化为0
            this.bias = new Variable(
                new ZeroInitializer().initialize(new int[]{numFeatures}), true);
            
            addParameter("weight", weight);
            addParameter("bias", bias);
        }
        
        // 初始化运行时统计信息
        if (trackRunningStats) {
            this.runningMean = new Variable(
                new ZeroInitializer().initialize(new int[]{numFeatures}), false);
            this.runningVar = new Variable(
                new OneInitializer().initialize(new int[]{numFeatures}), false);
            
            addBuffer("running_mean", runningMean);
            addBuffer("running_var", runningVar);
        }
    }

    @Override
    public Variable forward(Variable input) {
        validateInput(input);
        
        if (training) {
            return forwardTraining(input);
        } else {
            return forwardInference(input);
        }
    }

    /**
     * 训练模式前向传播
     */
    private Variable forwardTraining(Variable input) {
        // 计算批次统计信息
        BatchStats stats = computeBatchStatistics(input);
        
        // 更新运行时统计信息
        if (trackRunningStats) {
            updateRunningStatistics(stats);
        }
        
        // 执行归一化
        Variable normalized = normalize(input, stats.mean, stats.var);
        
        // 应用仿射变换
        if (affine) {
            normalized = applyAffineTransform(normalized);
        }
        
        return normalized;
    }

    /**
     * 推理模式前向传播
     */
    private Variable forwardInference(Variable input) {
        if (!trackRunningStats) {
            throw new IllegalStateException("Running statistics not tracked for inference mode");
        }
        
        // 使用运行时统计信息进行归一化
        Variable normalized = normalize(input, runningMean, runningVar);
        
        // 应用仿射变换
        if (affine) {
            normalized = applyAffineTransform(normalized);
        }
        
        return normalized;
    }

    /**
     * 计算批次统计信息
     */
    private BatchStats computeBatchStatistics(Variable input) {
        int[] shape = input.getShape();
        int batchSize = shape[0];
        int channels = shape[1];
        
        // 计算空间维度大小
        int spatialSize = 1;
        for (int i = 2; i < shape.length; i++) {
            spatialSize *= shape[i];
        }
        
        double[] batchMean = new double[channels];
        double[] batchVar = new double[channels];
        
        if (shape.length == 4) { // 4D输入 (N, C, H, W)
            computeStats4D(input.getData4D(), batchMean, batchVar, batchSize, spatialSize);
        } else if (shape.length == 2) { // 2D输入 (N, C)
            computeStats2D(input.getData2D(), batchMean, batchVar, batchSize);
        } else {
            throw new UnsupportedOperationException("Unsupported input dimension: " + shape.length);
        }
        
        return new BatchStats(
            new Variable(reshapeForBroadcast(batchMean, shape)), 
            new Variable(reshapeForBroadcast(batchVar, shape))
        );
    }

    /**
     * 计算4D输入的统计信息
     */
    private void computeStats4D(double[][][][] data, double[] mean, double[] var, 
                               int batchSize, int spatialSize) {
        int channels = data[0].length;
        int height = data[0][0].length;
        int width = data[0][0][0].length;
        
        // 计算均值
        for (int c = 0; c < channels; c++) {
            double sum = 0.0;
            for (int n = 0; n < batchSize; n++) {
                for (int h = 0; h < height; h++) {
                    for (int w = 0; w < width; w++) {
                        sum += data[n][c][h][w];
                    }
                }
            }
            mean[c] = sum / (batchSize * spatialSize);
        }
        
        // 计算方差
        for (int c = 0; c < channels; c++) {
            double sumSquaredDiff = 0.0;
            for (int n = 0; n < batchSize; n++) {
                for (int h = 0; h < height; h++) {
                    for (int w = 0; w < width; w++) {
                        double diff = data[n][c][h][w] - mean[c];
                        sumSquaredDiff += diff * diff;
                    }
                }
            }
            var[c] = sumSquaredDiff / (batchSize * spatialSize);
        }
    }

    /**
     * 计算2D输入的统计信息
     */
    private void computeStats2D(double[][] data, double[] mean, double[] var, int batchSize) {
        int features = data[0].length;
        
        // 计算均值
        for (int f = 0; f < features; f++) {
            double sum = 0.0;
            for (int n = 0; n < batchSize; n++) {
                sum += data[n][f];
            }
            mean[f] = sum / batchSize;
        }
        
        // 计算方差
        for (int f = 0; f < features; f++) {
            double sumSquaredDiff = 0.0;
            for (int n = 0; n < batchSize; n++) {
                double diff = data[n][f] - mean[f];
                sumSquaredDiff += diff * diff;
            }
            var[f] = sumSquaredDiff / batchSize;
        }
    }

    /**
     * 执行归一化
     */
    private Variable normalize(Variable input, Variable mean, Variable var) {
        // (input - mean) / sqrt(var + eps)
        Variable centered = input.subtract(mean);
        Variable denominator = var.add(eps).sqrt();
        return centered.divide(denominator);
    }

    /**
     * 应用仿射变换
     */
    private Variable applyAffineTransform(Variable normalized) {
        // gamma * normalized + beta
        Variable scaled = normalized.multiply(weight);
        return scaled.add(bias);
    }

    /**
     * 更新运行时统计信息
     */
    private void updateRunningStatistics(BatchStats batchStats) {
        double effectiveMomentum = momentum;
        
        // 在训练初期使用更精确的估计
        if (numBatchesTracked < 100) {
            effectiveMomentum = 1.0 / (numBatchesTracked + 1);
        }
        
        // 滑动平均更新
        updateRunningMean(batchStats.mean, effectiveMomentum);
        updateRunningVar(batchStats.var, effectiveMomentum);
        
        numBatchesTracked++;
    }

    /**
     * 重塑数组用于广播
     */
    private double[] reshapeForBroadcast(double[] stats, int[] inputShape) {
        if (inputShape.length == 2) {
            return stats;
        } else if (inputShape.length == 4) {
            // 对于4D输入，需要扩展为 [1, C, 1, 1] 形状用于广播
            return stats; // 简化实现，实际需要处理广播
        }
        return stats;
    }

    @Override
    protected void validateInput(Variable input) {
        int[] shape = input.getShape();
        if (shape.length != 2 && shape.length != 4) {
            throw new IllegalArgumentException(
                "BatchNorm only supports 2D or 4D inputs");
        }
        
        int channelDim = shape[1];
        if (channelDim != numFeatures) {
            throw new IllegalArgumentException(
                String.format("Input channel dimension %d doesn't match numFeatures %d", 
                             channelDim, numFeatures));
        }
    }

    /**
     * 设置训练模式
     */
    public void setTraining(boolean training) {
        this.training = training;
    }

    /**
     * 批次统计信息数据类
     */
    private static class BatchStats {
        final Variable mean;
        final Variable var;
        
        BatchStats(Variable mean, Variable var) {
            this.mean = mean;
            this.var = var;
        }
    }
}
```

## 5.4.3 Layer Normalization实现

Layer Normalization在特征维度上计算统计信息，特别适用于RNN和Transformer：

```java
/**
 * Layer Normalization层实现
 * 在特征维度上标准化，适用于RNN和Transformer
 */
public class LayerNorm extends Layer {
    private final int[] normalizedShape;
    private final double eps;
    private final boolean elementwiseAffine;
    
    private Variable weight;
    private Variable bias;
    private final int normalizedDim;

    public LayerNorm(int[] normalizedShape, double eps, boolean elementwiseAffine) {
        super("LayerNorm");
        this.normalizedShape = normalizedShape.clone();
        this.eps = eps;
        this.elementwiseAffine = elementwiseAffine;
        
        // 计算归一化维度的总大小
        this.normalizedDim = computeNormalizedDim();
    }

    public LayerNorm(int[] normalizedShape) {
        this(normalizedShape, 1e-5, true);
    }

    @Override
    public void initializeParameters(int[] inputShape) {
        if (elementwiseAffine) {
            this.weight = new Variable(
                new OneInitializer().initialize(normalizedShape), true);
            this.bias = new Variable(
                new ZeroInitializer().initialize(normalizedShape), true);
            
            addParameter("weight", weight);
            addParameter("bias", bias);
        }
    }

    @Override
    public Variable forward(Variable input) {
        validateInput(input);
        
        // 计算需要归一化的维度
        int[] shape = input.getShape();
        int[] meanVarShape = computeMeanVarShape(shape);
        
        // 计算均值和方差
        Variable mean = computeLayerMean(input, meanVarShape);
        Variable var = computeLayerVariance(input, mean, meanVarShape);
        
        // 执行归一化
        Variable normalized = layerNormalize(input, mean, var);
        
        // 应用仿射变换
        if (elementwiseAffine) {
            normalized = applyElementwiseAffine(normalized);
        }
        
        return normalized;
    }

    /**
     * 计算层级均值
     */
    private Variable computeLayerMean(Variable input, int[] meanVarShape) {
        // 沿着最后几个维度计算均值
        return input.mean(getLastNDims(input.getShape().length, normalizedShape.length));
    }

    /**
     * 计算层级方差
     */
    private Variable computeLayerVariance(Variable input, Variable mean, int[] meanVarShape) {
        Variable centered = input.subtract(mean);
        Variable squaredDiff = centered.square();
        return squaredDiff.mean(getLastNDims(input.getShape().length, normalizedShape.length));
    }

    /**
     * 执行层归一化
     */
    private Variable layerNormalize(Variable input, Variable mean, Variable var) {
        Variable centered = input.subtract(mean);
        Variable std = var.add(eps).sqrt();
        return centered.divide(std);
    }

    /**
     * 应用逐元素仿射变换
     */
    private Variable applyElementwiseAffine(Variable normalized) {
        Variable scaled = normalized.multiply(weight);
        return scaled.add(bias);
    }

    private int computeNormalizedDim() {
        int dim = 1;
        for (int size : normalizedShape) {
            dim *= size;
        }
        return dim;
    }

    private int[] getLastNDims(int totalDims, int lastN) {
        int[] dims = new int[lastN];
        for (int i = 0; i < lastN; i++) {
            dims[i] = totalDims - lastN + i;
        }
        return dims;
    }
}
```

## 5.4.4 Group Normalization实现

Group Normalization将通道分组进行归一化，在小batch size时表现更好：

```java
/**
 * Group Normalization层实现
 */
public class GroupNorm extends Layer {
    private final int numGroups;
    private final int numChannels;
    private final double eps;
    private final boolean affine;
    
    private Variable weight;
    private Variable bias;

    public GroupNorm(int numGroups, int numChannels, double eps, boolean affine) {
        super("GroupNorm");
        this.numGroups = numGroups;
        this.numChannels = numChannels;
        this.eps = eps;
        this.affine = affine;
        
        if (numChannels % numGroups != 0) {
            throw new IllegalArgumentException(
                "Number of channels must be divisible by number of groups");
        }
    }

    @Override
    public Variable forward(Variable input) {
        int[] shape = input.getShape();
        int batchSize = shape[0];
        int channels = shape[1];
        int spatialSize = computeSpatialSize(shape);
        
        int channelsPerGroup = channels / numGroups;
        
        // 重塑输入为 [N, numGroups, channelsPerGroup, spatial...]
        int[] groupShape = {batchSize, numGroups, channelsPerGroup, spatialSize};
        Variable groupedInput = input.reshape(groupShape);
        
        // 在每个组内计算统计信息
        Variable mean = computeGroupMean(groupedInput);
        Variable var = computeGroupVariance(groupedInput, mean);
        
        // 归一化
        Variable normalized = groupNormalize(groupedInput, mean, var);
        
        // 恢复原始形状
        normalized = normalized.reshape(shape);
        
        // 应用仿射变换
        if (affine) {
            normalized = applyGroupAffine(normalized);
        }
        
        return normalized;
    }

    private Variable computeGroupMean(Variable groupedInput) {
        // 在 channelsPerGroup 和 spatial 维度上计算均值
        return groupedInput.mean(new int[]{2, 3});
    }

    private Variable computeGroupVariance(Variable groupedInput, Variable mean) {
        Variable centered = groupedInput.subtract(mean.unsqueeze(2).unsqueeze(3));
        Variable squared = centered.square();
        return squared.mean(new int[]{2, 3});
    }

    private Variable groupNormalize(Variable input, Variable mean, Variable var) {
        Variable expandedMean = mean.unsqueeze(2).unsqueeze(3);
        Variable expandedStd = var.add(eps).sqrt().unsqueeze(2).unsqueeze(3);
        
        return input.subtract(expandedMean).divide(expandedStd);
    }
}
```

## 5.4.5 实例归一化（Instance Normalization）

Instance Normalization特别适用于风格迁移等任务：

```java
/**
 * Instance Normalization层实现
 */
public class InstanceNorm extends Layer {
    private final int numFeatures;
    private final double eps;
    private final boolean affine;
    private final boolean trackRunningStats;
    
    private Variable weight;
    private Variable bias;

    public InstanceNorm(int numFeatures, double eps, boolean affine, boolean trackRunningStats) {
        super("InstanceNorm");
        this.numFeatures = numFeatures;
        this.eps = eps;
        this.affine = affine;
        this.trackRunningStats = trackRunningStats;
    }

    @Override
    public Variable forward(Variable input) {
        // Instance Norm在每个样本的每个通道上独立计算统计信息
        int[] shape = input.getShape();
        int batchSize = shape[0];
        int channels = shape[1];
        
        Variable normalized = performInstanceNormalization(input);
        
        if (affine) {
            normalized = applyInstanceAffine(normalized);
        }
        
        return normalized;
    }

    private Variable performInstanceNormalization(Variable input) {
        // 对每个实例的每个通道独立归一化
        int[] shape = input.getShape();
        
        // 计算每个实例每个通道的均值和方差
        Variable mean = computeInstanceMean(input);
        Variable var = computeInstanceVariance(input, mean);
        
        // 归一化
        Variable centered = input.subtract(mean);
        Variable std = var.add(eps).sqrt();
        
        return centered.divide(std);
    }
}
```

## 5.4.6 归一化技术的对比分析

### 适用场景分析

| 归一化方法 | 适用场景 | 优势 | 劣势 |
|------------|----------|------|------|
| Batch Norm | CNN、大batch | 训练稳定，收敛快 | 依赖batch size |
| Layer Norm | RNN、Transformer | 不依赖batch | 计算量较大 |
| Group Norm | 小batch场景 | batch独立 | 需要调节组数 |
| Instance Norm | 风格迁移 | 风格无关 | 信息损失 |

### 性能基准测试

```java
/**
 * 归一化层性能对比测试
 */
public class NormalizationBenchmark {
    
    @Test
    public void compareNormalizationPerformance() {
        int batchSize = 32;
        int channels = 64;
        int height = 28;
        int width = 28;
        
        Variable input = createRandomInput(batchSize, channels, height, width);
        
        // 测试不同归一化方法
        benchmarkNormalization("BatchNorm", new BatchNorm(channels), input);
        benchmarkNormalization("LayerNorm", new LayerNorm(new int[]{channels, height, width}), input);
        benchmarkNormalization("GroupNorm", new GroupNorm(8, channels), input);
        benchmarkNormalization("InstanceNorm", new InstanceNorm(channels), input);
    }
    
    private void benchmarkNormalization(String name, Layer norm, Variable input) {
        norm.initializeParameters(input.getShape());
        
        // 预热
        for (int i = 0; i < 10; i++) {
            norm.forward(input);
        }
        
        // 性能测试
        long startTime = System.nanoTime();
        for (int i = 0; i < 1000; i++) {
            Variable output = norm.forward(input);
        }
        long endTime = System.nanoTime();
        
        double avgTime = (endTime - startTime) / 1000.0 / 1_000_000;
        System.out.printf("%s average time: %.3f ms%n", name, avgTime);
    }
}
```

## 5.4.7 自适应归一化技术

### 条件归一化（Conditional Normalization）

```java
/**
 * 条件批次归一化
 * 根据条件信息调整归一化参数
 */
public class ConditionalBatchNorm extends BatchNorm {
    private final int conditionDim;
    private Dense gammaProjection;
    private Dense betaProjection;

    public ConditionalBatchNorm(int numFeatures, int conditionDim) {
        super(numFeatures);
        this.conditionDim = conditionDim;
    }

    @Override
    public void initializeParameters(int[] inputShape) {
        super.initializeParameters(inputShape);
        
        // 条件投影层
        this.gammaProjection = new Dense(conditionDim, numFeatures);
        this.betaProjection = new Dense(conditionDim, numFeatures);
        
        gammaProjection.initializeParameters(new int[]{1, conditionDim});
        betaProjection.initializeParameters(new int[]{1, conditionDim});
    }

    public Variable forward(Variable input, Variable condition) {
        // 标准批次归一化
        Variable normalized = super.forward(input);
        
        // 基于条件生成自适应参数
        Variable adaptiveGamma = gammaProjection.forward(condition);
        Variable adaptiveBeta = betaProjection.forward(condition);
        
        // 应用条件仿射变换
        return normalized.multiply(adaptiveGamma).add(adaptiveBeta);
    }
}
```

### 自注意力归一化

```java
/**
 * 自注意力归一化
 * 结合注意力机制的归一化方法
 */
public class SelfAttentionNorm extends Layer {
    private final int features;
    private final int headDim;
    
    private Dense queryProjection;
    private Dense keyProjection;
    private Dense valueProjection;
    private LayerNorm norm;

    @Override
    public Variable forward(Variable input) {
        // 先进行层归一化
        Variable normalized = norm.forward(input);
        
        // 计算自注意力权重
        Variable query = queryProjection.forward(normalized);
        Variable key = keyProjection.forward(normalized);
        Variable value = valueProjection.forward(normalized);
        
        Variable attention = computeAttention(query, key, value);
        
        // 结合注意力权重进行归一化
        return applyAttentionNormalization(normalized, attention);
    }
}
```

## 5.4.8 归一化在不同架构中的应用

### 在ResNet中的应用

```java
/**
 * ResNet基本块中的归一化应用
 */
public class ResNetBlock extends Layer {
    private Conv2D conv1;
    private BatchNorm bn1;
    private ReLU relu1;
    private Conv2D conv2;
    private BatchNorm bn2;
    private ReLU relu2;

    @Override
    public Variable forward(Variable input) {
        Variable residual = input;
        
        // 第一个卷积块
        Variable out = conv1.forward(input);
        out = bn1.forward(out);
        out = relu1.forward(out);
        
        // 第二个卷积块
        out = conv2.forward(out);
        out = bn2.forward(out);
        
        // 残差连接
        out = out.add(residual);
        out = relu2.forward(out);
        
        return out;
    }
}
```

### 在Transformer中的应用

```java
/**
 * Transformer编码器层中的归一化
 */
public class TransformerEncoderLayer extends Layer {
    private MultiHeadAttention attention;
    private LayerNorm norm1;
    private Dense feedForward;
    private LayerNorm norm2;

    @Override
    public Variable forward(Variable input) {
        // 自注意力 + 残差 + 层归一化
        Variable attnOut = attention.forward(input);
        Variable residual1 = input.add(attnOut);
        Variable norm1Out = norm1.forward(residual1);
        
        // 前馈网络 + 残差 + 层归一化
        Variable ffOut = feedForward.forward(norm1Out);
        Variable residual2 = norm1Out.add(ffOut);
        Variable output = norm2.forward(residual2);
        
        return output;
    }
}
```

归一化技术作为现代深度学习的核心组件，极大地改善了训练稳定性和模型性能。通过本节的深入分析和完整实现，我们构建了多种归一化方法的完整体系，为构建高性能的深度学习模型奠定了坚实基础。

下一节我们将探讨Block组合模式，学习如何将这些基础构建块组合成复杂的网络架构。