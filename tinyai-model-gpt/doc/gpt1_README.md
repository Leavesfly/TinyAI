# GPT-1 æ¨¡å‹å®ç°

åŸºäºTinyAIæ¡†æ¶å®ç°çš„GPT-1è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨åŸå§‹Transformerè§£ç å™¨æ¶æ„ï¼Œå¿ å®è¿˜åŸäº†å¼€åˆ›æ€§çš„"Improving Language Understanding by Generative Pre-Training"è®ºæ–‡ä¸­çš„æ¨¡å‹è®¾è®¡ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
tinyai-model-gpt/src/main/java/io/leavesfly/tinyai/gpt1/
â”œâ”€â”€ GPT1Config.java              # GPT-1é…ç½®ç±»
â”œâ”€â”€ GPT1Model.java               # GPT-1æ¨¡å‹ç±»ï¼ˆç»§æ‰¿Modelï¼‰
â”œâ”€â”€ GPT1Block.java               # GPT-1æ ¸å¿ƒå—ï¼ˆç»§æ‰¿Blockï¼‰
â”œâ”€â”€ GPT1TransformerBlock.java    # Transformerè§£ç å™¨å—
â”œâ”€â”€ GPT1TokenEmbedding.java      # Tokenå’Œä½ç½®åµŒå…¥å±‚
â”œâ”€â”€ GPT1OutputHead.java          # è¯­è¨€æ¨¡å‹è¾“å‡ºå¤´
â””â”€â”€ GptDemo.java                 # ç»¼åˆæ¼”ç¤ºç¨‹åº
```

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### 1. åŸå§‹Transformeræ¶æ„
- **è§£ç å™¨-onlyç»“æ„**: ä¸“æ³¨äºè‡ªå›å½’è¯­è¨€å»ºæ¨¡ä»»åŠ¡
- **Post-LayerNormæ¶æ„**: éµå¾ªåŸå§‹Transformerè®ºæ–‡çš„å±‚å½’ä¸€åŒ–ä½ç½®
- **å­¦ä¹ ä½ç½®åµŒå…¥**: ä½¿ç”¨å¯å­¦ä¹ çš„ç»å¯¹ä½ç½®ç¼–ç 
- **æƒé‡å…±äº«**: æ”¯æŒè¾“å…¥åµŒå…¥å’Œè¾“å‡ºæŠ•å½±æƒé‡å…±äº«

### 2. å¤šè§„æ¨¡é…ç½®æ”¯æŒ
- **å°å‹æ¨¡å‹**: 256ç»´, 6å±‚, 8å¤´ (é€‚ç”¨äºæµ‹è¯•å’Œå­¦ä¹ )
- **ä¸­å‹æ¨¡å‹**: 512ç»´, 8å±‚, 8å¤´ (å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡)
- **æ ‡å‡†æ¨¡å‹**: 768ç»´, 12å±‚, 12å¤´ (åŸè®ºæ–‡é…ç½®)

### 3. å†å²æ„ä¹‰ç‰¹æ€§
- **é¦–ä¸ªGPTæ¶æ„**: Transformerè§£ç å™¨åœ¨è¯­è¨€å»ºæ¨¡çš„å¼€åˆ›æ€§åº”ç”¨
- **æ— ç›‘ç£é¢„è®­ç»ƒ**: å¥ å®šäº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€
- **è¿ç§»å­¦ä¹ èƒ½åŠ›**: ä¸ºä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæä¾›å¼ºå¤§åŸºç¡€

## ğŸ—ï¸ ç½‘ç»œæ¶æ„å›¾

### æ•´ä½“æ¶æ„
```mermaid
graph TB
    Input["Token IDs<br/>(batch_size, seq_len)"] --> TokenEmbed["TokenåµŒå…¥å±‚<br/>GPT1TokenEmbedding"]
    TokenEmbed --> TransBlock1["Transformerå— 1<br/>GPT1TransformerBlock"]
    TransBlock1 --> TransBlock2["Transformerå— 2<br/>GPT1TransformerBlock"]
    TransBlock2 --> TransBlockN["...<br/>Transformerå— N"]
    TransBlockN --> FinalLN["æœ€ç»ˆå±‚å½’ä¸€åŒ–<br/>LayerNorm"]
    FinalLN --> OutputHead["è¾“å‡ºå¤´<br/>GPT1OutputHead"]
    OutputHead --> Output["Logits<br/>(batch_size, seq_len, vocab_size)"]
    
    style TokenEmbed fill:#e1f5fe
    style TransBlock1 fill:#f3e5f5
    style TransBlock2 fill:#f3e5f5
    style TransBlockN fill:#f3e5f5
    style FinalLN fill:#fff3e0
    style OutputHead fill:#e8f5e8
```

### GPT1TransformerBlockå†…éƒ¨ç»“æ„ (Post-LayerNorm)
```mermaid
graph TD
    BlockInput["è¾“å…¥<br/>(batch_size, seq_len, hidden_size)"] --> MHA["å¤šå¤´è‡ªæ³¨æ„åŠ›<br/>å¸¦å› æœæ©ç "]
    MHA --> Res1["æ®‹å·®è¿æ¥ 1"]
    BlockInput --> Res1
    Res1 --> LN1["LayerNorm 1"]
    LN1 --> FFN["å‰é¦ˆç½‘ç»œ<br/>Linearâ†’GELUâ†’Linear"]
    FFN --> Res2["æ®‹å·®è¿æ¥ 2"]
    LN1 --> Res2
    Res2 --> LN2["LayerNorm 2"]
    LN2 --> BlockOutput["è¾“å‡º<br/>(batch_size, seq_len, hidden_size)"]
    
    style MHA fill:#e1f5fe
    style FFN fill:#f3e5f5
    style Res1 fill:#fff3e0
    style Res2 fill:#fff3e0
    style LN1 fill:#fce4ec
    style LN2 fill:#fce4ec
```

### TokenåµŒå…¥å±‚ç»“æ„
```mermaid
graph LR
    TokenIDs["Token IDs"] --> TokenLookup["TokenåµŒå…¥æŸ¥æ‰¾<br/>(vocab_size, hidden_size)"]
    Position["ä½ç½®ç´¢å¼•"] --> PosLookup["ä½ç½®åµŒå…¥æŸ¥æ‰¾<br/>(max_seq_len, hidden_size)"]
    TokenLookup --> Add["åµŒå…¥ç›¸åŠ "]
    PosLookup --> Add
    Add --> Dropout["Embedding Dropout"]
    Dropout --> EmbedOutput["åµŒå…¥è¾“å‡º<br/>(batch_size, seq_len, hidden_size)"]
    
    style TokenLookup fill:#e1f5fe
    style PosLookup fill:#f3e5f5
    style Add fill:#fff3e0
    style Dropout fill:#fce4ec
```

### GPT-1 vs GPT-2 æ¶æ„å¯¹æ¯”
```mermaid
graph TB
    subgraph GPT1 ["GPT-1 (Post-LayerNorm)"]
        G1Input["è¾“å…¥"] --> G1Attn["Multi-Head Attention"]
        G1Attn --> G1Res1["+ æ®‹å·®è¿æ¥"]
        G1Input --> G1Res1
        G1Res1 --> G1LN1["LayerNorm"]
        G1LN1 --> G1FFN["Feed Forward"]
        G1FFN --> G1Res2["+ æ®‹å·®è¿æ¥"]
        G1LN1 --> G1Res2
        G1Res2 --> G1LN2["LayerNorm"]
    end
    
    subgraph GPT2 ["GPT-2 (Pre-LayerNorm)"]
        G2Input["è¾“å…¥"] --> G2LN1["LayerNorm"]
        G2LN1 --> G2Attn["Multi-Head Attention"]
        G2Attn --> G2Res1["+ æ®‹å·®è¿æ¥"]
        G2Input --> G2Res1
        G2Res1 --> G2LN2["LayerNorm"]
        G2LN2 --> G2FFN["Feed Forward"]
        G2FFN --> G2Res2["+ æ®‹å·®è¿æ¥"]
        G2Res1 --> G2Res2
    end
    
    style G1LN1 fill:#ffcdd2
    style G1LN2 fill:#ffcdd2
    style G2LN1 fill:#c8e6c9
    style G2LN2 fill:#c8e6c9
```

### ç±»å›¾å…³ç³»
```mermaid
classDiagram
    class GPT1Model {
        -GPT1Config config
        -GPT1Block gpt1Block
        +GPT1Model(String, GPT1Config)
        +createTinyModel(String) GPT1Model
        +createMediumModel(String) GPT1Model
        +createFullModel(String, int) GPT1Model
        +predict(Variable) Variable
        +predictNextToken(Variable) Variable
        +generateText(List~Integer~, int, double) List~Integer~
        +getModelCapacity() String
        +printModelInfo() void
    }
    
    class GPT1Block {
        -GPT1Config config
        -GPT1TokenEmbedding tokenEmbedding
        -List~GPT1TransformerBlock~ transformerBlocks
        -LayerNorm finalLayerNorm
        -GPT1OutputHead outputHead
        +layerForward(Variable...) Variable
        +predictNextToken(Variable) Variable
        +generateSequence(List~Integer~, int, double) List~Integer~
    }
    
    class GPT1TransformerBlock {
        -MultiHeadAttention attention
        -LayerNorm layerNorm1
        -FeedForward feedForward
        -LayerNorm layerNorm2
        +layerForward(Variable...) Variable
        +addResidualConnection(Variable, Variable) Variable
    }
    
    class GPT1TokenEmbedding {
        -Parameter tokenEmbedding
        -Parameter positionEmbedding
        +layerForward(Variable...) Variable
        +getTokenEmbeddings(NdArray, int, int) Variable
        +getPositionEmbeddings(int, int) Variable
    }
    
    class GPT1OutputHead {
        -Parameter outputProjection
        -Parameter outputBias
        -boolean shareEmbeddingWeights
        +layerForward(Variable...) Variable
        +computeLinearProjection(Variable, int, int) Variable
        +setSharedEmbeddingWeights(Parameter) void
    }
    
    GPT1Model --> GPT1Block : "åŒ…å«"
    GPT1Block --> GPT1TokenEmbedding : "ä½¿ç”¨"
    GPT1Block --> GPT1TransformerBlock : "åŒ…å«å¤šä¸ª"
    GPT1Block --> GPT1OutputHead : "ä½¿ç”¨"
    GPT1TransformerBlock --> MultiHeadAttention : "ä½¿ç”¨"
    GPT1TransformerBlock --> LayerNorm : "ä½¿ç”¨"
    GPT1TransformerBlock --> FeedForward : "ä½¿ç”¨"
    GPT1OutputHead --> GPT1TokenEmbedding : "å¯é€‰æƒé‡å…±äº«"
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```java
// åˆ›å»ºæ ‡å‡†GPT-1æ¨¡å‹
GPT1Model model = new GPT1Model("my-gpt1", new GPT1Config());

// ä½¿ç”¨é¢„è®¾é…ç½®å¿«é€Ÿåˆ›å»º
GPT1Model tinyModel = GPT1Model.createTinyModel("gpt1-tiny");
GPT1Model mediumModel = GPT1Model.createMediumModel("gpt1-medium");
GPT1Model fullModel = GPT1Model.createFullModel("gpt1-full", 50000);

// å‰å‘ä¼ æ’­
Variable tokenIds = new Variable(NdArray.of(new float[][]{{1, 2, 3, 4, 5}}));
Variable output = model.predict(tokenIds);

// é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
Variable nextToken = model.predictNextToken(tokenIds);

// ç”Ÿæˆæ–‡æœ¬åºåˆ—
List<Integer> prompt = Arrays.asList(1, 2, 3);
List<Integer> generated = model.generateText(prompt, 20, 1.0);
```

### è‡ªå®šä¹‰é…ç½®

```java
// åˆ›å»ºè‡ªå®šä¹‰GPT-1é…ç½®
GPT1Config config = new GPT1Config(
    40000,  // vocabSize - è¯æ±‡è¡¨å¤§å°
    512,    // maxSequenceLength - æœ€å¤§åºåˆ—é•¿åº¦
    768,    // hiddenSize - éšè—å±‚ç»´åº¦
    12,     // numLayers - Transformerå±‚æ•°
    12      // numAttentionHeads - æ³¨æ„åŠ›å¤´æ•°
);

// è®¾ç½®è®­ç»ƒç›¸å…³å‚æ•°
config.setResidualDropoutProb(0.1);
config.setEmbeddingDropoutProb(0.1);
config.setAttentionDropoutProb(0.1);
config.setActivationFunction("gelu");

// éªŒè¯é…ç½®
config.validate();

// åˆ›å»ºæ¨¡å‹
GPT1Model model = new GPT1Model("custom-gpt1", config);
```

## ğŸ“Š æ¨¡å‹é…ç½®å¯¹æ¯”

| é…ç½®ç±»å‹ | è¯æ±‡è¡¨ | åºåˆ—é•¿åº¦ | éšè—ç»´åº¦ | å±‚æ•° | æ³¨æ„åŠ›å¤´ | å‰é¦ˆç»´åº¦ | å‚æ•°é‡ä¼°ç®— | é€‚ç”¨åœºæ™¯ |
|----------|--------|----------|----------|------|----------|----------|------------|----------|
| Tiny | 1000 | 128 | 256 | 6 | 8 | 1024 | ~2M | æµ‹è¯•ã€å­¦ä¹  |
| Medium | 5000 | 256 | 512 | 8 | 8 | 2048 | ~25M | ä¸­ç­‰ä»»åŠ¡ |
| Standard | 40000 | 512 | 768 | 12 | 12 | 3072 | ~117M | åŸå§‹è®ºæ–‡ |

## ğŸ§ª è¿è¡Œæ¼”ç¤º

### 1. ç»¼åˆæ¼”ç¤ºç¨‹åº
```java
// è¿è¡Œå®Œæ•´æ¼”ç¤º
GptDemo.main(new String[0]);

// è¿è¡Œç‰¹å®šæ¼”ç¤ºæ¨¡å—
GptDemo.runDemo("quick");        // å¿«é€Ÿå¼€å§‹
GptDemo.runDemo("detailed");     // è¯¦ç»†åŠŸèƒ½
GptDemo.runDemo("architecture"); // æ¶æ„å±•ç¤º
GptDemo.runDemo("performance");  // æ€§èƒ½æµ‹è¯•
```

### 2. æ¨¡å‹ä¿¡æ¯å±•ç¤º
```java
GPT1Model model = GPT1Model.createMediumModel("demo");
model.printModelInfo();

// è¾“å‡ºç¤ºä¾‹:
// === GPT-1 æ¨¡å‹è¯¦ç»†ä¿¡æ¯ ===
// æ¨¡å‹åç§°: demo
// æ¨¡å‹ç±»å‹: GPT-1 (Generative Pre-trained Transformer 1)
// 
// --- æ¶æ„é…ç½® ---
// è¯æ±‡è¡¨å¤§å°: 5000
// æœ€å¤§åºåˆ—é•¿åº¦: 256
// éšè—å±‚ç»´åº¦: 512
// Transformerå±‚æ•°: 8
// æ³¨æ„åŠ›å¤´æ•°: 8
// å‰é¦ˆç½‘ç»œç»´åº¦: 2048
// æ¿€æ´»å‡½æ•°: gelu
```

### 3. æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
```java
GPT1Model model = GPT1Model.createTinyModel("generator");

// åŸºç¡€ç”Ÿæˆ
List<Integer> prompt = Arrays.asList(1, 2, 3);
List<Integer> result = model.generateText(prompt, 10, 1.0);
System.out.println("ç”Ÿæˆç»“æœ: " + result);

// ä¸åŒæ¸©åº¦çš„ç”Ÿæˆå¯¹æ¯”
double[] temperatures = {0.5, 1.0, 1.5};
for (double temp : temperatures) {
    List<Integer> generated = model.generateText(prompt, 10, temp);
    System.out.printf("æ¸©åº¦%.1f: %s\n", temp, generated);
}
```

### 4. æ‰¹é‡å¤„ç†ç¤ºä¾‹
```java
// åˆ›å»ºæ‰¹é‡è¾“å…¥
float[][] batchData = {
    {1, 2, 3, 4},
    {5, 6, 7, 8},
    {9, 10, 11, 12}
};
Variable batchInput = new Variable(NdArray.of(batchData));

// æ‰¹é‡é¢„æµ‹
Variable batchResult = model.batchPredict(batchInput);
System.out.printf("æ‰¹é‡è¾“å…¥å½¢çŠ¶: %s\n", batchInput.getValue().getShape());
System.out.printf("æ‰¹é‡è¾“å‡ºå½¢çŠ¶: %s\n", batchResult.getValue().getShape());
```

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### 1. Post-LayerNormæ¶æ„
GPT-1é‡‡ç”¨Post-LayerNormç»“æ„ï¼Œä¸åŸå§‹Transformerä¿æŒä¸€è‡´ï¼š

```java
// GPT1TransformerBlockçš„å‰å‘ä¼ æ’­
Variable attentionOutput = attention.layerForward(x, x, x);
Variable residual1 = addResidualConnection(x, attentionOutput);
Variable norm1Output = layerNorm1.layerForward(residual1);

Variable ffnOutput = feedForward.layerForward(norm1Output);
Variable residual2 = addResidualConnection(norm1Output, ffnOutput);  
Variable norm2Output = layerNorm2.layerForward(residual2);
```

### 2. å› æœæ©ç æœºåˆ¶
ç¡®ä¿è‡ªå›å½’ç‰¹æ€§ï¼Œé˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²ï¼š

```java
// åœ¨MultiHeadAttentionä¸­ä½¿ç”¨å› æœæ©ç 
MultiHeadAttention attention = new MultiHeadAttention(
    name + "_attention", 
    config.getHiddenSize(), 
    config.getNumAttentionHeads(), 
    true  // å¯ç”¨å› æœæ©ç 
);
```

### 3. æƒé‡å…±äº«æœºåˆ¶
æ”¯æŒè¾“å…¥åµŒå…¥å’Œè¾“å‡ºæŠ•å½±æƒé‡å…±äº«ï¼Œå‡å°‘å‚æ•°é‡ï¼š

```java
// åˆ›å»ºå…±äº«æƒé‡çš„è¾“å‡ºå¤´
GPT1OutputHead outputHead = new GPT1OutputHead(
    name + "_output_head", 
    config, 
    false,  // ä¸ä½¿ç”¨åç½®
    tokenEmbedding.getTokenEmbedding()  // å…±äº«TokenåµŒå…¥æƒé‡
);
```

### 4. ä½ç½®ç¼–ç 
ä½¿ç”¨å­¦ä¹ çš„ç»å¯¹ä½ç½®åµŒå…¥ï¼š

```java
// åˆå§‹åŒ–ä½ç½®åµŒå…¥çŸ©é˜µ
positionEmbedding = new Parameter(
    NdArray.likeRandomN(Shape.of(config.getMaxSequenceLength(), config.getHiddenSize()))
           .mulNum((float) config.getInitializerRange())
);

// åµŒå…¥ç»„åˆ
Variable embeddings = tokenEmbeds.add(positionEmbeds);
```

## ğŸ“ˆ å†å²æ„ä¹‰ä¸ç‰¹ç‚¹

### GPT-1çš„åˆ›æ–°è´¡çŒ®
- âœ… **é¦–æ¬¡è¯æ˜**: Transformerè§£ç å™¨åœ¨è¯­è¨€å»ºæ¨¡ä¸Šçš„å¼ºå¤§èƒ½åŠ›
- âœ… **æ— ç›‘ç£é¢„è®­ç»ƒ**: å¼€åˆ›äº†é¢„è®­ç»ƒ+å¾®è°ƒçš„èŒƒå¼
- âœ… **è¿ç§»å­¦ä¹ **: è¯æ˜äº†è¯­è¨€æ¨¡å‹çš„é€šç”¨è¡¨ç¤ºèƒ½åŠ›
- âœ… **æ¶æ„ç®€æ´**: çº¯è§£ç å™¨ç»“æ„ï¼Œè®¾è®¡ä¼˜é›…

### ä¸åç»­GPTçš„å…³ç³»
- **GPT-1 â†’ GPT-2**: å¢åŠ æ¨¡å‹è§„æ¨¡ï¼Œæ”¹ç”¨Pre-LayerNorm
- **GPT-1 â†’ GPT-3**: å¤§å¹…æ‰©å±•å‚æ•°é‡ï¼Œå¼•å…¥In-Context Learning
- **æ¶æ„å»¶ç»­**: ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„

### é€‚ç”¨åœºæ™¯
- ğŸ¯ **å†å²ç ”ç©¶**: ç†è§£GPTç³»åˆ—æ¨¡å‹çš„æ¼”è¿›
- ğŸ¯ **æ•™å­¦æ¼”ç¤º**: å­¦ä¹ Transformerè§£ç å™¨åŸç†
- ğŸ¯ **åŸºç¡€ä»»åŠ¡**: ç®€å•çš„æ–‡æœ¬ç”Ÿæˆå’Œè¯­è¨€å»ºæ¨¡
- ğŸ¯ **æ¦‚å¿µéªŒè¯**: éªŒè¯æ¶æ„è®¾è®¡æ€æƒ³

## ğŸ” ä»£ç ç¤ºä¾‹

### æ¨¡å‹ç»„ä»¶è®¿é—®
```java
GPT1Model model = GPT1Model.createMediumModel("components");

// è·å–é…ç½®ä¿¡æ¯
GPT1Config config = model.getConfig();
System.out.println("éšè—ç»´åº¦: " + config.getHiddenSize());
System.out.println("æ³¨æ„åŠ›å¤´ç»´åº¦: " + config.getAttentionHeadSize());

// è·å–æ ¸å¿ƒç»„ä»¶
GPT1Block gpt1Block = model.getGPT1Block();
GPT1TokenEmbedding tokenEmbed = gpt1Block.getTokenEmbedding();
List<GPT1TransformerBlock> transformerBlocks = gpt1Block.getTransformerBlocks();
GPT1OutputHead outputHead = gpt1Block.getOutputHead();

// è®¿é—®Transformerå—çš„å†…éƒ¨ç»„ä»¶
GPT1TransformerBlock firstBlock = transformerBlocks.get(0);
MultiHeadAttention attention = firstBlock.getAttention();
FeedForward feedForward = firstBlock.getFeedForward();
```

### è¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†
```java
GPT1Model model = GPT1Model.createTinyModel("validator");

// åºåˆ—é•¿åº¦éªŒè¯
if (model.isValidSequenceLength(256)) {
    System.out.println("âœ… åºåˆ—é•¿åº¦æœ‰æ•ˆ");
} else {
    System.out.println("âŒ åºåˆ—é•¿åº¦è¶…å‡ºé™åˆ¶");
}

// Token IDéªŒè¯
if (model.isValidTokenId(500)) {
    System.out.println("âœ… Token IDæœ‰æ•ˆ");
} else {
    System.out.println("âŒ Token IDè¶…å‡ºè¯æ±‡è¡¨èŒƒå›´");
}

// è¾“å…¥éªŒè¯
try {
    Variable validInput = new Variable(NdArray.of(new float[][]{{1, 2, 3}}));
    model.validateInput(validInput);
    System.out.println("âœ… è¾“å…¥éªŒè¯é€šè¿‡");
} catch (IllegalArgumentException e) {
    System.out.println("âŒ è¾“å…¥éªŒè¯å¤±è´¥: " + e.getMessage());
}
```

### æ€§èƒ½ç›‘æ§
```java
GPT1Model model = GPT1Model.createMediumModel("benchmark");

// æ¨¡å‹å®¹é‡ä¿¡æ¯
String capacity = model.getModelCapacity();
System.out.println("æ¨¡å‹å®¹é‡: " + capacity);

// æ¨ç†æ€§èƒ½æµ‹è¯•
long startTime = System.currentTimeMillis();
Variable input = new Variable(NdArray.of(new float[][]{{1, 2, 3, 4, 5}}));
Variable output = model.predict(input);
long endTime = System.currentTimeMillis();

System.out.printf("æ¨ç†æ—¶é—´: %d ms\n", endTime - startTime);
System.out.printf("è¾“å‡ºå½¢çŠ¶: %s\n", output.getValue().getShape());
```

## ğŸ“ å­¦ä¹ èµ„æº

### æ ¸å¿ƒè®ºæ–‡
- **"Improving Language Understanding by Generative Pre-Training"** (GPT-1åŸè®ºæ–‡)
- **"Attention Is All You Need"** (TransformeråŸè®ºæ–‡)
- **"Deep Residual Learning for Image Recognition"** (æ®‹å·®è¿æ¥)

### æŠ€æœ¯åšå®¢
- GPT-1æ¶æ„æ·±åº¦è§£æ
- Post-LayerNorm vs Pre-LayerNormå¯¹æ¯”
- è‡ªå›å½’è¯­è¨€æ¨¡å‹åŸç†
- Transformerè§£ç å™¨è¯¦è§£

### ç›¸å…³æ¦‚å¿µ
- **è‡ªå›å½’è¯­è¨€å»ºæ¨¡**: æ ¹æ®å‰æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
- **å› æœæ©ç **: é˜²æ­¢è§£ç å™¨çœ‹åˆ°æœªæ¥ä¿¡æ¯
- **æ— ç›‘ç£é¢„è®­ç»ƒ**: åœ¨å¤§é‡æ–‡æœ¬ä¸Šå­¦ä¹ è¯­è¨€è¡¨ç¤º
- **è¿ç§»å­¦ä¹ **: å°†é¢„è®­ç»ƒæ¨¡å‹ç”¨äºä¸‹æ¸¸ä»»åŠ¡

## ğŸ¤ æ‰©å±•å»ºè®®

### å¯èƒ½çš„æ”¹è¿›æ–¹å‘
- [ ] å®ç°çœŸæ­£çš„Dropoutæœºåˆ¶
- [ ] æ·»åŠ æ¸©åº¦é‡‡æ ·ã€Top-kã€Top-pè§£ç ç­–ç•¥
- [ ] æ”¯æŒæŸæœç´¢(Beam Search)è§£ç 
- [ ] å®ç°æ¨¡å‹å¹¶è¡Œå’Œæ¢¯åº¦ç´¯ç§¯
- [ ] æ·»åŠ æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–

### é«˜çº§ç‰¹æ€§
- [ ] æ”¯æŒä¸åŒçš„ä½ç½®ç¼–ç æ–¹æ¡ˆ
- [ ] å®ç°çŸ¥è¯†è’¸é¦åŠŸèƒ½
- [ ] æ·»åŠ æ¨¡å‹å‹ç¼©å’Œé‡åŒ–
- [ ] æ”¯æŒå¢é‡ç”Ÿæˆä¼˜åŒ–

### ä¸å…¶ä»–GPTæ¨¡å‹çš„é›†æˆ
- [ ] æä¾›GPT-1åˆ°GPT-2çš„è¿ç§»å·¥å…·
- [ ] å®ç°æ¶æ„å¯¹æ¯”åˆ†æåŠŸèƒ½
- [ ] æ”¯æŒæ¨¡å‹èƒ½åŠ›åŸºå‡†æµ‹è¯•

---

*åŸºäºTinyAIæ¡†æ¶å®ç°ï¼Œå¿ å®è¿˜åŸGPT-1åŸå§‹æ¶æ„ï¼Œä¸ºç†è§£å’Œå­¦ä¹ ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•å†ç¨‹æä¾›å®è´µèµ„æºã€‚*