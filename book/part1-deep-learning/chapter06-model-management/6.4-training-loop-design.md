# 6.4 训练循环设计：从批次到轮次

训练循环是深度学习框架的核心引擎，它协调数据加载、前向传播、损失计算、反向传播和参数更新等所有训练步骤。本节将深入探讨训练循环的设计原理和完整实现。

## 6.4.1 训练循环的核心组件

训练循环需要管理以下关键组件：
- **数据加载器**：批次数据的生成和预处理
- **损失函数**：模型输出与目标的差异度量
- **评估指标**：模型性能的量化评估
- **回调系统**：训练过程中的事件处理

### 训练器基类设计

```java
package com.tinyai.training;

import com.tinyai.core.Variable;
import com.tinyai.data.DataLoader;
import com.tinyai.losses.Loss;
import com.tinyai.metrics.Metric;
import com.tinyai.models.Model;
import com.tinyai.optimizers.Optimizer;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * 训练器基类
 */
public class Trainer {
    private final Model model;
    private final Optimizer optimizer;
    private final Loss lossFunction;
    
    private final List<Metric> trainMetrics;
    private final List<Metric> validMetrics;
    private final List<TrainingCallback> callbacks;
    
    // 训练状态
    private int currentEpoch = 0;
    private int currentStep = 0;
    private double bestValidLoss = Double.MAX_VALUE;
    private Map<String, Object> history = new HashMap<>();

    public Trainer(Model model, Optimizer optimizer, Loss lossFunction) {
        this.model = model;
        this.optimizer = optimizer;
        this.lossFunction = lossFunction;
        this.trainMetrics = new ArrayList<>();
        this.validMetrics = new ArrayList<>();
        this.callbacks = new ArrayList<>();
        
        // 设置优化器参数
        optimizer.setParameters(model.getParameters());
    }

    /**
     * 添加训练指标
     */
    public Trainer addTrainMetric(Metric metric) {
        trainMetrics.add(metric);
        return this;
    }

    /**
     * 添加验证指标
     */
    public Trainer addValidMetric(Metric metric) {
        validMetrics.add(metric);
        return this;
    }

    /**
     * 添加回调
     */
    public Trainer addCallback(TrainingCallback callback) {
        callbacks.add(callback);
        return this;
    }

    /**
     * 训练模型
     */
    public TrainingHistory fit(DataLoader trainLoader, DataLoader validLoader, 
                              int epochs, boolean verbose) {
        TrainingHistory history = new TrainingHistory();
        
        // 训练开始回调
        callOnTrainBegin();
        
        for (int epoch = 0; epoch < epochs; epoch++) {
            currentEpoch = epoch;
            
            // Epoch开始回调
            callOnEpochBegin(epoch);
            
            // 训练阶段
            EpochResult trainResult = trainEpoch(trainLoader, verbose);
            
            // 验证阶段
            EpochResult validResult = null;
            if (validLoader != null) {
                validResult = validateEpoch(validLoader, verbose);
            }
            
            // 记录训练历史
            history.addEpoch(trainResult, validResult);
            
            // 更新最佳验证损失
            if (validResult != null && validResult.loss < bestValidLoss) {
                bestValidLoss = validResult.loss;
            }
            
            // Epoch结束回调
            callOnEpochEnd(epoch, trainResult, validResult);
            
            if (verbose) {
                printEpochSummary(epoch, trainResult, validResult);
            }
        }
        
        // 训练结束回调
        callOnTrainEnd();
        
        return history;
    }

    /**
     * 训练一个epoch
     */
    private EpochResult trainEpoch(DataLoader dataLoader, boolean verbose) {
        model.train();
        EpochResult result = new EpochResult();
        
        int batchCount = 0;
        double totalLoss = 0.0;
        
        // 重置指标
        resetMetrics(trainMetrics);
        
        for (DataLoader.Batch batch : dataLoader) {
            // Batch开始回调
            callOnBatchBegin(currentStep);
            
            // 前向传播
            Variable predictions = model.forward(batch.inputs);
            Variable loss = lossFunction.forward(predictions, batch.targets);
            
            // 反向传播
            model.zeroGrad();
            loss.backward();
            
            // 参数更新
            optimizer.step();
            
            // 更新统计信息
            double batchLoss = loss.getData();
            totalLoss += batchLoss;
            batchCount++;
            currentStep++;
            
            // 更新指标
            updateMetrics(trainMetrics, predictions, batch.targets);
            
            // Batch结束回调
            callOnBatchEnd(currentStep, batchLoss);
            
            if (verbose && currentStep % 100 == 0) {
                System.out.printf("Step %d, Loss: %.4f\n", currentStep, batchLoss);
            }
        }
        
        result.loss = totalLoss / batchCount;
        result.metrics = computeMetrics(trainMetrics);
        
        return result;
    }

    /**
     * 验证一个epoch
     */
    private EpochResult validateEpoch(DataLoader dataLoader, boolean verbose) {
        model.eval();
        EpochResult result = new EpochResult();
        
        int batchCount = 0;
        double totalLoss = 0.0;
        
        // 重置指标
        resetMetrics(validMetrics);
        
        // 验证模式下不需要梯度
        try (NoGradContext noGrad = new NoGradContext()) {
            for (DataLoader.Batch batch : dataLoader) {
                // 前向传播
                Variable predictions = model.forward(batch.inputs);
                Variable loss = lossFunction.forward(predictions, batch.targets);
                
                // 更新统计信息
                totalLoss += loss.getData();
                batchCount++;
                
                // 更新指标
                updateMetrics(validMetrics, predictions, batch.targets);
            }
        }
        
        result.loss = totalLoss / batchCount;
        result.metrics = computeMetrics(validMetrics);
        
        return result;
    }

    /**
     * 单步训练
     */
    public double trainStep(Variable inputs, Variable targets) {
        model.train();
        
        // 前向传播
        Variable predictions = model.forward(inputs);
        Variable loss = lossFunction.forward(predictions, targets);
        
        // 反向传播
        model.zeroGrad();
        loss.backward();
        optimizer.step();
        
        return loss.getData();
    }

    /**
     * 单步验证
     */
    public double validStep(Variable inputs, Variable targets) {
        model.eval();
        
        try (NoGradContext noGrad = new NoGradContext()) {
            Variable predictions = model.forward(inputs);
            Variable loss = lossFunction.forward(predictions, targets);
            return loss.getData();
        }
    }

    // 私有辅助方法
    
    private void resetMetrics(List<Metric> metrics) {
        for (Metric metric : metrics) {
            metric.reset();
        }
    }
    
    private void updateMetrics(List<Metric> metrics, Variable predictions, Variable targets) {
        for (Metric metric : metrics) {
            metric.update(predictions, targets);
        }
    }
    
    private Map<String, Double> computeMetrics(List<Metric> metrics) {
        Map<String, Double> results = new HashMap<>();
        for (Metric metric : metrics) {
            results.put(metric.getName(), metric.compute());
        }
        return results;
    }

    private void printEpochSummary(int epoch, EpochResult trainResult, EpochResult validResult) {
        StringBuilder sb = new StringBuilder();
        sb.append(String.format("Epoch %d/%d - ", epoch + 1, currentEpoch + 1));
        sb.append(String.format("loss: %.4f", trainResult.loss));
        
        for (Map.Entry<String, Double> entry : trainResult.metrics.entrySet()) {
            sb.append(String.format(" - %s: %.4f", entry.getKey(), entry.getValue()));
        }
        
        if (validResult != null) {
            sb.append(String.format(" - val_loss: %.4f", validResult.loss));
            for (Map.Entry<String, Double> entry : validResult.metrics.entrySet()) {
                sb.append(String.format(" - val_%s: %.4f", entry.getKey(), entry.getValue()));
            }
        }
        
        System.out.println(sb.toString());
    }

    // 回调方法
    
    private void callOnTrainBegin() {
        for (TrainingCallback callback : callbacks) {
            callback.onTrainBegin(this);
        }
    }
    
    private void callOnTrainEnd() {
        for (TrainingCallback callback : callbacks) {
            callback.onTrainEnd(this);
        }
    }
    
    private void callOnEpochBegin(int epoch) {
        for (TrainingCallback callback : callbacks) {
            callback.onEpochBegin(epoch, this);
        }
    }
    
    private void callOnEpochEnd(int epoch, EpochResult trainResult, EpochResult validResult) {
        for (TrainingCallback callback : callbacks) {
            callback.onEpochEnd(epoch, trainResult, validResult, this);
        }
    }
    
    private void callOnBatchBegin(int step) {
        for (TrainingCallback callback : callbacks) {
            callback.onBatchBegin(step, this);
        }
    }
    
    private void callOnBatchEnd(int step, double loss) {
        for (TrainingCallback callback : callbacks) {
            callback.onBatchEnd(step, loss, this);
        }
    }

    // Getter方法
    public Model getModel() { return model; }
    public Optimizer getOptimizer() { return optimizer; }
    public int getCurrentEpoch() { return currentEpoch; }
    public int getCurrentStep() { return currentStep; }
    public double getBestValidLoss() { return bestValidLoss; }
}
```

## 6.4.2 训练回调系统

```java
/**
 * 训练回调接口
 */
public interface TrainingCallback {
    default void onTrainBegin(Trainer trainer) {}
    default void onTrainEnd(Trainer trainer) {}
    default void onEpochBegin(int epoch, Trainer trainer) {}
    default void onEpochEnd(int epoch, EpochResult trainResult, EpochResult validResult, Trainer trainer) {}
    default void onBatchBegin(int step, Trainer trainer) {}
    default void onBatchEnd(int step, double loss, Trainer trainer) {}
}

/**
 * 早停回调
 */
public class EarlyStopping implements TrainingCallback {
    private final int patience;
    private final double minDelta;
    private final String monitor;
    
    private int waitCount = 0;
    private double bestScore = Double.MAX_VALUE;
    private boolean stopped = false;

    public EarlyStopping(int patience, double minDelta, String monitor) {
        this.patience = patience;
        this.minDelta = minDelta;
        this.monitor = monitor;
    }

    @Override
    public void onEpochEnd(int epoch, EpochResult trainResult, EpochResult validResult, Trainer trainer) {
        if (stopped) return;
        
        double currentScore = getCurrentScore(trainResult, validResult);
        
        if (currentScore < bestScore - minDelta) {
            bestScore = currentScore;
            waitCount = 0;
        } else {
            waitCount++;
            if (waitCount >= patience) {
                System.out.printf("Early stopping at epoch %d. Best %s: %.4f\n", 
                                epoch, monitor, bestScore);
                stopped = true;
            }
        }
    }
    
    private double getCurrentScore(EpochResult trainResult, EpochResult validResult) {
        if (monitor.equals("loss")) {
            return trainResult.loss;
        } else if (monitor.equals("val_loss") && validResult != null) {
            return validResult.loss;
        }
        return Double.MAX_VALUE;
    }
    
    public boolean isStopped() { return stopped; }
}

/**
 * 模型检查点回调
 */
public class ModelCheckpoint implements TrainingCallback {
    private final String filepath;
    private final String monitor;
    private final boolean saveWeightsOnly;
    private final boolean saveBestOnly;
    
    private double bestScore = Double.MAX_VALUE;

    public ModelCheckpoint(String filepath, String monitor, boolean saveWeightsOnly, boolean saveBestOnly) {
        this.filepath = filepath;
        this.monitor = monitor;
        this.saveWeightsOnly = saveWeightsOnly;
        this.saveBestOnly = saveBestOnly;
    }

    @Override
    public void onEpochEnd(int epoch, EpochResult trainResult, EpochResult validResult, Trainer trainer) {
        double currentScore = getCurrentScore(trainResult, validResult);
        
        boolean shouldSave = !saveBestOnly || currentScore < bestScore;
        
        if (shouldSave) {
            if (currentScore < bestScore) {
                bestScore = currentScore;
            }
            
            try {
                String filename = filepath.replace("{epoch}", String.valueOf(epoch))
                                        .replace("{score}", String.format("%.4f", currentScore));
                
                if (saveWeightsOnly) {
                    saveWeights(trainer.getModel(), filename);
                } else {
                    trainer.getModel().save(filename);
                }
                
                System.out.printf("Checkpoint saved: %s\n", filename);
            } catch (Exception e) {
                System.err.println("Failed to save checkpoint: " + e.getMessage());
            }
        }
    }
    
    private double getCurrentScore(EpochResult trainResult, EpochResult validResult) {
        if (monitor.equals("loss")) {
            return trainResult.loss;
        } else if (monitor.equals("val_loss") && validResult != null) {
            return validResult.loss;
        }
        return Double.MAX_VALUE;
    }
    
    private void saveWeights(Model model, String filename) {
        // 简化实现，仅保存权重
    }
}

/**
 * 学习率调度回调
 */
public class LearningRateScheduler implements TrainingCallback {
    private final double initialLR;
    private final ScheduleFunction scheduleFunction;

    public LearningRateScheduler(double initialLR, ScheduleFunction scheduleFunction) {
        this.initialLR = initialLR;
        this.scheduleFunction = scheduleFunction;
    }

    @Override
    public void onEpochBegin(int epoch, Trainer trainer) {
        double newLR = scheduleFunction.getLearningRate(epoch, initialLR);
        trainer.getOptimizer().setLearningRate(newLR);
    }

    public interface ScheduleFunction {
        double getLearningRate(int epoch, double initialLR);
    }
    
    // 预定义调度函数
    public static class ExponentialDecay implements ScheduleFunction {
        private final double decayRate;
        
        public ExponentialDecay(double decayRate) {
            this.decayRate = decayRate;
        }
        
        @Override
        public double getLearningRate(int epoch, double initialLR) {
            return initialLR * Math.pow(decayRate, epoch);
        }
    }
}
```

## 6.4.3 训练历史记录

```java
/**
 * 训练历史记录
 */
public class TrainingHistory {
    private final List<EpochResult> trainHistory = new ArrayList<>();
    private final List<EpochResult> validHistory = new ArrayList<>();

    public void addEpoch(EpochResult trainResult, EpochResult validResult) {
        trainHistory.add(trainResult);
        if (validResult != null) {
            validHistory.add(validResult);
        }
    }

    public List<Double> getTrainLosses() {
        return trainHistory.stream().map(r -> r.loss).collect(Collectors.toList());
    }

    public List<Double> getValidLosses() {
        return validHistory.stream().map(r -> r.loss).collect(Collectors.toList());
    }

    public List<Double> getTrainMetric(String metricName) {
        return trainHistory.stream()
                .map(r -> r.metrics.getOrDefault(metricName, 0.0))
                .collect(Collectors.toList());
    }

    public List<Double> getValidMetric(String metricName) {
        return validHistory.stream()
                .map(r -> r.metrics.getOrDefault(metricName, 0.0))
                .collect(Collectors.toList());
    }

    public void plot() {
        // 简化的绘图实现
        System.out.println("Training History:");
        System.out.println("Epoch\tTrain Loss\tValid Loss");
        
        for (int i = 0; i < trainHistory.size(); i++) {
            double trainLoss = trainHistory.get(i).loss;
            double validLoss = i < validHistory.size() ? validHistory.get(i).loss : 0.0;
            System.out.printf("%d\t%.4f\t\t%.4f\n", i + 1, trainLoss, validLoss);
        }
    }
}

/**
 * Epoch结果
 */
public class EpochResult {
    public double loss;
    public Map<String, Double> metrics = new HashMap<>();
    
    public EpochResult() {}
    
    public EpochResult(double loss, Map<String, Double> metrics) {
        this.loss = loss;
        this.metrics = metrics;
    }
}
```

## 6.4.4 无梯度上下文

```java
/**
 * 无梯度计算上下文
 */
public class NoGradContext implements AutoCloseable {
    private static final ThreadLocal<Boolean> gradEnabled = ThreadLocal.withInitial(() -> true);
    private final boolean previousState;

    public NoGradContext() {
        this.previousState = gradEnabled.get();
        gradEnabled.set(false);
    }

    public static boolean isGradEnabled() {
        return gradEnabled.get();
    }

    @Override
    public void close() {
        gradEnabled.set(previousState);
    }
}
```

## 6.4.5 使用示例

```java
/**
 * 训练循环使用示例
 */
public class TrainingExample {
    
    public static void demonstrateTraining() {
        // 创建模型
        SequentialModel model = new SequentialModel("MNIST_Classifier");
        model.add(new Dense(784, 128))
             .add(new ReLU())
             .add(new Dropout(0.2))
             .add(new Dense(128, 64))
             .add(new ReLU())
             .add(new Dropout(0.2))
             .add(new Dense(64, 10));
        
        model.initializeParameters(new int[]{1, 784});
        
        // 创建优化器和损失函数
        Optimizer optimizer = new Adam(0.001, 0.9, 0.999, 1e-8, 0.01, false);
        Loss lossFunction = new CrossEntropyLoss();
        
        // 创建训练器
        Trainer trainer = new Trainer(model, optimizer, lossFunction);
        
        // 添加指标
        trainer.addTrainMetric(new Accuracy())
               .addValidMetric(new Accuracy());
        
        // 添加回调
        trainer.addCallback(new EarlyStopping(10, 0.001, "val_loss"));
        trainer.addCallback(new ModelCheckpoint("best_model_{epoch}_{score}.model", 
                                               "val_loss", false, true));
        trainer.addCallback(new LearningRateScheduler(0.001, 
                new LearningRateScheduler.ExponentialDecay(0.95)));
        
        // 创建数据加载器（简化实现）
        DataLoader trainLoader = createTrainLoader();
        DataLoader validLoader = createValidLoader();
        
        // 开始训练
        TrainingHistory history = trainer.fit(trainLoader, validLoader, 50, true);
        
        // 查看训练历史
        history.plot();
        
        System.out.println("Training completed!");
    }
    
    public static void demonstrateCustomTrainingLoop() {
        SequentialModel model = createModel();
        Optimizer optimizer = new Adam(0.001);
        Loss lossFunction = new MeanSquaredError();
        
        Trainer trainer = new Trainer(model, optimizer, lossFunction);
        
        // 自定义训练循环
        for (int epoch = 0; epoch < 100; epoch++) {
            double totalLoss = 0.0;
            int batchCount = 0;
            
            // 训练阶段
            model.train();
            for (DataLoader.Batch batch : createTrainLoader()) {
                double loss = trainer.trainStep(batch.inputs, batch.targets);
                totalLoss += loss;
                batchCount++;
            }
            
            // 验证阶段
            model.eval();
            double validLoss = 0.0;
            int validBatchCount = 0;
            
            for (DataLoader.Batch batch : createValidLoader()) {
                double loss = trainer.validStep(batch.inputs, batch.targets);
                validLoss += loss;
                validBatchCount++;
            }
            
            double avgTrainLoss = totalLoss / batchCount;
            double avgValidLoss = validLoss / validBatchCount;
            
            System.out.printf("Epoch %d: Train Loss = %.4f, Valid Loss = %.4f\n", 
                            epoch, avgTrainLoss, avgValidLoss);
        }
    }
    
    private static SequentialModel createModel() {
        SequentialModel model = new SequentialModel("RegressionModel");
        model.add(new Dense(10, 64))
             .add(new ReLU())
             .add(new Dense(64, 32))
             .add(new ReLU())
             .add(new Dense(32, 1));
        
        model.initializeParameters(new int[]{1, 10});
        return model;
    }
    
    private static DataLoader createTrainLoader() {
        // 简化的数据加载器创建
        return new DataLoader(generateSampleData(1000), 32, true);
    }
    
    private static DataLoader createValidLoader() {
        return new DataLoader(generateSampleData(200), 32, false);
    }
    
    private static List<DataLoader.DataPoint> generateSampleData(int count) {
        List<DataLoader.DataPoint> data = new ArrayList<>();
        for (int i = 0; i < count; i++) {
            Variable input = Variable.randn(new int[]{784});
            Variable target = Variable.randint(0, 10, new int[]{1});
            data.add(new DataLoader.DataPoint(input, target));
        }
        return data;
    }
}
```

训练循环作为深度学习框架的核心引擎，协调了模型训练的所有关键步骤。通过合理的设计和完整的实现，我们构建了一个功能完备、易于使用的训练系统，支持各种训练策略和自定义需求。

下一节我们将探讨推理引擎的设计，学习如何优化模型部署和推理性能。