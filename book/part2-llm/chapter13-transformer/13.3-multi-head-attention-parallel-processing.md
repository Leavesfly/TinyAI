# 13.3 多头注意力：并行处理不同信息

> **设计思想**：掌握多头注意力的设计思想，理解并行处理不同类型关系的优势

## 本节概述

单头注意力机制虽然强大，但只能捕获序列中的一种类型的依赖关系。为了增强模型的表达能力，Transformer引入了多头注意力机制，通过并行计算多个注意力头来捕获不同类型的信息。本节将深入探讨多头注意力的设计动机、实现细节和优势。

## 学习目标

完成本节学习后，你将：

- ✅ **理解多头注意力的设计动机**：掌握捕获不同类型关系的需求
- ✅ **掌握多头注意力的实现细节**：理解头部投影矩阵的作用和初始化
- ✅ **学会多头输出的处理方法**：掌握拼接和线性变换的过程
- ✅ **具备注意力头分析能力**：能够可视化和分析不同注意力头的作用
- ✅ **实现完整的多头注意力机制**：能够编写多头注意力的完整代码

## 多头注意力的设计动机

### 单头注意力的局限性

单头注意力机制虽然能够捕获序列中的依赖关系，但其表达能力有限。在处理复杂的语言结构时，不同类型的依赖关系可能需要不同的关注模式。例如：

1. **语法依赖**：主谓宾等语法结构关系
2. **语义依赖**：同义词、反义词等语义关系
3. **位置依赖**：相邻词、远距离词等位置关系
4. **指代依赖**：代词与其指代对象的关系

单头注意力难以同时有效地捕获这些不同类型的依赖关系。

### 多头注意力的优势

多头注意力通过并行计算多个注意力头，每个头可以专注于捕获特定类型的依赖关系，从而显著增强模型的表达能力：

1. **并行处理**：多个头可以并行计算，提高计算效率
2. **多样性捕获**：不同头可以捕获不同类型的依赖关系
3. **鲁棒性增强**：即使某些头失效，其他头仍能提供有效信息
4. **表示能力提升**：组合多个头的信息可以表示更复杂的模式

## 多头注意力的实现细节

### 架构设计

多头注意力的基本架构包括以下几个步骤：

1. **线性投影**：将输入分别投影到多个头的Query、Key、Value空间
2. **并行计算**：在每个头上独立计算注意力
3. **输出拼接**：将所有头的输出拼接在一起
4. **线性变换**：通过线性变换将拼接后的输出映射到目标维度

### 数学表示

多头注意力的数学表示如下：

```
MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O

其中 head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)

W_i^Q ∈ R^{d_model × d_k}
W_i^K ∈ R^{d_model × d_k}
W_i^V ∈ R^{d_model × d_v}
W^O ∈ R^{hd_v × d_model}
```

### 参数设置

在标准的Transformer中，通常设置：
- h = 8（头数）
- d_model = 512（模型维度）
- d_k = d_v = 64（每个头的Key和Value维度）

这样每个头的参数数量为512×64×3=98,304，总参数数量与单头注意力相当。

## 头部投影矩阵的作用和初始化

### 投影矩阵的作用

每个注意力头都有独立的投影矩阵，这些矩阵的作用是：

1. **特征变换**：将输入映射到不同的表示空间
2. **关注模式**：不同的投影矩阵会产生不同的注意力关注模式
3. **信息提取**：每个头可以专注于提取不同类型的信息

### 初始化策略

投影矩阵通常采用以下初始化策略：

```java
public class MultiHeadAttention {
    private List<LinearLayer> queryProjections;
    private List<LinearLayer> keyProjections;
    private List<LinearLayer> valueProjections;
    
    public MultiHeadAttention(int numHeads, int dModel, int dHead) {
        this.queryProjections = new ArrayList<>();
        this.keyProjections = new ArrayList<>();
        this.valueProjections = new ArrayList<>();
        
        // 为每个头初始化独立的投影层
        for (int i = 0; i < numHeads; i++) {
            // 使用Xavier初始化
            queryProjections.add(new LinearLayer(dModel, dHead));
            keyProjections.add(new LinearLayer(dModel, dHead));
            valueProjections.add(new LinearLayer(dModel, dHead));
        }
    }
}
```

## 多头输出的拼接和线性变换

### 输出拼接过程

在计算完所有头的注意力输出后，需要将它们拼接在一起：

```java
private Variable concatHeads(List<Variable> headOutputs) {
    // headOutputs: List of (batch, seq_len, d_head)
    // 输出: (batch, seq_len, num_heads * d_head)
    return Variable.concat(headOutputs, -1);
}
```

### 线性变换

拼接后的输出维度为(num_heads × d_head)，通常不等于原始的d_model，因此需要通过线性变换将其映射回目标维度：

```java
private Variable linearTransform(Variable concatenated) {
    // concatenated: (batch, seq_len, num_heads * d_head)
    // 输出: (batch, seq_len, d_model)
    return outputProjection.forward(concatenated);
}
```

## 完整的多头注意力实现

### 类定义

```java
public class MultiHeadAttention extends Layer {
    private int numHeads;
    private int dModel;
    private int dHead;
    
    // 每个头的投影矩阵
    private List<LinearLayer> queryProjections;
    private List<LinearLayer> keyProjections;
    private List<LinearLayer> valueProjections;
    
    // 输出投影矩阵
    private LinearLayer outputProjection;
    
    public MultiHeadAttention(String name, int numHeads, int dModel) {
        super(name);
        this.numHeads = numHeads;
        this.dModel = dModel;
        this.dHead = dModel / numHeads;
        
        // 初始化投影矩阵
        initializeProjections();
    }
    
    private void initializeProjections() {
        queryProjections = new ArrayList<>();
        keyProjections = new ArrayList<>();
        valueProjections = new ArrayList<>();
        
        for (int i = 0; i < numHeads; i++) {
            queryProjections.add(new LinearLayer(dModel, dHead));
            keyProjections.add(new LinearLayer(dModel, dHead));
            valueProjections.add(new LinearLayer(dModel, dHead));
        }
        
        // 输出投影矩阵
        outputProjection = new LinearLayer(numHeads * dHead, dModel);
    }
}
```

### 前向计算

```java
@Override
public Variable forward(Variable... inputs) {
    Variable query = inputs[0];
    Variable key = inputs[1];
    Variable value = inputs[2];
    
    List<Variable> headOutputs = new ArrayList<>();
    
    // 并行计算每个头的注意力
    for (int i = 0; i < numHeads; i++) {
        // 1. 线性投影
        Variable Q = queryProjections.get(i).forward(query);
        Variable K = keyProjections.get(i).forward(key);
        Variable V = valueProjections.get(i).forward(value);
        
        // 2. 计算注意力
        Variable headOutput = computeAttention(Q, K, V);
        headOutputs.add(headOutput);
    }
    
    // 3. 拼接所有头的输出
    Variable concatenated = concatHeads(headOutputs);
    
    // 4. 线性变换
    return outputProjection.forward(concatenated);
}

private Variable computeAttention(Variable Q, Variable K, Variable V) {
    // 计算注意力分数
    Variable scores = Q.dot(K.transpose(-2, -1));
    scores = scores.div(Math.sqrt(dHead));
    
    // Softmax归一化
    Variable attentionWeights = scores.softmax(-1);
    
    // 加权求和
    return attentionWeights.dot(V);
}
```

## 注意力头的可视化和分析

### 可视化方法

通过可视化注意力权重矩阵，我们可以分析不同头的关注模式：

```java
public void visualizeAttentionHeads(Variable attentionWeights) {
    // attentionWeights: (batch, num_heads, seq_len, seq_len)
    for (int head = 0; head < numHeads; head++) {
        // 提取第head个头的注意力权重
        Variable headWeights = attentionWeights.slice(1, head);
        
        // 可视化热力图
        plotHeatmap(headWeights.getData(), "Head " + head + " Attention");
    }
}
```

### 头部功能分析

研究表明，不同的注意力头确实会关注不同类型的信息：

1. **语法头**：关注语法结构，如主谓宾关系
2. **指代头**：关注代词与其指代对象的关系
3. **语义头**：关注语义相似的词
4. **位置头**：关注相邻或特定位置的词

## 性能优化策略

### 并行计算优化

多头注意力天然支持并行计算，可以通过以下方式优化：

1. **批量处理**：将多个头的计算合并为批量操作
2. **内存优化**：合理管理中间变量的内存分配
3. **计算融合**：将多个操作融合为单个内核

### 稀疏化策略

对于某些头，可以采用稀疏化策略：

1. **局部注意力**：只关注局部窗口内的位置
2. **稀疏模式**：预定义稀疏的关注模式
3. **自适应稀疏**：根据输入动态调整稀疏模式

## 本节小结

本节深入探讨了多头注意力机制的设计思想和实现细节，我们学习了：

1. **多头注意力的设计动机**：理解了捕获不同类型依赖关系的需求
2. **多头注意力的实现细节**：掌握了线性投影、并行计算、输出拼接等过程
3. **头部投影矩阵的作用**：理解了特征变换和关注模式的形成
4. **多头输出的处理方法**：掌握了拼接和线性变换的技术
5. **注意力头的分析方法**：学会了可视化和分析不同头的作用

多头注意力机制通过并行处理多个注意力头，显著增强了模型的表达能力，是Transformer架构成功的关键因素之一。在下一节中，我们将学习位置编码技术，解决序列建模中的位置信息表示问题。