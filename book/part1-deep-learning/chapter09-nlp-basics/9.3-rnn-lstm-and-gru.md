# 9.3 循环神经网络：RNN、LSTM与GRU

> **本节学习目标**：掌握循环神经网络的核心结构和实现，理解LSTM和GRU如何解决RNN的梯度消失问题

## 内容概览

循环神经网络（Recurrent Neural Network, RNN）是一类专门用于处理序列数据的神经网络。与前馈神经网络不同，RNN具有记忆能力，能够利用序列中的历史信息来影响当前的输出。本节将详细介绍RNN的基本结构、存在的问题以及改进版本LSTM和GRU。

## 为什么需要循环神经网络？

在处理序列数据（如文本、时间序列）时，传统的前馈神经网络面临以下挑战：

1. **变长输入**：序列长度不固定，难以用固定大小的网络处理
2. **长距离依赖**：序列中相距较远的元素之间可能存在重要关系
3. **参数共享**：不同位置的相同模式应该使用相同的参数处理

RNN通过引入循环连接和参数共享机制，有效解决了这些问题。

## 9.3.1 RNN基本结构

RNN的核心思想是在时间维度上展开网络，每个时间步共享相同的参数。

### 数学表示

对于输入序列 $x = \{x_1, x_2, ..., x_T\}$，RNN在每个时间步t的计算如下：

$$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$
$$y_t = W_{hy}h_t + b_y$$

其中：
- $h_t$ 是时间步t的隐藏状态
- $W_{hh}$ 是隐藏状态到隐藏状态的权重矩阵
- $W_{xh}$ 是输入到隐藏状态的权重矩阵
- $W_{hy}$ 是隐藏状态到输出的权重矩阵

### RNN实现

```java
/**
 * 循环神经网络（RNN）实现
 */
public class RNN {
    private int inputSize;      // 输入维度
    private int hiddenSize;     // 隐藏层维度
    private int outputSize;     // 输出维度
    
    // 权重矩阵
    private double[][] wXh;     // 输入到隐藏层权重
    private double[][] wHh;     // 隐藏层到隐藏层权重
    private double[][] wHy;     // 隐藏层到输出权重
    
    // 偏置向量
    private double[] bH;        // 隐藏层偏置
    private double[] bY;        // 输出层偏置
    
    // 激活函数
    private ActivationFunction activation;
    
    /**
     * 构造函数
     */
    public RNN(int inputSize, int hiddenSize, int outputSize) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.outputSize = outputSize;
        
        // 初始化权重矩阵
        this.wXh = initializeMatrix(inputSize, hiddenSize);
        this.wHh = initializeMatrix(hiddenSize, hiddenSize);
        this.wHy = initializeMatrix(hiddenSize, outputSize);
        
        // 初始化偏置向量
        this.bH = new double[hiddenSize];
        this.bY = new double[outputSize];
        
        // 使用tanh激活函数
        this.activation = new TanhActivation();
    }
    
    /**
     * 初始化权重矩阵
     */
    private double[][] initializeMatrix(int rows, int cols) {
        double[][] matrix = new double[rows][cols];
        Random random = new Random(42);
        
        // Xavier初始化
        double scale = Math.sqrt(6.0 / (rows + cols));
        for (int i = 0; i < rows; i++) {
            for (int j = 0; j < cols; j++) {
                matrix[i][j] = (random.nextDouble() * 2 - 1) * scale;
            }
        }
        
        return matrix;
    }
    
    /**
     * 前向传播（单个时间步）
     * @param input 输入向量
     * @param prevHidden 上一时间步的隐藏状态
     * @return 当前时间步的隐藏状态
     */
    public double[] forwardStep(double[] input, double[] prevHidden) {
        // 计算隐藏状态: h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)
        double[] hidden = new double[hiddenSize];
        
        // W_hh * h_{t-1}
        for (int i = 0; i < hiddenSize; i++) {
            for (int j = 0; j < hiddenSize; j++) {
                hidden[i] += wHh[j][i] * prevHidden[j];
            }
        }
        
        // W_xh * x_t
        for (int i = 0; i < hiddenSize; i++) {
            for (int j = 0; j < inputSize; j++) {
                hidden[i] += wXh[j][i] * input[j];
            }
        }
        
        // 加上偏置
        for (int i = 0; i < hiddenSize; i++) {
            hidden[i] += bH[i];
        }
        
        // 应用激活函数
        return activation.forward(hidden);
    }
    
    /**
     * 完整序列的前向传播
     * @param inputs 输入序列
     * @return 隐藏状态序列和输出序列
     */
    public RNNOutput forward(double[][] inputs) {
        int sequenceLength = inputs.length;
        double[][] hiddenStates = new double[sequenceLength][hiddenSize];
        double[][] outputs = new double[sequenceLength][outputSize];
        
        // 初始隐藏状态为零向量
        double[] prevHidden = new double[hiddenSize];
        
        // 逐时间步计算
        for (int t = 0; t < sequenceLength; t++) {
            // 计算当前时间步的隐藏状态
            hiddenStates[t] = forwardStep(inputs[t], prevHidden);
            prevHidden = hiddenStates[t];
            
            // 计算输出: y_t = W_hy * h_t + b_y
            for (int i = 0; i < outputSize; i++) {
                outputs[t][i] = bY[i];
                for (int j = 0; j < hiddenSize; j++) {
                    outputs[t][i] += wHy[j][i] * hiddenStates[t][j];
                }
            }
        }
        
        return new RNNOutput(hiddenStates, outputs);
    }
    
    /**
     * 计算损失函数（均方误差）
     * @param predictions 预测值
     * @param targets 真实值
     * @return 损失值
     */
    public double computeLoss(double[][] predictions, double[][] targets) {
        if (predictions.length != targets.length) {
            throw new IllegalArgumentException("预测值和真实值序列长度不匹配");
        }
        
        double totalLoss = 0;
        int count = 0;
        
        for (int t = 0; t < predictions.length; t++) {
            for (int i = 0; i < predictions[t].length; i++) {
                double diff = predictions[t][i] - targets[t][i];
                totalLoss += diff * diff;
                count++;
            }
        }
        
        return totalLoss / count;
    }
    
    /**
     * 获取模型参数（用于序列化）
     */
    public RNNParameters getParameters() {
        return new RNNParameters(wXh, wHh, wHy, bH, bY);
    }
    
    /**
     * 设置模型参数
     */
    public void setParameters(RNNParameters parameters) {
        this.wXh = parameters.getWXh();
        this.wHh = parameters.getWHh();
        this.wHy = parameters.getWHy();
        this.bH = parameters.getBH();
        this.bY = parameters.getBY();
    }
    
    // Getter方法
    public int getInputSize() { return inputSize; }
    public int getHiddenSize() { return hiddenSize; }
    public int getOutputSize() { return outputSize; }
}

/**
 * RNN输出结果封装类
 */
class RNNOutput {
    private double[][] hiddenStates;
    private double[][] outputs;
    
    public RNNOutput(double[][] hiddenStates, double[][] outputs) {
        this.hiddenStates = hiddenStates;
        this.outputs = outputs;
    }
    
    public double[][] getHiddenStates() { return hiddenStates; }
    public double[][] getOutputs() { return outputs; }
}

/**
 * RNN参数封装类
 */
class RNNParameters {
    private double[][] wXh;
    private double[][] wHh;
    private double[][] wHy;
    private double[] bH;
    private double[] bY;
    
    public RNNParameters(double[][] wXh, double[][] wHh, double[][] wHy, 
                        double[] bH, double[] bY) {
        this.wXh = wXh;
        this.wHh = wHh;
        this.wHy = wHy;
        this.bH = bH;
        this.bY = bY;
    }
    
    // Getter方法
    public double[][] getWXh() { return wXh; }
    public double[][] getWHh() { return wHh; }
    public double[][] getWHy() { return wHy; }
    public double[] getBH() { return bH; }
    public double[] getBY() { return bY; }
}
```

## 9.3.2 RNN存在的问题

尽管RNN在理论上能够处理任意长度的序列，但在实际应用中存在严重问题：

### 梯度消失问题

在反向传播过程中，梯度需要通过时间反向传播（Backpropagation Through Time, BPTT），当序列较长时，梯度会逐层衰减，导致：

1. **难以学习长距离依赖**：网络无法有效利用序列早期的信息
2. **训练困难**：梯度过小导致参数更新缓慢

### 梯度爆炸问题

在某些情况下，梯度可能会指数级增长，导致：

1. **数值不稳定**：权重更新过大，模型无法收敛
2. **训练失败**：损失函数出现NaN或无穷大值

### 梯度裁剪实现

为了解决梯度爆炸问题，我们可以使用梯度裁剪技术：

```java
/**
 * 梯度裁剪工具类
 */
public class GradientClipping {
    
    /**
     * 按范数裁剪梯度
     * @param gradients 梯度数组
     * @param maxNorm 最大范数阈值
     * @return 裁剪后的梯度
     */
    public static double[][] clipByNorm(double[][] gradients, double maxNorm) {
        double norm = computeNorm(gradients);
        
        if (norm > maxNorm) {
            double scale = maxNorm / norm;
            return scaleMatrix(gradients, scale);
        }
        
        return gradients;
    }
    
    /**
     * 按值裁剪梯度
     * @param gradients 梯度数组
     * @param clipValue 裁剪值
     * @return 裁剪后的梯度
     */
    public static double[][] clipByValue(double[][] gradients, double clipValue) {
        double[][] clipped = new double[gradients.length][];
        
        for (int i = 0; i < gradients.length; i++) {
            clipped[i] = new double[gradients[i].length];
            for (int j = 0; j < gradients[i].length; j++) {
                clipped[i][j] = Math.max(-clipValue, Math.min(clipValue, gradients[i][j]));
            }
        }
        
        return clipped;
    }
    
    /**
     * 计算矩阵的L2范数
     */
    private static double computeNorm(double[][] matrix) {
        double sum = 0;
        for (int i = 0; i < matrix.length; i++) {
            for (int j = 0; j < matrix[i].length; j++) {
                sum += matrix[i][j] * matrix[i][j];
            }
        }
        return Math.sqrt(sum);
    }
    
    /**
     * 缩放矩阵
     */
    private static double[][] scaleMatrix(double[][] matrix, double scale) {
        double[][] scaled = new double[matrix.length][];
        
        for (int i = 0; i < matrix.length; i++) {
            scaled[i] = new double[matrix[i].length];
            for (int j = 0; j < matrix[i].length; j++) {
                scaled[i][j] = matrix[i][j] * scale;
            }
        }
        
        return scaled;
    }
}
```

## 9.3.3 LSTM网络

长短期记忆网络（Long Short-Term Memory, LSTM）通过引入门控机制解决了RNN的梯度消失问题。

### LSTM核心结构

LSTM包含三个门控机制：
1. **遗忘门（Forget Gate）**：决定丢弃多少历史信息
2. **输入门（Input Gate）**：决定存储多少新信息
3. **输出门（Output Gate）**：决定输出多少细胞状态

### LSTM数学公式

$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$
$$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
$$h_t = o_t * \tanh(C_t)$$

### LSTM实现

```java
/**
 * 长短期记忆网络（LSTM）实现
 */
public class LSTM {
    private int inputSize;      // 输入维度
    private int hiddenSize;     // 隐藏层维度
    private int outputSize;     // 输出维度
    
    // LSTM门控权重矩阵
    private double[][] wF;      // 遗忘门权重
    private double[][] wI;      // 输入门权重
    private double[][] wC;      // 候选细胞状态权重
    private double[][] wO;      // 输出门权重
    
    // 隐藏状态到门控的权重矩阵
    private double[][] uF;      // 遗忘门隐藏权重
    private double[][] uI;      // 输入门隐藏权重
    private double[][] uC;      // 候选细胞状态隐藏权重
    private double[][] uO;      // 输出门隐藏权重
    
    // 门控偏置向量
    private double[] bF;        // 遗忘门偏置
    private double[] bI;        // 输入门偏置
    private double[] bC;        // 候选细胞状态偏置
    private double[] bO;        // 输出门偏置
    
    // 输出层权重和偏置
    private double[][] wY;      // 输出权重
    private double[] bY;        // 输出偏置
    
    // 激活函数
    private SigmoidActivation sigmoid;
    private TanhActivation tanh;
    
    /**
     * 构造函数
     */
    public LSTM(int inputSize, int hiddenSize, int outputSize) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.outputSize = outputSize;
        
        // 初始化门控权重
        this.wF = initializeMatrix(inputSize, hiddenSize);
        this.wI = initializeMatrix(inputSize, hiddenSize);
        this.wC = initializeMatrix(inputSize, hiddenSize);
        this.wO = initializeMatrix(inputSize, hiddenSize);
        
        // 初始化隐藏状态权重
        this.uF = initializeMatrix(hiddenSize, hiddenSize);
        this.uI = initializeMatrix(hiddenSize, hiddenSize);
        this.uC = initializeMatrix(hiddenSize, hiddenSize);
        this.uO = initializeMatrix(hiddenSize, hiddenSize);
        
        // 初始化偏置
        this.bF = new double[hiddenSize];
        this.bI = new double[hiddenSize];
        this.bC = new double[hiddenSize];
        this.bO = new double[hiddenSize];
        
        // 初始化输出层参数
        this.wY = initializeMatrix(hiddenSize, outputSize);
        this.bY = new double[outputSize];
        
        // 初始化激活函数
        this.sigmoid = new SigmoidActivation();
        this.tanh = new TanhActivation();
    }
    
    /**
     * 初始化权重矩阵
     */
    private double[][] initializeMatrix(int rows, int cols) {
        double[][] matrix = new double[rows][cols];
        Random random = new Random(42);
        
        // Xavier初始化
        double scale = Math.sqrt(6.0 / (rows + cols));
        for (int i = 0; i < rows; i++) {
            for (int j = 0; j < cols; j++) {
                matrix[i][j] = (random.nextDouble() * 2 - 1) * scale;
            }
        }
        
        return matrix;
    }
    
    /**
     * LSTM单元计算（单个时间步）
     * @param input 输入向量
     * @param prevState 前一时间步的状态
     * @return 当前时间步的状态
     */
    public LSTMState forwardStep(double[] input, LSTMState prevState) {
        double[] prevHidden = prevState.getHidden();
        double[] prevCell = prevState.getCell();
        
        // 连接输入和前一隐藏状态
        double[] concat = concatenate(input, prevHidden);
        
        // 计算各个门控
        // 遗忘门: f_t = σ(W_f * [h_{t-1}, x_t] + b_f)
        double[] forgetGate = computeGate(wF, uF, bF, concat, prevHidden, sigmoid);
        
        // 输入门: i_t = σ(W_i * [h_{t-1}, x_t] + b_i)
        double[] inputGate = computeGate(wI, uI, bI, concat, prevHidden, sigmoid);
        
        // 候选细胞状态: C̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)
        double[] candidateCell = computeGate(wC, uC, bC, concat, prevHidden, tanh);
        
        // 细胞状态: C_t = f_t * C_{t-1} + i_t * C̃_t
        double[] cellState = new double[hiddenSize];
        for (int i = 0; i < hiddenSize; i++) {
            cellState[i] = forgetGate[i] * prevCell[i] + inputGate[i] * candidateCell[i];
        }
        
        // 输出门: o_t = σ(W_o * [h_{t-1}, x_t] + b_o)
        double[] outputGate = computeGate(wO, uO, bO, concat, prevHidden, sigmoid);
        
        // 隐藏状态: h_t = o_t * tanh(C_t)
        double[] hiddenState = new double[hiddenSize];
        double[] tanhCell = tanh.forward(cellState);
        for (int i = 0; i < hiddenSize; i++) {
            hiddenState[i] = outputGate[i] * tanhCell[i];
        }
        
        return new LSTMState(hiddenState, cellState);
    }
    
    /**
     * 计算门控值
     */
    private double[] computeGate(double[][] wInput, double[][] wHidden, double[] bias,
                               double[] concat, double[] prevHidden, 
                               ActivationFunction activation) {
        double[] gate = new double[hiddenSize];
        
        // W * input
        for (int i = 0; i < hiddenSize; i++) {
            for (int j = 0; j < concat.length; j++) {
                gate[i] += wInput[j][i] * concat[j];
            }
        }
        
        // U * hidden
        for (int i = 0; i < hiddenSize; i++) {
            for (int j = 0; j < hiddenSize; j++) {
                gate[i] += wHidden[j][i] * prevHidden[j];
            }
        }
        
        // 加上偏置
        for (int i = 0; i < hiddenSize; i++) {
            gate[i] += bias[i];
        }
        
        // 应用激活函数
        return activation.forward(gate);
    }
    
    /**
     * 连接两个向量
     */
    private double[] concatenate(double[] a, double[] b) {
        double[] result = new double[a.length + b.length];
        System.arraycopy(a, 0, result, 0, a.length);
        System.arraycopy(b, 0, result, a.length, b.length);
        return result;
    }
    
    /**
     * 完整序列的前向传播
     * @param inputs 输入序列
     * @return LSTM输出结果
     */
    public LSTMOutput forward(double[][] inputs) {
        int sequenceLength = inputs.length;
        double[][] hiddenStates = new double[sequenceLength][hiddenSize];
        double[][] cellStates = new double[sequenceLength][hiddenSize];
        double[][] outputs = new double[sequenceLength][outputSize];
        
        // 初始状态
        LSTMState prevState = new LSTMState(
            new double[hiddenSize],  // 初始隐藏状态为零向量
            new double[hiddenSize]   // 初始细胞状态为零向量
        );
        
        // 逐时间步计算
        for (int t = 0; t < sequenceLength; t++) {
            LSTMState currentState = forwardStep(inputs[t], prevState);
            hiddenStates[t] = currentState.getHidden();
            cellStates[t] = currentState.getCell();
            prevState = currentState;
            
            // 计算输出
            for (int i = 0; i < outputSize; i++) {
                outputs[t][i] = bY[i];
                for (int j = 0; j < hiddenSize; j++) {
                    outputs[t][i] += wY[j][i] * hiddenStates[t][j];
                }
            }
        }
        
        return new LSTMOutput(hiddenStates, cellStates, outputs);
    }
    
    // Getter方法
    public int getInputSize() { return inputSize; }
    public int getHiddenSize() { return hiddenSize; }
    public int getOutputSize() { return outputSize; }
}

/**
 * LSTM状态封装类
 */
class LSTMState {
    private double[] hidden;
    private double[] cell;
    
    public LSTMState(double[] hidden, double[] cell) {
        this.hidden = hidden;
        this.cell = cell;
    }
    
    public double[] getHidden() { return hidden; }
    public double[] getCell() { return cell; }
}

/**
 * LSTM输出结果封装类
 */
class LSTMOutput {
    private double[][] hiddenStates;
    private double[][] cellStates;
    private double[][] outputs;
    
    public LSTMOutput(double[][] hiddenStates, double[][] cellStates, double[][] outputs) {
        this.hiddenStates = hiddenStates;
        this.cellStates = cellStates;
        this.outputs = outputs;
    }
    
    public double[][] getHiddenStates() { return hiddenStates; }
    public double[][] getCellStates() { return cellStates; }
    public double[][] getOutputs() { return outputs; }
}
```

## 9.3.4 GRU网络

门控循环单元（Gated Recurrent Unit, GRU）是LSTM的简化版本，只有两个门控机制，但性能相当。

### GRU核心结构

GRU包含两个门控机制：
1. **重置门（Reset Gate）**：决定忽略多少历史信息
2. **更新门（Update Gate）**：决定更新多少历史信息

### GRU数学公式

$$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$
$$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$
$$\tilde{h}_t = \tanh(W \cdot [r_t * h_{t-1}, x_t] + b)$$
$$h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$$

### GRU实现

```java
/**
 * 门控循环单元（GRU）实现
 */
public class GRU {
    private int inputSize;      // 输入维度
    private int hiddenSize;     // 隐藏层维度
    private int outputSize;     // 输出维度
    
    // GRU权重矩阵
    private double[][] wZ;      // 更新门输入权重
    private double[][] wR;      // 重置门输入权重
    private double[][] wH;      // 候选隐藏状态输入权重
    
    // 隐藏状态权重矩阵
    private double[][] uZ;      // 更新门隐藏权重
    private double[][] uR;      // 重置门隐藏权重
    private double[][] uH;      // 候选隐藏状态隐藏权重
    
    // 偏置向量
    private double[] bZ;        // 更新门偏置
    private double[] bR;        // 重置门偏置
    private double[] bH;        // 候选隐藏状态偏置
    
    // 输出层参数
    private double[][] wY;      // 输出权重
    private double[] bY;        // 输出偏置
    
    // 激活函数
    private SigmoidActivation sigmoid;
    private TanhActivation tanh;
    
    /**
     * 构造函数
     */
    public GRU(int inputSize, int hiddenSize, int outputSize) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.outputSize = outputSize;
        
        // 初始化权重矩阵
        this.wZ = initializeMatrix(inputSize, hiddenSize);
        this.wR = initializeMatrix(inputSize, hiddenSize);
        this.wH = initializeMatrix(inputSize, hiddenSize);
        
        this.uZ = initializeMatrix(hiddenSize, hiddenSize);
        this.uR = initializeMatrix(hiddenSize, hiddenSize);
        this.uH = initializeMatrix(hiddenSize, hiddenSize);
        
        // 初始化偏置
        this.bZ = new double[hiddenSize];
        this.bR = new double[hiddenSize];
        this.bH = new double[hiddenSize];
        
        // 初始化输出层参数
        this.wY = initializeMatrix(hiddenSize, outputSize);
        this.bY = new double[outputSize];
        
        // 初始化激活函数
        this.sigmoid = new SigmoidActivation();
        this.tanh = new TanhActivation();
    }
    
    /**
     * 初始化权重矩阵
     */
    private double[][] initializeMatrix(int rows, int cols) {
        double[][] matrix = new double[rows][cols];
        Random random = new Random(42);
        
        // Xavier初始化
        double scale = Math.sqrt(6.0 / (rows + cols));
        for (int i = 0; i < rows; i++) {
            for (int j = 0; j < cols; j++) {
                matrix[i][j] = (random.nextDouble() * 2 - 1) * scale;
            }
        }
        
        return matrix;
    }
    
    /**
     * GRU单元计算（单个时间步）
     * @param input 输入向量
     * @param prevHidden 前一时间步的隐藏状态
     * @return 当前时间步的隐藏状态
     */
    public double[] forwardStep(double[] input, double[] prevHidden) {
        // 连接输入和前一隐藏状态
        double[] concat = concatenate(input, prevHidden);
        
        // 计算更新门: z_t = σ(W_z * [h_{t-1}, x_t] + b_z)
        double[] updateGate = computeGate(wZ, uZ, bZ, concat, prevHidden, sigmoid);
        
        // 计算重置门: r_t = σ(W_r * [h_{t-1}, x_t] + b_r)
        double[] resetGate = computeGate(wR, uR, bR, concat, prevHidden, sigmoid);
        
        // 计算候选隐藏状态
        // r_t * h_{t-1}
        double[] resetHidden = new double[hiddenSize];
        for (int i = 0; i < hiddenSize; i++) {
            resetHidden[i] = resetGate[i] * prevHidden[i];
        }
        
        // 连接重置后的隐藏状态和输入
        double[] resetConcat = concatenate(input, resetHidden);
        
        // 候选隐藏状态: h̃_t = tanh(W * [r_t * h_{t-1}, x_t] + b)
        double[] candidateHidden = new double[hiddenSize];
        
        // W * [r_t * h_{t-1}, x_t]
        for (int i = 0; i < hiddenSize; i++) {
            for (int j = 0; j < resetConcat.length; j++) {
                candidateHidden[i] += wH[j][i] * resetConcat[j];
            }
        }
        
        // 加上偏置
        for (int i = 0; i < hiddenSize; i++) {
            candidateHidden[i] += bH[i];
        }
        
        candidateHidden = tanh.forward(candidateHidden);
        
        // 计算最终隐藏状态: h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t
        double[] hiddenState = new double[hiddenSize];
        for (int i = 0; i < hiddenSize; i++) {
            hiddenState[i] = (1 - updateGate[i]) * prevHidden[i] + 
                           updateGate[i] * candidateHidden[i];
        }
        
        return hiddenState;
    }
    
    /**
     * 计算门控值
     */
    private double[] computeGate(double[][] wInput, double[][] wHidden, double[] bias,
                               double[] concat, double[] prevHidden, 
                               ActivationFunction activation) {
        double[] gate = new double[hiddenSize];
        
        // W * input
        for (int i = 0; i < hiddenSize; i++) {
            for (int j = 0; j < concat.length; j++) {
                gate[i] += wInput[j][i] * concat[j];
            }
        }
        
        // U * hidden
        for (int i = 0; i < hiddenSize; i++) {
            for (int j = 0; j < hiddenSize; j++) {
                gate[i] += wHidden[j][i] * prevHidden[j];
            }
        }
        
        // 加上偏置
        for (int i = 0; i < hiddenSize; i++) {
            gate[i] += bias[i];
        }
        
        // 应用激活函数
        return activation.forward(gate);
    }
    
    /**
     * 连接两个向量
     */
    private double[] concatenate(double[] a, double[] b) {
        double[] result = new double[a.length + b.length];
        System.arraycopy(a, 0, result, 0, a.length);
        System.arraycopy(b, 0, result, a.length, b.length);
        return result;
    }
    
    /**
     * 完整序列的前向传播
     * @param inputs 输入序列
     * @return 隐藏状态序列和输出序列
     */
    public GRUOutput forward(double[][] inputs) {
        int sequenceLength = inputs.length;
        double[][] hiddenStates = new double[sequenceLength][hiddenSize];
        double[][] outputs = new double[sequenceLength][outputSize];
        
        // 初始隐藏状态为零向量
        double[] prevHidden = new double[hiddenSize];
        
        // 逐时间步计算
        for (int t = 0; t < sequenceLength; t++) {
            hiddenStates[t] = forwardStep(inputs[t], prevHidden);
            prevHidden = hiddenStates[t];
            
            // 计算输出
            for (int i = 0; i < outputSize; i++) {
                outputs[t][i] = bY[i];
                for (int j = 0; j < hiddenSize; j++) {
                    outputs[t][i] += wY[j][i] * hiddenStates[t][j];
                }
            }
        }
        
        return new GRUOutput(hiddenStates, outputs);
    }
    
    // Getter方法
    public int getInputSize() { return inputSize; }
    public int getHiddenSize() { return hiddenSize; }
    public int getOutputSize() { return outputSize; }
}

/**
 * GRU输出结果封装类
 */
class GRUOutput {
    private double[][] hiddenStates;
    private double[][] outputs;
    
    public GRUOutput(double[][] hiddenStates, double[][] outputs) {
        this.hiddenStates = hiddenStates;
        this.outputs = outputs;
    }
    
    public double[][] getHiddenStates() { return hiddenStates; }
    public double[][] getOutputs() { return outputs; }
}
```

## 9.3.5 激活函数实现

为了完整性，我们需要实现使用的激活函数：

```java
/**
 * 激活函数接口
 */
interface ActivationFunction {
    double[] forward(double[] input);
    double[] backward(double[] input, double[] output);
}

/**
 * Sigmoid激活函数
 */
class SigmoidActivation implements ActivationFunction {
    @Override
    public double[] forward(double[] input) {
        double[] output = new double[input.length];
        for (int i = 0; i < input.length; i++) {
            output[i] = 1.0 / (1.0 + Math.exp(-input[i]));
        }
        return output;
    }
    
    @Override
    public double[] backward(double[] input, double[] output) {
        double[] gradient = new double[input.length];
        for (int i = 0; i < input.length; i++) {
            gradient[i] = output[i] * (1.0 - output[i]);
        }
        return gradient;
    }
}

/**
 * Tanh激活函数
 */
class TanhActivation implements ActivationFunction {
    @Override
    public double[] forward(double[] input) {
        double[] output = new double[input.length];
        for (int i = 0; i < input.length; i++) {
            output[i] = Math.tanh(input[i]);
        }
        return output;
    }
    
    @Override
    public double[] backward(double[] input, double[] output) {
        double[] gradient = new double[input.length];
        for (int i = 0; i < input.length; i++) {
            gradient[i] = 1.0 - output[i] * output[i];
        }
        return gradient;
    }
}
```

## 9.3.6 完整示例

让我们通过一个完整的示例来演示如何使用这些循环神经网络：

```java
/**
 * RNN、LSTM、GRU对比示例
 */
public class RNNComparisonExample {
    public static void main(String[] args) {
        // 示例数据：简单的序列预测任务
        // 输入：[0, 1, 2, 3, 4] -> 输出：[1, 2, 3, 4, 5]
        
        int sequenceLength = 5;
        int inputSize = 1;
        int hiddenSize = 10;
        int outputSize = 1;
        
        // 生成训练数据
        double[][][] trainingData = generateTrainingData(100);
        
        System.out.println("=== RNN 示例 ===");
        demonstrateRNN(trainingData, inputSize, hiddenSize, outputSize);
        
        System.out.println("\n=== LSTM 示例 ===");
        demonstrateLSTM(trainingData, inputSize, hiddenSize, outputSize);
        
        System.out.println("\n=== GRU 示例 ===");
        demonstrateGRU(trainingData, inputSize, hiddenSize, outputSize);
    }
    
    /**
     * 生成训练数据
     */
    private static double[][][] generateTrainingData(int count) {
        double[][][] data = new double[count][][];
        
        Random random = new Random(42);
        for (int i = 0; i < count; i++) {
            double start = random.nextDouble() * 10;
            double[][] sequence = new double[5][2]; // [input, target]
            
            for (int j = 0; j < 5; j++) {
                sequence[j][0] = start + j;      // 输入
                sequence[j][1] = start + j + 1;  // 目标（下一个数）
            }
            
            data[i] = sequence;
        }
        
        return data;
    }
    
    /**
     * RNN演示
     */
    private static void demonstrateRNN(double[][][] trainingData, 
                                     int inputSize, int hiddenSize, int outputSize) {
        RNN rnn = new RNN(inputSize, hiddenSize, outputSize);
        
        // 训练
        int epochs = 100;
        double learningRate = 0.01;
        
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalLoss = 0;
            
            for (double[][] sequence : trainingData) {
                // 准备输入和目标
                double[][] inputs = new double[sequence.length][inputSize];
                double[][] targets = new double[sequence.length][outputSize];
                
                for (int t = 0; t < sequence.length; t++) {
                    inputs[t][0] = sequence[t][0];
                    targets[t][0] = sequence[t][1];
                }
                
                // 前向传播
                RNNOutput output = rnn.forward(inputs);
                double[][] predictions = output.getOutputs();
                
                // 计算损失
                double loss = rnn.computeLoss(predictions, targets);
                totalLoss += loss;
                
                // 简化的反向传播（这里省略具体实现）
                // 在实际应用中需要实现完整的BPTT算法
            }
            
            if (epoch % 20 == 0) {
                System.out.printf("Epoch %d, Average Loss: %.6f%n", epoch, 
                                totalLoss / trainingData.length);
            }
        }
        
        // 测试
        double[][] testInput = {{1.0}, {2.0}, {3.0}, {4.0}, {5.0}};
        RNNOutput testOutput = rnn.forward(testInput);
        double[][] predictions = testOutput.getOutputs();
        
        System.out.println("测试结果:");
        for (int i = 0; i < testInput.length; i++) {
            System.out.printf("输入: %.1f, 预测: %.3f, 目标: %.1f%n", 
                            testInput[i][0], predictions[i][0], testInput[i][0] + 1);
        }
    }
    
    /**
     * LSTM演示
     */
    private static void demonstrateLSTM(double[][][] trainingData, 
                                      int inputSize, int hiddenSize, int outputSize) {
        LSTM lstm = new LSTM(inputSize, hiddenSize, outputSize);
        
        // 训练
        int epochs = 100;
        double learningRate = 0.01;
        
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalLoss = 0;
            
            for (double[][] sequence : trainingData) {
                // 准备输入和目标
                double[][] inputs = new double[sequence.length][inputSize];
                double[][] targets = new double[sequence.length][outputSize];
                
                for (int t = 0; t < sequence.length; t++) {
                    inputs[t][0] = sequence[t][0];
                    targets[t][0] = sequence[t][1];
                }
                
                // 前向传播
                LSTMOutput output = lstm.forward(inputs);
                double[][] predictions = output.getOutputs();
                
                // 计算损失
                double loss = computeLoss(predictions, targets);
                totalLoss += loss;
                
                // 简化的反向传播（这里省略具体实现）
            }
            
            if (epoch % 20 == 0) {
                System.out.printf("Epoch %d, Average Loss: %.6f%n", epoch, 
                                totalLoss / trainingData.length);
            }
        }
        
        // 测试
        double[][] testInput = {{1.0}, {2.0}, {3.0}, {4.0}, {5.0}};
        LSTMOutput testOutput = lstm.forward(testInput);
        double[][] predictions = testOutput.getOutputs();
        
        System.out.println("测试结果:");
        for (int i = 0; i < testInput.length; i++) {
            System.out.printf("输入: %.1f, 预测: %.3f, 目标: %.1f%n", 
                            testInput[i][0], predictions[i][0], testInput[i][0] + 1);
        }
    }
    
    /**
     * GRU演示
     */
    private static void demonstrateGRU(double[][][] trainingData, 
                                     int inputSize, int hiddenSize, int outputSize) {
        GRU gru = new GRU(inputSize, hiddenSize, outputSize);
        
        // 训练
        int epochs = 100;
        double learningRate = 0.01;
        
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalLoss = 0;
            
            for (double[][] sequence : trainingData) {
                // 准备输入和目标
                double[][] inputs = new double[sequence.length][inputSize];
                double[][] targets = new double[sequence.length][outputSize];
                
                for (int t = 0; t < sequence.length; t++) {
                    inputs[t][0] = sequence[t][0];
                    targets[t][0] = sequence[t][1];
                }
                
                // 前向传播
                GRUOutput output = gru.forward(inputs);
                double[][] predictions = output.getOutputs();
                
                // 计算损失
                double loss = computeLoss(predictions, targets);
                totalLoss += loss;
                
                // 简化的反向传播（这里省略具体实现）
            }
            
            if (epoch % 20 == 0) {
                System.out.printf("Epoch %d, Average Loss: %.6f%n", epoch, 
                                totalLoss / trainingData.length);
            }
        }
        
        // 测试
        double[][] testInput = {{1.0}, {2.0}, {3.0}, {4.0}, {5.0}};
        GRUOutput testOutput = gru.forward(testInput);
        double[][] predictions = testOutput.getOutputs();
        
        System.out.println("测试结果:");
        for (int i = 0; i < testInput.length; i++) {
            System.out.printf("输入: %.1f, 预测: %.3f, 目标: %.1f%n", 
                            testInput[i][0], predictions[i][0], testInput[i][0] + 1);
        }
    }
    
    /**
     * 计算损失函数（均方误差）
     */
    private static double computeLoss(double[][] predictions, double[][] targets) {
        if (predictions.length != targets.length) {
            throw new IllegalArgumentException("预测值和真实值序列长度不匹配");
        }
        
        double totalLoss = 0;
        int count = 0;
        
        for (int t = 0; t < predictions.length; t++) {
            for (int i = 0; i < predictions[t].length; i++) {
                double diff = predictions[t][i] - targets[t][i];
                totalLoss += diff * diff;
                count++;
            }
        }
        
        return totalLoss / count;
    }
}
```

## 本节小结

在本节中，我们深入学习了循环神经网络的核心技术：

1. **RNN基础**：理解了RNN的基本结构和工作原理
2. **RNN问题**：分析了梯度消失和梯度爆炸问题及其解决方案
3. **LSTM网络**：掌握了长短期记忆网络的门控机制和实现
4. **GRU网络**：学习了门控循环单元的简化结构
5. **实际应用**：通过完整示例演示了各种RNN模型的使用

通过TinyAI框架的实现，我们不仅掌握了理论知识，还获得了实际的编程经验。RNN及其变体是处理序列数据的强大工具，在NLP、时间序列分析等领域有着广泛应用。

## 下一步计划

在下一节中，我们将学习如何将这些技术应用到实际的文本分类任务中，通过情感分析项目来巩固所学知识。