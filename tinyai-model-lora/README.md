# TinyAI LoRA æ¨¡å—

> é«˜æ•ˆå‚æ•°å¾®è°ƒæŠ€æœ¯çš„å®Œæ•´å®ç° - åŸºäºTinyAIæ·±åº¦å­¦ä¹ æ¡†æ¶

![Java](https://img.shields.io/badge/Java-17+-brightgreen.svg)
![Maven](https://img.shields.io/badge/Maven-3.6+-green.svg)
![License](https://img.shields.io/badge/License-MIT-blue.svg)

## ğŸ“‹ æ¦‚è¿°

TinyAI LoRAæ¨¡å—å®ç°äº†å®Œæ•´çš„LoRAï¼ˆLow-Rank Adaptationï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶å¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚æœ¬æ¨¡å—åŸºäºTinyAIæ·±åº¦å­¦ä¹ æ¡†æ¶æ„å»ºï¼Œæä¾›äº†ä»é…ç½®ç®¡ç†åˆ°æ¨¡å‹æ„å»ºçš„å…¨æ ˆå®ç°ã€‚

### ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

- **å‚æ•°é«˜æ•ˆ**: å¯å®ç°95%+çš„å‚æ•°å‡å°‘ï¼Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨
- **æ€§èƒ½ä¿æŒ**: åœ¨å‡å°‘å‚æ•°çš„åŒæ—¶ä¿æŒåŸæœ‰æ¨¡å‹æ€§èƒ½
- **çµæ´»æ§åˆ¶**: æ”¯æŒåŠ¨æ€å¯ç”¨/ç¦ç”¨LoRAé€‚é…å™¨
- **æ˜“äºé›†æˆ**: ä¸TinyAIæ¡†æ¶æ— ç¼é›†æˆï¼ŒAPIç®€æ´æ˜“ç”¨
- **æƒé‡ç®¡ç†**: æ”¯æŒæƒé‡å†»ç»“ã€åˆå¹¶ç­‰é«˜çº§åŠŸèƒ½

## ğŸ”¬ æŠ€æœ¯åŸç†

LoRAé€šè¿‡ä½ç§©çŸ©é˜µåˆ†è§£æŠ€æœ¯å°†åŸå§‹æƒé‡çŸ©é˜µåˆ†è§£ä¸ºï¼š

```
W' = W + Î”W = W + A Ã— B Ã— scaling
```

å…¶ä¸­ï¼š
- **W**: å†»ç»“çš„é¢„è®­ç»ƒæƒé‡çŸ©é˜µ (d Ã— k)
- **A**: å¯è®­ç»ƒçš„ä¸‹é™çŸ©é˜µ (d Ã— r)  
- **B**: å¯è®­ç»ƒçš„ä¸Šå‡çŸ©é˜µ (r Ã— k)
- **r**: ä½ç§©å€¼ï¼Œé€šå¸¸ r â‰ª min(d, k)
- **scaling**: ç¼©æ”¾å› å­ = Î± / r

## ğŸ“¦ æ¨¡å—ç»“æ„

```
tinyai-model-lora/
â”œâ”€â”€ src/main/java/io/leavesfly/tinyai/lora/
â”‚   â”œâ”€â”€ LoraConfig.java           # LoRAé…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ LoraAdapter.java          # æ ¸å¿ƒé€‚é…å™¨å®ç°
â”‚   â”œâ”€â”€ LoraLinearLayer.java      # é›†æˆLoRAçš„çº¿æ€§å±‚
â”‚   â”œâ”€â”€ LoraModel.java           # å®Œæ•´LoRAæ¨¡å‹
â”‚   â””â”€â”€ LoraDemo.java            # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ src/test/java/io/leavesfly/tinyai/lora/
â”‚   â”œâ”€â”€ LoraConfigTest.java       # é…ç½®ç±»æµ‹è¯•
â”‚   â”œâ”€â”€ LoraAdapterTest.java      # é€‚é…å™¨æµ‹è¯•
â”‚   â”œâ”€â”€ LoraLinearLayerTest.java  # çº¿æ€§å±‚æµ‹è¯•
â”‚   â””â”€â”€ LoraModelTest.java       # æ¨¡å‹æµ‹è¯•
â”œâ”€â”€ doc/
â”‚   â””â”€â”€ LoRAå®ç°è¯´æ˜.md          # è¯¦ç»†æŠ€æœ¯æ–‡æ¡£
â””â”€â”€ pom.xml                      # Mavené…ç½®
```

## ğŸ§© æ ¸å¿ƒç»„ä»¶

### 1. LoraConfig - é…ç½®ç®¡ç†

ç®¡ç†LoRAçš„æ‰€æœ‰è¶…å‚æ•°å’Œé…ç½®é€‰é¡¹ï¼š

```java
// é¢„è®¾é…ç½®
LoraConfig config = LoraConfig.createMediumRank(); // rank=16, alpha=32

// è‡ªå®šä¹‰é…ç½®
LoraConfig customConfig = new LoraConfig(
    8,      // rank: ä½ç§©çŸ©é˜µçš„ç§©
    16.0,   // alpha: ç¼©æ”¾å‚æ•°
    0.1,    // dropout: æ­£åˆ™åŒ–å‚æ•°
    true,   // enableBias: æ˜¯å¦å¯ç”¨åç½®å¾®è°ƒ
    new String[]{"linear"} // targetModules: ç›®æ ‡æ¨¡å—ç±»å‹
);
```

**é¢„è®¾é…ç½®é€‰é¡¹**ï¼š
- `createLowRank()`: rank=4, alpha=8 (å¿«é€Ÿå®éªŒ)
- `createMediumRank()`: rank=16, alpha=32 (å¸¸è§„å¾®è°ƒ)
- `createHighRank()`: rank=64, alpha=128 (å¤æ‚ä»»åŠ¡)

### 2. LoraAdapter - æ ¸å¿ƒé€‚é…å™¨

å®ç°ä½ç§©çŸ©é˜µåˆ†è§£çš„æ ¸å¿ƒç»„ä»¶ï¼š

```java
// åˆ›å»ºé€‚é…å™¨
LoraAdapter adapter = new LoraAdapter(inputDim, outputDim, config);

// å‰å‘ä¼ æ’­
Variable output = adapter.forward(input);

// æ§åˆ¶å¯ç”¨çŠ¶æ€
adapter.enable();   // å¯ç”¨LoRA
adapter.disable();  // ç¦ç”¨LoRA

// å‚æ•°ç»Ÿè®¡
int paramCount = adapter.getParameterCount();
double reduction = adapter.getParameterReduction(originalParamCount);
```

### 3. LoraLinearLayer - LoRAçº¿æ€§å±‚

é›†æˆäº†LoRAé€‚é…å™¨çš„å®Œæ•´çº¿æ€§å±‚ï¼š

```java
// åˆ›å»ºLoRAçº¿æ€§å±‚
LoraLinearLayer layer = new LoraLinearLayer(
    "lora_layer",    // å±‚åç§°
    512,             // è¾“å…¥ç»´åº¦
    256,             // è¾“å‡ºç»´åº¦
    config,          // LoRAé…ç½®
    true             // æ˜¯å¦åŒ…å«åç½®
);

// å‰å‘ä¼ æ’­
Variable output = layer.layerForward(input);

// æƒé‡ç®¡ç†
layer.freezeOriginalWeights();   // å†»ç»“åŸå§‹æƒé‡
layer.enableLora();              // å¯ç”¨LoRAé€‚é…å™¨
```

### 4. LoraModel - å®Œæ•´LoRAæ¨¡å‹

æä¾›å®Œæ•´çš„å¤šå±‚LoRAæ¨¡å‹å®ç°ï¼š

```java
// åˆ›å»ºLoRAæ¨¡å‹
int[] layerSizes = {784, 256, 128, 10}; // ç½‘ç»œæ¶æ„
LoraModel model = new LoraModel("classifier", layerSizes, config, false);

// æ‰¹é‡æ“ä½œ
model.freezeAllOriginalWeights(); // å†»ç»“æ‰€æœ‰åŸå§‹æƒé‡
model.enableAllLora();            // å¯ç”¨æ‰€æœ‰LoRAé€‚é…å™¨

// æƒé‡åˆå¹¶
List<NdArray> mergedWeights = model.mergeAllLoraWeights();
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

```java
import io.leavesfly.tinyai.lora.*;
import io.leavesfly.tinyai.func.Variable;
import io.leavesfly.tinyai.ndarr.NdArray;
import io.leavesfly.tinyai.ndarr.Shape;

// åˆ›å»ºLoRAé…ç½®
LoraConfig config = LoraConfig.createMediumRank();

// åˆ›å»ºLoRAçº¿æ€§å±‚
LoraLinearLayer layer = new LoraLinearLayer(
    "demo_layer", 256, 128, config, true);

// å‡†å¤‡è¾“å…¥æ•°æ®
NdArray inputData = NdArray.likeRandomN(Shape.of(32, 256)); // batch_size=32
Variable input = new Variable(inputData);

// å‰å‘ä¼ æ’­
Variable output = layer.layerForward(input);

System.out.println("è¾“å…¥å½¢çŠ¶: " + input.getValue().getShape());
System.out.println("è¾“å‡ºå½¢çŠ¶: " + output.getValue().getShape());
System.out.println("å‚æ•°å‡å°‘ç‡: " + layer.getParameterReduction() + "%");
```

### 2. å®Œæ•´æ¨¡å‹å¾®è°ƒ

```java
// å®šä¹‰ç½‘ç»œæ¶æ„ (MNISTåˆ†ç±»å™¨)
int[] layerSizes = {784, 512, 256, 10};
LoraConfig config = LoraConfig.createMediumRank();

// åˆ›å»ºLoRAæ¨¡å‹
LoraModel model = new LoraModel("mnist_classifier", layerSizes, config, false);

// è®¾ç½®è®­ç»ƒæ¨¡å¼ï¼šåªè®­ç»ƒLoRAå‚æ•°
model.freezeAllOriginalWeights();
model.enableAllLora();

// æ¨¡å‹ä¿¡æ¯
System.out.println("æ¨¡å‹ä¿¡æ¯:");
System.out.println(model.getModelInfo());
System.out.println("å¯è®­ç»ƒå‚æ•°: " + model.getTrainableParameterCount());

// æµ‹è¯•å‰å‘ä¼ æ’­
NdArray testInput = NdArray.likeRandomN(Shape.of(64, 784));
Variable output = model.layerForward(new Variable(testInput));
```

### 3. é¢„è®­ç»ƒæ¨¡å‹é€‚é…

```java
// å‡è®¾æœ‰é¢„è®­ç»ƒæƒé‡
List<NdArray> pretrainedWeights = loadPretrainedWeights();
List<NdArray> pretrainedBiases = loadPretrainedBiases();

// ä»é¢„è®­ç»ƒæƒé‡åˆ›å»ºLoRAæ¨¡å‹
LoraModel fineTunedModel = LoraModel.fromPretrained(
    "finetuned_model", 
    pretrainedWeights, 
    pretrainedBiases, 
    config, 
    false
);

// å¾®è°ƒå®Œæˆååˆå¹¶æƒé‡
List<NdArray> finalWeights = fineTunedModel.mergeAllLoraWeights();
```

## ğŸ“Š æ€§èƒ½åˆ†æ

### å‚æ•°æ•ˆç‡å¯¹æ¯”

ä»¥1024Ã—1024çš„å…¨è¿æ¥å±‚ä¸ºä¾‹ï¼š

| Rank | LoRAå‚æ•° | åŸå§‹å‚æ•° | å‚æ•°å‡å°‘ç‡ | è®¡ç®—å¼€é”€æ¯” |
|------|----------|----------|------------|------------|
| 4    | 8,192    | 1,048,576| 99.22%     | 0.0078x    |
| 8    | 16,384   | 1,048,576| 98.44%     | 0.0156x    |
| 16   | 32,768   | 1,048,576| 96.88%     | 0.0312x    |
| 32   | 65,536   | 1,048,576| 93.75%     | 0.0625x    |
| 64   | 131,072  | 1,048,576| 87.50%     | 0.1250x    |

### å†…å­˜ä½¿ç”¨å¯¹æ¯”

```java
// è·å–å‚æ•°ç»Ÿè®¡ä¿¡æ¯
public void analyzeParameters() {
    LoraConfig config = LoraConfig.createMediumRank();
    LoraLinearLayer layer = new LoraLinearLayer("test", 1024, 1024, config, true);
    
    System.out.println("åŸå§‹å‚æ•°æ•°é‡: " + (1024 * 1024));
    System.out.println("LoRAå‚æ•°æ•°é‡: " + layer.getLoraParameterCount());
    System.out.println("å‚æ•°å‡å°‘ç‡: " + layer.getParameterReduction() + "%");
    System.out.println("å†…å­˜èŠ‚çœ: " + layer.getMemorySaving() + " MB");
}
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. åŠ¨æ€LoRAæ§åˆ¶

```java
// è®­ç»ƒæ—¶å¯ç”¨LoRA
model.enableAllLora();
Variable trainOutput = model.layerForward(trainInput);

// æ¨ç†æ—¶å¯é€‰æ‹©ç¦ç”¨LoRAä½¿ç”¨åŸå§‹æƒé‡
model.disableAllLora();
Variable inferOutput = model.layerForward(testInput);

// æ¯”è¾ƒä¸¤ç§æ¨¡å¼çš„è¾“å‡ºå·®å¼‚
NdArray diff = trainOutput.getValue().sub(inferOutput.getValue());
```

### 2. æƒé‡çŠ¶æ€ç®¡ç†

```java
// ä¿å­˜LoRAçŠ¶æ€
Map<String, NdArray> loraState = model.saveLoraState();

// æ¢å¤LoRAçŠ¶æ€
model.loadLoraState(loraState);

// åˆå¹¶æƒé‡ï¼ˆç”¨äºéƒ¨ç½²ï¼‰
List<NdArray> mergedWeights = model.mergeAllLoraWeights();
```

### 3. æ‰¹é‡å‚æ•°æ“ä½œ

```java
// æ‰¹é‡å†»ç»“/è§£å†»
model.freezeAllOriginalWeights();   // å†»ç»“æ‰€æœ‰åŸå§‹æƒé‡
model.unfreezeAllOriginalWeights(); // è§£å†»æ‰€æœ‰åŸå§‹æƒé‡

// æ‰¹é‡å¯ç”¨/ç¦ç”¨LoRA
model.enableAllLora();              // å¯ç”¨æ‰€æœ‰LoRAé€‚é…å™¨
model.disableAllLora();             // ç¦ç”¨æ‰€æœ‰LoRAé€‚é…å™¨
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š

```bash
cd /Users/yefei.yf/Qoder/TinyAI
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home
mvn test -pl tinyai-model-lora
```

è¿è¡Œæ¼”ç¤ºç¨‹åºï¼š

```bash
mvn exec:java -pl tinyai-model-lora -Dexec.mainClass="io.leavesfly.tinyai.lora.LoraDemo"
```

### æµ‹è¯•è¦†ç›–

- âœ… **é…ç½®éªŒè¯**: å‚æ•°èŒƒå›´å’Œåˆç†æ€§æ£€æŸ¥
- âœ… **é€‚é…å™¨åŠŸèƒ½**: å‰å‘ä¼ æ’­å’Œå‚æ•°ç»Ÿè®¡
- âœ… **çº¿æ€§å±‚é›†æˆ**: LoRAä¸æ ‡å‡†çº¿æ€§å±‚çš„é›†æˆ
- âœ… **æ¨¡å‹æ„å»º**: å¤šå±‚LoRAæ¨¡å‹çš„å®Œæ•´åŠŸèƒ½
- âœ… **æƒé‡ç®¡ç†**: å†»ç»“ã€åˆå¹¶ã€çŠ¶æ€ä¿å­˜ç­‰åŠŸèƒ½
- âœ… **æ€§èƒ½éªŒè¯**: å‚æ•°å‡å°‘ç‡å’Œè®¡ç®—æ•ˆç‡

## ğŸ“š ä¾èµ–å…³ç³»

æœ¬æ¨¡å—ä¾èµ–ä»¥ä¸‹TinyAIç»„ä»¶ï¼š

```xml
<dependencies>
    <!-- æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¨¡å— -->
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-ml</artifactId>
    </dependency>
    
    <!-- å¼ºåŒ–å­¦ä¹ æ¨¡å— -->
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-rl</artifactId>
    </dependency>
    
    <!-- GPTæ¨¡å‹æ¨¡å— -->
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-model-gpt</artifactId>
    </dependency>
</dependencies>
```

## ğŸ› ï¸ ç¼–è¯‘ä¸å®‰è£…

### å‰ç½®æ­¥éª¤

ç¡®ä¿æŒ‰ä¾èµ–é¡ºåºç¼–è¯‘ç›¸å…³æ¨¡å—ï¼š

```bash
cd /Users/yefei.yf/Qoder/TinyAI
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home

# æŒ‰é¡ºåºç¼–è¯‘ä¾èµ–æ¨¡å—
mvn install -DskipTests -pl tinyai-deeplearning-ndarr
mvn install -DskipTests -pl tinyai-deeplearning-func  
mvn install -DskipTests -pl tinyai-deeplearning-nnet
mvn install -DskipTests -pl tinyai-deeplearning-ml
mvn install -DskipTests -pl tinyai-deeplearning-rl
mvn install -DskipTests -pl tinyai-model-gpt
```

### ç¼–è¯‘æœ¬æ¨¡å—

```bash
# ç¼–è¯‘LoRAæ¨¡å—
mvn clean compile -pl tinyai-model-lora

# è¿è¡Œæµ‹è¯•
mvn test -pl tinyai-model-lora

# å®‰è£…åˆ°æœ¬åœ°ä»“åº“
mvn install -pl tinyai-model-lora
```

## ğŸ“– æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„Rankå€¼

```java
// å¿«é€Ÿå®éªŒæˆ–èµ„æºå—é™ç¯å¢ƒ
LoraConfig lowConfig = LoraConfig.createLowRank(); // rank=4

// å¤§å¤šæ•°å®é™…åº”ç”¨
LoraConfig mediumConfig = LoraConfig.createMediumRank(); // rank=16

// å¤æ‚ä»»åŠ¡æˆ–å¯¹æ€§èƒ½è¦æ±‚ä¸¥æ ¼
LoraConfig highConfig = LoraConfig.createHighRank(); // rank=64
```

### 2. å¾®è°ƒæµç¨‹å»ºè®®

```java
// 1. åˆ›å»ºæ¨¡å‹å¹¶å†»ç»“åŸå§‹æƒé‡
model.freezeAllOriginalWeights();
model.enableAllLora();

// 2. è®­ç»ƒLoRAå‚æ•°
// ... è®­ç»ƒå¾ªç¯ ...

// 3. è¯„ä¼°æ—¶å¯å°è¯•ç¦ç”¨LoRAå¯¹æ¯”
model.disableAllLora();
// ... è¯„ä¼° ...

// 4. éƒ¨ç½²æ—¶åˆå¹¶æƒé‡ä»¥æé«˜æ¨ç†æ•ˆç‡
List<NdArray> finalWeights = model.mergeAllLoraWeights();
```

### 3. å†…å­˜ä¼˜åŒ–

```java
// å¯¹äºå¤§æ¨¡å‹ï¼Œå¯ä»¥åŠ¨æ€æ§åˆ¶LoRAçŠ¶æ€ä»¥èŠ‚çœå†…å­˜
if (isTraining) {
    model.enableAllLora();
} else {
    model.disableAllLora(); // æ¨ç†æ—¶ç¦ç”¨å¯èŠ‚çœå†…å­˜
}
```

## ğŸ”® æœªæ¥æ‰©å±•

### è®¡åˆ’ç‰¹æ€§

- [ ] **åŠ¨æ€Rankè°ƒæ•´**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è‡ªåŠ¨è°ƒæ•´Rankå€¼
- [ ] **å¤šä»»åŠ¡LoRA**: æ”¯æŒä¸€ä¸ªæ¨¡å‹åŒæ—¶é€‚é…å¤šä¸ªä»»åŠ¡
- [ ] **é‡åŒ–LoRA**: ç»“åˆé‡åŒ–æŠ€æœ¯è¿›ä¸€æ­¥å‡å°‘å†…å­˜å ç”¨
- [ ] **è‡ªé€‚åº”ç¼©æ”¾**: æ™ºèƒ½è°ƒæ•´ç¼©æ”¾å› å­Î±
- [ ] **LoRAèåˆ**: æ”¯æŒå¤šä¸ªLoRAé€‚é…å™¨çš„èåˆ

### æ‰©å±•æ–¹å‘

- **å·ç§¯LoRA**: æ‰©å±•åˆ°å·ç§¯å±‚çš„LoRAé€‚é…
- **æ³¨æ„åŠ›LoRA**: ä¸“é—¨é’ˆå¯¹Transformeræ³¨æ„åŠ›æœºåˆ¶çš„LoRA
- **ç¨€ç–LoRA**: ç»“åˆç¨€ç–æ€§è¿›ä¸€æ­¥æé«˜æ•ˆç‡
- **åˆ†å±‚LoRA**: ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„LoRAé…ç½®

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº [MIT License](../LICENSE) å¼€æºã€‚

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼åœ¨è´¡çŒ®ä»£ç å‰ï¼Œè¯·ç¡®ä¿ï¼š

1. éµå¾ªç°æœ‰çš„ä»£ç é£æ ¼å’Œå‘½åè§„èŒƒ
2. æ·»åŠ é€‚å½“çš„ä¸­æ–‡æ³¨é‡Š
3. ç¼–å†™ç›¸åº”çš„å•å…ƒæµ‹è¯•
4. æ›´æ–°ç›¸å…³æ–‡æ¡£

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰æŠ€æœ¯é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- æäº¤ [GitHub Issue](../../issues)
- æŸ¥çœ‹ [è¯¦ç»†æŠ€æœ¯æ–‡æ¡£](doc/LoRAå®ç°è¯´æ˜.md)
- å‚è€ƒ [é¡¹ç›®æ•´ä½“æ–‡æ¡£](../README.md)

---

**TinyAI LoRAæ¨¡å—** - è®©å¤§æ¨¡å‹å¾®è°ƒå˜å¾—é«˜æ•ˆè€Œç®€å•ï¼ ğŸš€