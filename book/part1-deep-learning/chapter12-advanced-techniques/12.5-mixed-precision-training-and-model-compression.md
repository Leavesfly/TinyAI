# 12.5 混合精度训练与模型压缩

随着深度学习模型规模的不断增大，计算资源和内存需求也急剧增加。混合精度训练和模型压缩技术成为解决这些问题的关键手段。混合精度训练通过使用16位浮点数（FP16）代替32位浮点数（FP32）来减少内存占用和计算时间，而模型压缩则通过量化、剪枝和知识蒸馏等技术减小模型大小并提升推理速度。

## 本节内容概览

- 混合精度训练的原理和实现机制
- 模型量化和剪枝技术
- 知识蒸馏技术详解
- 模型部署优化策略

## 设计思考与技术理念

混合精度训练和模型压缩的核心目标是在保持模型性能的前提下，显著减少计算资源消耗和内存占用。这些技术需要在精度损失和效率提升之间找到最佳平衡点。

在设计这些技术时，我们需要考虑：
1. **数值稳定性**：确保低精度计算不会导致数值溢出或下溢
2. **精度保持**：通过适当的技术保持模型的预测精度
3. **计算效率**：最大化计算资源的利用效率
4. **兼容性**：确保技术能够适配不同的硬件平台

## 12.5.1 混合精度训练原理

混合精度训练结合了FP16和FP32的优势，在保持数值稳定性的同时提升训练效率。

### 混合精度训练的核心组件

1. **主权重（Master Weights）**：使用FP32存储，确保数值精度
2. **前向/反向传播**：使用FP16计算，提升计算效率
3. **损失缩放（Loss Scaling）**：防止梯度下溢

```java
/**
 * 混合精度训练实现
 */
public class MixedPrecisionTraining {
    
    /**
     * FP16和FP32数值表示
     */
    public static class PrecisionTypes {
        public static final int FP32 = 32;  // 32位浮点数
        public static final int FP16 = 16;  // 16位浮点数
        public static final int BF16 = 16;  // Brain Floating Point 16
    }
    
    /**
     * 混合精度训练器
     */
    public static class MixedPrecisionTrainer {
        private float[] masterWeights;     // FP32主权重
        private short[] workingWeights;    // FP16工作权重
        private float lossScale;           // 损失缩放因子
        private float initialLossScale;    // 初始损失缩放因子
        private int lossScaleWindow;       // 损失缩放窗口
        private int lossScaleCounter;      // 损失缩放计数器
        private boolean dynamicLossScaling; // 是否使用动态损失缩放
        
        /**
         * 构造函数
         * @param weightSize 权重大小
         * @param initialLossScale 初始损失缩放因子
         * @param dynamicLossScaling 是否使用动态损失缩放
         */
        public MixedPrecisionTrainer(int weightSize, float initialLossScale, boolean dynamicLossScaling) {
            this.masterWeights = new float[weightSize];
            this.workingWeights = new short[weightSize];
            this.initialLossScale = initialLossScale;
            this.lossScale = initialLossScale;
            this.dynamicLossScaling = dynamicLossScaling;
            this.lossScaleWindow = 1000;
            this.lossScaleCounter = 0;
            
            // 初始化权重
            initializeWeights();
        }
        
        /**
         * 初始化权重
         */
        private void initializeWeights() {
            Random random = new Random(42);
            for (int i = 0; i < masterWeights.length; i++) {
                masterWeights[i] = (random.nextFloat() - 0.5f) * 0.1f;
                workingWeights[i] = floatToHalf(masterWeights[i]);
            }
        }
        
        /**
         * 前向传播（使用FP16）
         * @param inputs 输入数据
         * @return 输出结果
         */
        public float[] forward(float[] inputs) {
            // 将输入转换为FP16
            short[] fp16Inputs = new short[inputs.length];
            for (int i = 0; i < inputs.length; i++) {
                fp16Inputs[i] = floatToHalf(inputs[i]);
            }
            
            // 使用FP16权重进行计算
            short[] fp16Outputs = computeWithFP16(fp16Inputs, workingWeights);
            
            // 将输出转换回FP32
            float[] outputs = new float[fp16Outputs.length];
            for (int i = 0; i < fp16Outputs.length; i++) {
                outputs[i] = halfToFloat(fp16Outputs[i]);
            }
            
            return outputs;
        }
        
        /**
         * 反向传播（使用FP16）
         * @param gradOutputs 输出梯度
         * @return 输入梯度
         */
        public float[] backward(float[] gradOutputs) {
            // 应用损失缩放
            float[] scaledGrads = new float[gradOutputs.length];
            for (int i = 0; i < gradOutputs.length; i++) {
                scaledGrads[i] = gradOutputs[i] * lossScale;
            }
            
            // 将梯度转换为FP16
            short[] fp16Grads = new short[scaledGrads.length];
            for (int i = 0; i < scaledGrads.length; i++) {
                fp16Grads[i] = floatToHalf(scaledGrads[i]);
            }
            
            // 使用FP16进行反向传播计算
            short[] fp16InputGrads = computeBackwardWithFP16(fp16Grads, workingWeights);
            
            // 将输入梯度转换回FP32并应用损失缩放
            float[] inputGrads = new float[fp16InputGrads.length];
            for (int i = 0; i < fp16InputGrads.length; i++) {
                inputGrads[i] = halfToFloat(fp16InputGrads[i]) / lossScale;
            }
            
            return inputGrads;
        }
        
        /**
         * 更新权重
         * @param gradients 梯度
         * @param learningRate 学习率
         */
        public void updateWeights(float[] gradients, float learningRate) {
            // 应用损失缩放到梯度
            float[] scaledGradients = new float[gradients.length];
            for (int i = 0; i < gradients.length; i++) {
                scaledGradients[i] = gradients[i] * lossScale;
            }
            
            // 使用FP32主权重更新
            for (int i = 0; i < masterWeights.length; i++) {
                masterWeights[i] -= learningRate * scaledGradients[i];
            }
            
            // 将FP32权重转换为FP16工作权重
            for (int i = 0; i < workingWeights.length; i++) {
                workingWeights[i] = floatToHalf(masterWeights[i]);
            }
            
            // 动态调整损失缩放
            if (dynamicLossScaling) {
                adjustLossScale(scaledGradients);
            }
        }
        
        /**
         * 动态调整损失缩放
         * @param gradients 梯度
         */
        private void adjustLossScale(float[] gradients) {
            lossScaleCounter++;
            
            // 检查是否有梯度溢出
            boolean hasOverflow = false;
            for (float grad : gradients) {
                if (Float.isInfinite(grad) || Float.isNaN(grad)) {
                    hasOverflow = true;
                    break;
                }
            }
            
            if (hasOverflow) {
                // 发生溢出，减小损失缩放因子
                lossScale /= 2.0f;
                System.out.println("检测到梯度溢出，损失缩放因子调整为: " + lossScale);
                
                // 重置权重到上一次的状态
                for (int i = 0; i < workingWeights.length; i++) {
                    workingWeights[i] = floatToHalf(masterWeights[i]);
                }
            } else if (lossScaleCounter >= lossScaleWindow) {
                // 没有溢出，增加损失缩放因子
                lossScale *= 2.0f;
                lossScale = Math.min(lossScale, initialLossScale * 8.0f); // 设置上限
                lossScaleCounter = 0;
                System.out.println("损失缩放因子调整为: " + lossScale);
            }
        }
        
        /**
         * FP32到FP16转换
         * @param f FP32值
         * @return FP16值
         */
        private short floatToHalf(float f) {
            // 简化的FP32到FP16转换实现
            int bits = Float.floatToIntBits(f);
            int sign = (bits >> 16) & 0x8000;
            int exp = ((bits >> 23) & 0xff) - 127 + 15;
            int mantissa = (bits >> 13) & 0x3ff;
            
            if (exp <= 0) {
                if (exp < -10) {
                    return (short) sign; // 低于下溢范围
                }
                mantissa = (mantissa | 0x400) >> (1 - exp);
                return (short) (sign | mantissa);
            } else if (exp >= 31) {
                return (short) (sign | 0x7c00); // 无穷大或NaN
            }
            
            return (short) (sign | (exp << 10) | mantissa);
        }
        
        /**
         * FP16到FP32转换
         * @param h FP16值
         * @return FP32值
         */
        private float halfToFloat(short h) {
            // 简化的FP16到FP32转换实现
            int sign = (h >> 15) & 0x1;
            int exp = (h >> 10) & 0x1f;
            int mantissa = h & 0x3ff;
            
            if (exp == 0) {
                if (mantissa == 0) {
                    return sign == 0 ? 0.0f : -0.0f;
                } else {
                    // 非规格化数
                    while ((mantissa & 0x400) == 0) {
                        mantissa <<= 1;
                        exp--;
                    }
                    exp++;
                    mantissa &= 0x3ff;
                }
            } else if (exp == 31) {
                if (mantissa == 0) {
                    return sign == 0 ? Float.POSITIVE_INFINITY : Float.NEGATIVE_INFINITY;
                } else {
                    return Float.NaN;
                }
            }
            
            exp += 127 - 15;
            int result = (sign << 31) | (exp << 23) | (mantissa << 13);
            return Float.intBitsToFloat(result);
        }
        
        /**
         * 使用FP16进行前向计算
         */
        private short[] computeWithFP16(short[] inputs, short[] weights) {
            // 简化的线性变换计算
            short[] outputs = new short[Math.min(inputs.length, weights.length)];
            for (int i = 0; i < outputs.length; i++) {
                float input = halfToFloat(inputs[i]);
                float weight = halfToFloat(weights[i]);
                float result = input * weight;
                outputs[i] = floatToHalf(result);
            }
            return outputs;
        }
        
        /**
         * 使用FP16进行反向计算
         */
        private short[] computeBackwardWithFP16(short[] gradOutputs, short[] weights) {
            // 简化的反向传播计算
            short[] inputGrads = new short[gradOutputs.length];
            for (int i = 0; i < inputGrads.length; i++) {
                float grad = halfToFloat(gradOutputs[i]);
                float weight = halfToFloat(weights[i]);
                float result = grad * weight;
                inputGrads[i] = floatToHalf(result);
            }
            return inputGrads;
        }
        
        /**
         * 获取主权重
         */
        public float[] getMasterWeights() {
            return masterWeights.clone();
        }
        
        /**
         * 获取工作权重
         */
        public short[] getWorkingWeights() {
            return workingWeights.clone();
        }
        
        /**
         * 获取当前损失缩放因子
         */
        public float getLossScale() {
            return lossScale;
        }
    }
}
```

## 12.5.2 模型量化技术

模型量化通过减少参数的位宽来压缩模型大小并提升推理速度。

### 量化原理

量化将高精度的浮点数参数映射到低精度的整数表示：

$$Q(x) = round(\frac{x - x_{min}}{x_{max} - x_{min}} \times (2^n - 1))$$

其中 $n$ 是量化位数。

```java
/**
 * 模型量化实现
 */
public class ModelQuantization {
    
    /**
     * 量化参数类
     */
    public static class QuantizationParams {
        private float scale;
        private float zeroPoint;
        private int bitWidth;
        
        public QuantizationParams(float scale, float zeroPoint, int bitWidth) {
            this.scale = scale;
            this.zeroPoint = zeroPoint;
            this.bitWidth = bitWidth;
        }
        
        // Getters
        public float getScale() { return scale; }
        public float getZeroPoint() { return zeroPoint; }
        public int getBitWidth() { return bitWidth; }
    }
    
    /**
     * 对称量化器
     */
    public static class SymmetricQuantizer {
        
        /**
         * 量化浮点数组到整数数组
         * @param floatArray 浮点数组
         * @param bitWidth 量化位宽
         * @return 量化参数和整数数组
         */
        public static QuantizationResult quantize(float[] floatArray, int bitWidth) {
            // 计算范围
            float maxAbs = 0.0f;
            for (float value : floatArray) {
                maxAbs = Math.max(maxAbs, Math.abs(value));
            }
            
            // 计算量化参数
            int qMax = (1 << (bitWidth - 1)) - 1;
            int qMin = -(1 << (bitWidth - 1));
            float scale = maxAbs / qMax;
            float zeroPoint = 0.0f;
            
            QuantizationParams params = new QuantizationParams(scale, zeroPoint, bitWidth);
            
            // 执行量化
            int[] quantizedArray = new int[floatArray.length];
            for (int i = 0; i < floatArray.length; i++) {
                float quantized = floatArray[i] / scale;
                quantizedArray[i] = Math.max(qMin, Math.min(qMax, Math.round(quantized)));
            }
            
            return new QuantizationResult(params, quantizedArray);
        }
        
        /**
         * 反量化整数数组到浮点数组
         * @param quantizedArray 量化数组
         * @param params 量化参数
         * @return 浮点数组
         */
        public static float[] dequantize(int[] quantizedArray, QuantizationParams params) {
            float[] floatArray = new float[quantizedArray.length];
            for (int i = 0; i < quantizedArray.length; i++) {
                floatArray[i] = quantizedArray[i] * params.getScale() + params.getZeroPoint();
            }
            return floatArray;
        }
    }
    
    /**
     * 非对称量化器
     */
    public static class AsymmetricQuantizer {
        
        /**
         * 量化浮点数组到整数数组
         * @param floatArray 浮点数组
         * @param bitWidth 量化位宽
         * @return 量化参数和整数数组
         */
        public static QuantizationResult quantize(float[] floatArray, int bitWidth) {
            // 计算范围
            float min = Float.MAX_VALUE;
            float max = Float.MIN_VALUE;
            for (float value : floatArray) {
                min = Math.min(min, value);
                max = Math.max(max, value);
            }
            
            // 计算量化参数
            int qMax = (1 << bitWidth) - 1;
            int qMin = 0;
            float scale = (max - min) / qMax;
            float zeroPoint = qMin - min / scale;
            
            // 调整零点到整数
            zeroPoint = Math.round(zeroPoint);
            zeroPoint = Math.max(qMin, Math.min(qMax, zeroPoint));
            
            QuantizationParams params = new QuantizationParams(scale, zeroPoint, bitWidth);
            
            // 执行量化
            int[] quantizedArray = new int[floatArray.length];
            for (int i = 0; i < floatArray.length; i++) {
                float quantized = floatArray[i] / scale + zeroPoint;
                quantizedArray[i] = Math.max(qMin, Math.min(qMax, Math.round(quantized)));
            }
            
            return new QuantizationResult(params, quantizedArray);
        }
        
        /**
         * 反量化整数数组到浮点数组
         * @param quantizedArray 量化数组
         * @param params 量化参数
         * @return 浮点数组
         */
        public static float[] dequantize(int[] quantizedArray, QuantizationParams params) {
            float[] floatArray = new float[quantizedArray.length];
            for (int i = 0; i < quantizedArray.length; i++) {
                floatArray[i] = (quantizedArray[i] - params.getZeroPoint()) * params.getScale();
            }
            return floatArray;
        }
    }
    
    /**
     * 量化结果类
     */
    public static class QuantizationResult {
        private QuantizationParams params;
        private int[] quantizedArray;
        
        public QuantizationResult(QuantizationParams params, int[] quantizedArray) {
            this.params = params;
            this.quantizedArray = quantizedArray;
        }
        
        public QuantizationParams getParams() { return params; }
        public int[] getQuantizedArray() { return quantizedArray; }
    }
    
    /**
     * 量化感知训练
     */
    public static class QuantizationAwareTraining {
        private QuantizationParams params;
        private boolean isQuantized;
        
        /**
         * 构造函数
         * @param bitWidth 量化位宽
         */
        public QuantizationAwareTraining(int bitWidth) {
            this.isQuantized = false;
        }
        
        /**
         * 模拟量化操作（在训练中使用）
         * @param weights 权重
         * @param bitWidth 量化位宽
         * @return 量化后的权重
         */
        public float[] simulateQuantization(float[] weights, int bitWidth) {
            // 量化
            QuantizationResult result = SymmetricQuantizer.quantize(weights, bitWidth);
            this.params = result.getParams();
            
            // 反量化
            float[] dequantized = SymmetricQuantizer.dequantize(result.getQuantizedArray(), params);
            
            return dequantized;
        }
        
        /**
         * 获取量化参数
         */
        public QuantizationParams getQuantizationParams() {
            return params;
        }
    }
}
```

## 12.5.3 模型剪枝技术

模型剪枝通过移除不重要的权重或神经元来压缩模型。

```java
/**
 * 模型剪枝实现
 */
public class ModelPruning {
    
    /**
     * 剪枝掩码
     */
    public static class PruningMask {
        private boolean[] mask;
        private int totalElements;
        private int prunedElements;
        
        public PruningMask(int size) {
            this.mask = new boolean[size];
            this.totalElements = size;
            this.prunedElements = 0;
            // 初始化为全部保留
            Arrays.fill(mask, true);
        }
        
        /**
         * 应用剪枝掩码到权重
         * @param weights 权重数组
         * @return 剪枝后的权重
         */
        public float[] applyMask(float[] weights) {
            if (weights.length != mask.length) {
                throw new IllegalArgumentException("权重数组大小与掩码不匹配");
            }
            
            float[] prunedWeights = new float[weights.length];
            for (int i = 0; i < weights.length; i++) {
                prunedWeights[i] = mask[i] ? weights[i] : 0.0f;
            }
            
            return prunedWeights;
        }
        
        /**
         * 基于权重大小的剪枝
         * @param weights 权重数组
         * @param pruneRatio 剪枝比例
         */
        public void pruneByMagnitude(float[] weights, float pruneRatio) {
            if (pruneRatio < 0.0f || pruneRatio > 1.0f) {
                throw new IllegalArgumentException("剪枝比例必须在[0,1]范围内");
            }
            
            int pruneCount = (int) (weights.length * pruneRatio);
            if (pruneCount == 0) return;
            
            // 计算权重绝对值
            float[] absWeights = new float[weights.length];
            for (int i = 0; i < weights.length; i++) {
                absWeights[i] = Math.abs(weights[i]);
            }
            
            // 获取剪枝阈值
            float threshold = getPruningThreshold(absWeights, pruneCount);
            
            // 应用剪枝
            prunedElements = 0;
            for (int i = 0; i < weights.length; i++) {
                if (Math.abs(weights[i]) < threshold) {
                    mask[i] = false;
                    prunedElements++;
                }
            }
            
            System.out.println("剪枝了 " + prunedElements + " 个元素，剪枝比例: " + 
                             String.format("%.2f%%", (float) prunedElements / totalElements * 100));
        }
        
        /**
         * 获取剪枝阈值
         */
        private float getPruningThreshold(float[] absWeights, int pruneCount) {
            float[] sortedWeights = absWeights.clone();
            Arrays.sort(sortedWeights);
            return sortedWeights[pruneCount - 1];
        }
        
        /**
         * 获取剪枝统计信息
         */
        public PruningStats getStats() {
            return new PruningStats(totalElements, prunedElements, 
                                  (float) prunedElements / totalElements);
        }
        
        /**
         * 重置掩码
         */
        public void reset() {
            Arrays.fill(mask, true);
            prunedElements = 0;
        }
    }
    
    /**
     * 剪枝统计信息
     */
    public static class PruningStats {
        private int totalElements;
        private int prunedElements;
        private float pruneRatio;
        
        public PruningStats(int totalElements, int prunedElements, float pruneRatio) {
            this.totalElements = totalElements;
            this.prunedElements = prunedElements;
            this.pruneRatio = pruneRatio;
        }
        
        // Getters
        public int getTotalElements() { return totalElements; }
        public int getPrunedElements() { return prunedElements; }
        public float getPruneRatio() { return pruneRatio; }
        
        @Override
        public String toString() {
            return String.format("PruningStats{total=%d, pruned=%d, ratio=%.2f%%}",
                               totalElements, prunedElements, pruneRatio * 100);
        }
    }
    
    /**
     * 结构化剪枝（通道剪枝）
     */
    public static class StructuredPruning {
        
        /**
         * 对卷积层进行通道剪枝
         * @param weights 卷积权重 (outputChannels x inputChannels x kernelHeight x kernelWidth)
         * @param pruneRatio 剪枝比例
         * @return 剪枝掩码
         */
        public static boolean[] pruneConvChannels(float[][][][] weights, float pruneRatio) {
            int outputChannels = weights.length;
            boolean[] channelMask = new boolean[outputChannels];
            Arrays.fill(channelMask, true);
            
            // 计算每个通道的重要性（L2范数）
            float[] channelImportance = new float[outputChannels];
            for (int outCh = 0; outCh < outputChannels; outCh++) {
                float sumSquares = 0.0f;
                for (int inCh = 0; inCh < weights[outCh].length; inCh++) {
                    for (int h = 0; h < weights[outCh][inCh].length; h++) {
                        for (int w = 0; w < weights[outCh][inCh][h].length; w++) {
                            sumSquares += weights[outCh][inCh][h][w] * weights[outCh][inCh][h][w];
                        }
                    }
                }
                channelImportance[outCh] = (float) Math.sqrt(sumSquares);
            }
            
            // 确定剪枝数量
            int pruneCount = (int) (outputChannels * pruneRatio);
            if (pruneCount == 0) return channelMask;
            
            // 获取剪枝阈值
            float[] sortedImportance = channelImportance.clone();
            Arrays.sort(sortedImportance);
            float threshold = sortedImportance[pruneCount - 1];
            
            // 应用剪枝
            int prunedChannels = 0;
            for (int outCh = 0; outCh < outputChannels; outCh++) {
                if (channelImportance[outCh] < threshold) {
                    channelMask[outCh] = false;
                    prunedChannels++;
                }
            }
            
            System.out.println("剪枝了 " + prunedChannels + " 个通道");
            return channelMask;
        }
    }
}
```

## 12.5.4 知识蒸馏技术

知识蒸馏通过让小模型学习大模型的输出来提升小模型的性能。

```java
/**
 * 知识蒸馏实现
 */
public class KnowledgeDistillation {
    
    /**
     * 教师模型接口
     */
    public interface TeacherModel {
        float[] predict(float[] inputs);
    }
    
    /**
     * 学生模型接口
     */
    public interface StudentModel {
        float[] predict(float[] inputs);
        void train(float[] inputs, float[] targets, float[] teacherOutputs, float temperature);
        float[] getParameters();
    }
    
    /**
     * 知识蒸馏训练器
     */
    public static class DistillationTrainer {
        private TeacherModel teacher;
        private StudentModel student;
        private float temperature;
        private float alpha;
        
        /**
         * 构造函数
         * @param teacher 教师模型
         * @param student 学生模型
         * @param temperature 蒸馏温度
         * @param alpha 损失权重
         */
        public DistillationTrainer(TeacherModel teacher, StudentModel student, 
                                 float temperature, float alpha) {
            this.teacher = teacher;
            this.student = student;
            this.temperature = temperature;
            this.alpha = alpha;
        }
        
        /**
         * 蒸馏训练
         * @param inputs 输入数据
         * @param trueLabels 真实标签
         */
        public void distill(float[][] inputs, float[][] trueLabels) {
            for (int i = 0; i < inputs.length; i++) {
                // 教师模型预测
                float[] teacherOutputs = teacher.predict(inputs[i]);
                
                // 应用温度缩放
                float[] softenedTeacherOutputs = applyTemperature(teacherOutputs, temperature);
                
                // 学生模型训练
                student.train(inputs[i], trueLabels[i], softenedTeacherOutputs, temperature);
            }
        }
        
        /**
         * 应用温度缩放
         */
        private float[] applyTemperature(float[] logits, float temperature) {
            float[] softened = new float[logits.length];
            float sum = 0.0f;
            
            // 计算softmax with temperature
            for (int i = 0; i < logits.length; i++) {
                softened[i] = (float) Math.exp(logits[i] / temperature);
                sum += softened[i];
            }
            
            // 归一化
            for (int i = 0; i < softened.length; i++) {
                softened[i] /= sum;
            }
            
            return softened;
        }
        
        /**
         * 计算蒸馏损失
         * @param studentOutputs 学生模型输出
         * @param teacherOutputs 教师模型输出
         * @param trueLabels 真实标签
         * @param temperature 温度参数
         * @return 总损失
         */
        public float computeDistillationLoss(float[] studentOutputs, float[] teacherOutputs, 
                                           float[] trueLabels, float temperature) {
            // 学生损失（与真实标签）
            float studentLoss = computeCrossEntropyLoss(studentOutputs, trueLabels);
            
            // 蒸馏损失（与教师模型）
            float distillationLoss = computeKLDivergenceLoss(studentOutputs, teacherOutputs, temperature);
            
            // 加权组合
            return alpha * studentLoss + (1 - alpha) * distillationLoss;
        }
        
        /**
         * 计算交叉熵损失
         */
        private float computeCrossEntropyLoss(float[] predictions, float[] targets) {
            float loss = 0.0f;
            for (int i = 0; i < predictions.length; i++) {
                loss -= targets[i] * Math.log(Math.max(predictions[i], 1e-10f));
            }
            return loss;
        }
        
        /**
         * 计算KL散度损失
         */
        private float computeKLDivergenceLoss(float[] studentOutputs, float[] teacherOutputs, 
                                            float temperature) {
            float loss = 0.0f;
            for (int i = 0; i < studentOutputs.length; i++) {
                float studentSoft = (float) Math.exp(studentOutputs[i] / temperature);
                float teacherSoft = (float) Math.exp(teacherOutputs[i] / temperature);
                loss += teacherSoft * Math.log(Math.max(teacherSoft / studentSoft, 1e-10f));
            }
            return loss * temperature * temperature;
        }
    }
    
    /**
     * 简单的教师模型实现
     */
    public static class SimpleTeacherModel implements TeacherModel {
        private float[][] weights;
        private Random random;
        
        public SimpleTeacherModel(int inputSize, int outputSize) {
            this.weights = new float[outputSize][inputSize];
            this.random = new Random(42);
            
            // 初始化权重
            for (int i = 0; i < outputSize; i++) {
                for (int j = 0; j < inputSize; j++) {
                    weights[i][j] = (random.nextFloat() - 0.5f) * 0.1f;
                }
            }
        }
        
        @Override
        public float[] predict(float[] inputs) {
            float[] outputs = new float[weights.length];
            for (int i = 0; i < weights.length; i++) {
                for (int j = 0; j < inputs.length; j++) {
                    outputs[i] += weights[i][j] * inputs[j];
                }
                // 应用激活函数
                outputs[i] = (float) Math.tanh(outputs[i]);
            }
            return outputs;
        }
    }
    
    /**
     * 简单的学生模型实现
     */
    public static class SimpleStudentModel implements StudentModel {
        private float[][] weights;
        private Random random;
        
        public SimpleStudentModel(int inputSize, int outputSize) {
            this.weights = new float[outputSize][inputSize];
            this.random = new Random(42);
            
            // 初始化权重（比教师模型更小）
            for (int i = 0; i < outputSize; i++) {
                for (int j = 0; j < inputSize; j++) {
                    weights[i][j] = (random.nextFloat() - 0.5f) * 0.1f;
                }
            }
        }
        
        @Override
        public float[] predict(float[] inputs) {
            float[] outputs = new float[weights.length];
            for (int i = 0; i < weights.length; i++) {
                for (int j = 0; j < inputs.length; j++) {
                    outputs[i] += weights[i][j] * inputs[j];
                }
                // 应用激活函数
                outputs[i] = (float) Math.tanh(outputs[i]);
            }
            return outputs;
        }
        
        @Override
        public void train(float[] inputs, float[] targets, float[] teacherOutputs, float temperature) {
            // 简化的训练实现
            float learningRate = 0.01f;
            
            // 计算学生模型输出
            float[] studentOutputs = predict(inputs);
            
            // 计算梯度并更新权重
            for (int i = 0; i < weights.length; i++) {
                float error = studentOutputs[i] - targets[i];
                for (int j = 0; j < inputs.length; j++) {
                    weights[i][j] -= learningRate * error * inputs[j];
                }
            }
        }
        
        @Override
        public float[] getParameters() {
            // 简化实现：返回所有权重的扁平化数组
            float[] params = new float[weights.length * weights[0].length];
            int index = 0;
            for (int i = 0; i < weights.length; i++) {
                for (int j = 0; j < weights[i].length; j++) {
                    params[index++] = weights[i][j];
                }
            }
            return params;
        }
    }
}
```

## 12.5.5 模型部署优化

模型部署优化确保压缩后的模型在实际应用中能够高效运行。

```java
/**
 * 模型部署优化工具
 */
public class ModelDeploymentOptimization {
    
    /**
     * 模型优化器
     */
    public static class ModelOptimizer {
        
        /**
         * 优化模型权重
         * @param weights 原始权重
         * @param optimizationType 优化类型
         * @return 优化后的权重
         */
        public static OptimizedWeights optimizeWeights(float[] weights, OptimizationType optimizationType) {
            switch (optimizationType) {
                case QUANTIZATION:
                    return optimizeWithQuantization(weights);
                case PRUNING:
                    return optimizeWithPruning(weights);
                case CLUSTERING:
                    return optimizeWithClustering(weights);
                default:
                    return new OptimizedWeights(weights, new int[0]);
            }
        }
        
        /**
         * 量化优化
         */
        private static OptimizedWeights optimizeWithQuantization(float[] weights) {
            ModelQuantization.QuantizationResult result = 
                ModelQuantization.SymmetricQuantizer.quantize(weights, 8);
            
            return new OptimizedWeights(
                ModelQuantization.SymmetricQuantizer.dequantize(
                    result.getQuantizedArray(), result.getParams()),
                result.getQuantizedArray()
            );
        }
        
        /**
         * 剪枝优化
         */
        private static OptimizedWeights optimizeWithPruning(float[] weights) {
            ModelPruning.PruningMask mask = new ModelPruning.PruningMask(weights.length);
            mask.pruneByMagnitude(weights, 0.5f); // 剪枝50%
            float[] prunedWeights = mask.applyMask(weights);
            
            return new OptimizedWeights(prunedWeights, new int[0]);
        }
        
        /**
         * 聚类优化
         */
        private static OptimizedWeights optimizeWithClustering(float[] weights) {
            // 简化的聚类实现：将相似权重聚类到相同值
            float[] clusteredWeights = weights.clone();
            int[] clusterCenters = new int[Math.min(256, weights.length / 4)];
            
            // 简单的K-means聚类（简化实现）
            Arrays.sort(clusteredWeights);
            for (int i = 0; i < clusterCenters.length; i++) {
                int index = i * clusteredWeights.length / clusterCenters.length;
                clusterCenters[i] = index;
            }
            
            // 将权重分配到最近的聚类中心
            for (int i = 0; i < clusteredWeights.length; i++) {
                float value = clusteredWeights[i];
                float minDistance = Float.MAX_VALUE;
                float closestCenter = value;
                
                for (int centerIndex : clusterCenters) {
                    float centerValue = clusteredWeights[centerIndex];
                    float distance = Math.abs(value - centerValue);
                    if (distance < minDistance) {
                        minDistance = distance;
                        closestCenter = centerValue;
                    }
                }
                
                clusteredWeights[i] = closestCenter;
            }
            
            return new OptimizedWeights(clusteredWeights, clusterCenters);
        }
    }
    
    /**
     * 优化类型枚举
     */
    public enum OptimizationType {
        QUANTIZATION,
        PRUNING,
        CLUSTERING
    }
    
    /**
     * 优化权重类
     */
    public static class OptimizedWeights {
        private float[] weights;
        private int[] metadata;
        
        public OptimizedWeights(float[] weights, int[] metadata) {
            this.weights = weights;
            this.metadata = metadata;
        }
        
        public float[] getWeights() { return weights; }
        public int[] getMetadata() { return metadata; }
    }
    
    /**
     * 模型推理优化器
     */
    public static class InferenceOptimizer {
        
        /**
         * 优化推理性能
         * @param model 模型
         * @param optimizationLevel 优化级别
         * @return 优化后的模型
         */
        public static OptimizedModel optimizeForInference(InferenceModel model, int optimizationLevel) {
            // 根据优化级别应用不同的优化策略
            switch (optimizationLevel) {
                case 1: // 基础优化
                    return applyBasicOptimizations(model);
                case 2: // 中级优化
                    return applyMediumOptimizations(model);
                case 3: // 高级优化
                    return applyAdvancedOptimizations(model);
                default:
                    return new OptimizedModel(model.getWeights(), model.getMetadata());
            }
        }
        
        /**
         * 基础优化
         */
        private static OptimizedModel applyBasicOptimizations(InferenceModel model) {
            // 应用量化
            float[] quantizedWeights = quantizeWeights(model.getWeights(), 8);
            return new OptimizedModel(quantizedWeights, model.getMetadata());
        }
        
        /**
         * 中级优化
         */
        private static OptimizedModel applyMediumOptimizations(InferenceModel model) {
            // 应用量化和剪枝
            float[] quantizedWeights = quantizeWeights(model.getWeights(), 8);
            float[] prunedWeights = pruneWeights(quantizedWeights, 0.3f);
            return new OptimizedModel(prunedWeights, model.getMetadata());
        }
        
        /**
         * 高级优化
         */
        private static OptimizedModel applyAdvancedOptimizations(InferenceModel model) {
            // 应用量化、剪枝和聚类
            float[] quantizedWeights = quantizeWeights(model.getWeights(), 8);
            float[] prunedWeights = pruneWeights(quantizedWeights, 0.3f);
            float[] clusteredWeights = clusterWeights(prunedWeights, 128);
            return new OptimizedModel(clusteredWeights, model.getMetadata());
        }
        
        /**
         * 权重量化
         */
        private static float[] quantizeWeights(float[] weights, int bitWidth) {
            ModelQuantization.QuantizationResult result = 
                ModelQuantization.SymmetricQuantizer.quantize(weights, bitWidth);
            return ModelQuantization.SymmetricQuantizer.dequantize(
                result.getQuantizedArray(), result.getParams());
        }
        
        /**
         * 权重剪枝
         */
        private static float[] pruneWeights(float[] weights, float pruneRatio) {
            ModelPruning.PruningMask mask = new ModelPruning.PruningMask(weights.length);
            mask.pruneByMagnitude(weights, pruneRatio);
            return mask.applyMask(weights);
        }
        
        /**
         * 权重聚类
         */
        private static float[] clusterWeights(float[] weights, int numClusters) {
            // 简化的聚类实现
            float[] clustered = weights.clone();
            float[] uniqueValues = getUniqueValues(clustered, numClusters);
            
            // 将权重映射到最近的聚类中心
            for (int i = 0; i < clustered.length; i++) {
                clustered[i] = findClosestValue(clustered[i], uniqueValues);
            }
            
            return clustered;
        }
        
        /**
         * 获取唯一值（聚类中心）
         */
        private static float[] getUniqueValues(float[] values, int numClusters) {
            Arrays.sort(values);
            float[] unique = new float[numClusters];
            for (int i = 0; i < numClusters; i++) {
                int index = i * values.length / numClusters;
                unique[i] = values[Math.min(index, values.length - 1)];
            }
            return unique;
        }
        
        /**
         * 查找最近的值
         */
        private static float findClosestValue(float value, float[] candidates) {
            float minDistance = Float.MAX_VALUE;
            float closest = value;
            
            for (float candidate : candidates) {
                float distance = Math.abs(value - candidate);
                if (distance < minDistance) {
                    minDistance = distance;
                    closest = candidate;
                }
            }
            
            return closest;
        }
    }
    
    /**
     * 推理模型接口
     */
    public interface InferenceModel {
        float[] getWeights();
        int[] getMetadata();
        float[] predict(float[] inputs);
    }
    
    /**
     * 优化模型类
     */
    public static class OptimizedModel implements InferenceModel {
        private float[] weights;
        private int[] metadata;
        
        public OptimizedModel(float[] weights, int[] metadata) {
            this.weights = weights;
            this.metadata = metadata;
        }
        
        @Override
        public float[] getWeights() { return weights; }
        
        @Override
        public int[] getMetadata() { return metadata; }
        
        @Override
        public float[] predict(float[] inputs) {
            // 简化的预测实现
            float[] outputs = new float[Math.min(inputs.length, weights.length)];
            for (int i = 0; i < outputs.length; i++) {
                outputs[i] = inputs[i] * weights[i];
            }
            return outputs;
        }
    }
}
```

## 综合示例与性能对比

```java
/**
 * 模型优化综合示例
 */
public class ModelOptimizationExample {
    
    public static void main(String[] args) {
        // 演示混合精度训练
        demonstrateMixedPrecisionTraining();
        
        // 演示模型量化
        demonstrateModelQuantization();
        
        // 演示模型剪枝
        demonstrateModelPruning();
        
        // 演示知识蒸馏
        demonstrateKnowledgeDistillation();
        
        // 演示模型部署优化
        demonstrateModelDeploymentOptimization();
    }
    
    /**
     * 演示混合精度训练
     */
    private static void demonstrateMixedPrecisionTraining() {
        System.out.println("=== 混合精度训练演示 ===");
        
        MixedPrecisionTraining.MixedPrecisionTrainer trainer = 
            new MixedPrecisionTraining.MixedPrecisionTrainer(100, 128.0f, true);
        
        // 模拟训练过程
        float[] inputs = new float[10];
        Random random = new Random(42);
        for (int i = 0; i < inputs.length; i++) {
            inputs[i] = random.nextFloat();
        }
        
        for (int step = 0; step < 5; step++) {
            float[] outputs = trainer.forward(inputs);
            float[] gradients = new float[outputs.length];
            for (int i = 0; i < gradients.length; i++) {
                gradients[i] = (outputs[i] - 0.5f) * 2.0f; // 简化的梯度计算
            }
            
            trainer.updateWeights(gradients, 0.01f);
            
            System.out.printf("步骤 %d: 损失缩放因子 = %.2f%n", 
                            step + 1, trainer.getLossScale());
        }
        
        System.out.println();
    }
    
    /**
     * 演示模型量化
     */
    private static void demonstrateModelQuantization() {
        System.out.println("=== 模型量化演示 ===");
        
        // 生成模拟权重
        float[] weights = new float[100];
        Random random = new Random(42);
        for (int i = 0; i < weights.length; i++) {
            weights[i] = (random.nextFloat() - 0.5f) * 2.0f;
        }
        
        System.out.println("原始权重前10个: " + Arrays.toString(Arrays.copyOf(weights, 10)));
        
        // 对称量化
        ModelQuantization.QuantizationResult symmetricResult = 
            ModelQuantization.SymmetricQuantizer.quantize(weights, 8);
        float[] symmetricDequantized = 
            ModelQuantization.SymmetricQuantizer.dequantize(
                symmetricResult.getQuantizedArray(), symmetricResult.getParams());
        
        System.out.println("对称量化后前10个: " + 
                         Arrays.toString(Arrays.copyOf(symmetricDequantized, 10)));
        
        // 非对称量化
        ModelQuantization.QuantizationResult asymmetricResult = 
            ModelQuantization.AsymmetricQuantizer.quantize(weights, 8);
        float[] asymmetricDequantized = 
            ModelQuantization.AsymmetricQuantizer.dequantize(
                asymmetricResult.getQuantizedArray(), asymmetricResult.getParams());
        
        System.out.println("非对称量化后前10个: " + 
                         Arrays.toString(Arrays.copyOf(asymmetricDequantized, 10)));
        
        // 计算量化误差
        float symmetricError = computeMSE(weights, symmetricDequantized);
        float asymmetricError = computeMSE(weights, asymmetricDequantized);
        
        System.out.printf("对称量化MSE: %.6f%n", symmetricError);
        System.out.printf("非对称量化MSE: %.6f%n", asymmetricError);
        System.out.println();
    }
    
    /**
     * 演示模型剪枝
     */
    private static void demonstrateModelPruning() {
        System.out.println("=== 模型剪枝演示 ===");
        
        // 生成模拟权重
        float[] weights = new float[100];
        Random random = new Random(42);
        for (int i = 0; i < weights.length; i++) {
            weights[i] = (random.nextFloat() - 0.5f) * 2.0f;
        }
        
        System.out.println("原始权重前10个: " + Arrays.toString(Arrays.copyOf(weights, 10)));
        
        // 创建剪枝掩码
        ModelPruning.PruningMask mask = new ModelPruning.PruningMask(weights.length);
        
        // 剪枝30%的权重
        mask.pruneByMagnitude(weights, 0.3f);
        
        // 应用剪枝
        float[] prunedWeights = mask.applyMask(weights);
        
        System.out.println("剪枝后前10个: " + Arrays.toString(Arrays.copyOf(prunedWeights, 10)));
        
        // 显示剪枝统计信息
        ModelPruning.PruningStats stats = mask.getStats();
        System.out.println("剪枝统计: " + stats);
        System.out.println();
    }
    
    /**
     * 演示知识蒸馏
     */
    private static void demonstrateKnowledgeDistillation() {
        System.out.println("=== 知识蒸馏演示 ===");
        
        // 创建教师和学生模型
        KnowledgeDistillation.TeacherModel teacher = 
            new KnowledgeDistillation.SimpleTeacherModel(10, 5);
        KnowledgeDistillation.StudentModel student = 
            new KnowledgeDistillation.SimpleStudentModel(10, 5);
        
        // 创建蒸馏训练器
        KnowledgeDistillation.DistillationTrainer trainer = 
            new KnowledgeDistillation.DistillationTrainer(teacher, student, 3.0f, 0.7f);
        
        // 生成模拟数据
        float[][] inputs = new float[5][10];
        float[][] labels = new float[5][5];
        Random random = new Random(42);
        
        for (int i = 0; i < inputs.length; i++) {
            for (int j = 0; j < inputs[i].length; j++) {
                inputs[i][j] = random.nextFloat();
            }
            for (int j = 0; j < labels[i].length; j++) {
                labels[i][j] = random.nextFloat();
            }
        }
        
        // 执行蒸馏训练
        trainer.distill(inputs, labels);
        
        // 比较教师和学生模型的输出
        float[] testInput = new float[10];
        for (int i = 0; i < testInput.length; i++) {
            testInput[i] = random.nextFloat();
        }
        
        float[] teacherOutput = teacher.predict(testInput);
        float[] studentOutput = student.predict(testInput);
        
        System.out.println("教师模型输出: " + Arrays.toString(teacherOutput));
        System.out.println("学生模型输出: " + Arrays.toString(studentOutput));
        System.out.println();
    }
    
    /**
     * 演示模型部署优化
     */
    private static void demonstrateModelDeploymentOptimization() {
        System.out.println("=== 模型部署优化演示 ===");
        
        // 生成模拟权重
        float[] weights = new float[100];
        Random random = new Random(42);
        for (int i = 0; i < weights.length; i++) {
            weights[i] = (random.nextFloat() - 0.5f) * 2.0f;
        }
        
        System.out.println("原始权重前10个: " + Arrays.toString(Arrays.copyOf(weights, 10)));
        
        // 应用不同类型的优化
        ModelDeploymentOptimization.OptimizedWeights quantized = 
            ModelDeploymentOptimization.ModelOptimizer.optimizeWeights(
                weights, ModelDeploymentOptimization.OptimizationType.QUANTIZATION);
        
        ModelDeploymentOptimization.OptimizedWeights pruned = 
            ModelDeploymentOptimization.ModelOptimizer.optimizeWeights(
                weights, ModelDeploymentOptimization.OptimizationType.PRUNING);
        
        System.out.println("量化后前10个: " + 
                         Arrays.toString(Arrays.copyOf(quantized.getWeights(), 10)));
        System.out.println("剪枝后前10个: " + 
                         Arrays.toString(Arrays.copyOf(pruned.getWeights(), 10)));
        
        // 应用推理优化
        ModelDeploymentOptimization.InferenceModel mockModel = 
            new ModelDeploymentOptimization.OptimizedModel(weights, new int[0]);
        
        ModelDeploymentOptimization.OptimizedModel optimizedModel = 
            ModelDeploymentOptimization.InferenceOptimizer.optimizeForInference(mockModel, 3);
        
        System.out.println("高级优化后前10个: " + 
                         Arrays.toString(Arrays.copyOf(optimizedModel.getWeights(), 10)));
        System.out.println();
    }
    
    /**
     * 计算均方误差
     */
    private static float computeMSE(float[] original, float[] reconstructed) {
        if (original.length != reconstructed.length) {
            throw new IllegalArgumentException("数组长度不匹配");
        }
        
        float sumSquaredError = 0.0f;
        for (int i = 0; i < original.length; i++) {
            float error = original[i] - reconstructed[i];
            sumSquaredError += error * error;
        }
        
        return sumSquaredError / original.length;
    }
}
```

## 总结

本节详细介绍了现代深度学习中的重要模型优化技术：

1. **混合精度训练**：
   - 结合FP16和FP32的优势提升训练效率
   - 通过损失缩放解决数值下溢问题
   - 显著减少内存占用和计算时间

2. **模型量化**：
   - **对称量化**：将权重映射到对称的整数范围
   - **非对称量化**：将权重映射到非对称的整数范围
   - **量化感知训练**：在训练过程中模拟量化效果

3. **模型剪枝**：
   - **权重剪枝**：移除不重要的权重参数
   - **结构化剪枝**：移除整个神经元或通道
   - **稀疏化**：创建稀疏模型以提升推理效率

4. **知识蒸馏**：
   - 通过教师-学生框架传递知识
   - 利用温度参数软化输出分布
   - 在保持性能的同时减小模型规模

5. **模型部署优化**：
   - 综合应用多种优化技术
   - 根据部署环境选择合适的优化策略
   - 平衡模型大小、推理速度和精度

这些技术能够显著提升深度学习模型的效率和实用性，在实际应用中应根据具体需求选择合适的优化组合。