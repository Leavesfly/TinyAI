# 13.2 自注意力机制的数学推导

> **设计思想**：深入理解自注意力机制的数学原理，掌握Scaled Dot-Product Attention的实现

## 本节概述

在上一节中，我们从直觉角度理解了注意力机制的基本概念。本节将深入探讨自注意力机制的数学原理，详细推导Scaled Dot-Product Attention的计算过程，并分析其复杂度和优化策略。

自注意力机制是Transformer架构的核心组件，它允许序列中的每个位置关注序列中的所有位置（包括自己），从而捕获序列内部的复杂依赖关系。

## 学习目标

完成本节学习后，你将：

- ✅ **掌握自注意力机制的数学公式**：理解Scaled Dot-Product Attention的数学推导
- ✅ **理解注意力分数的计算方法**：掌握相似度度量的不同方式
- ✅ **掌握Softmax归一化的作用**：理解概率分布的生成过程
- ✅ **分析自注意力的计算复杂度**：理解O(n²)问题及其影响
- ✅ **实现基础的自注意力机制**：能够编写简单的自注意力代码

## Scaled Dot-Product Attention公式推导

### 基本定义

Scaled Dot-Product Attention是Transformer中使用的注意力机制，其数学定义如下：

```
Attention(Q, K, V) = softmax(QK^T/√d_k)V
```

其中：
- Q是Query矩阵，形状为(n, d_k)
- K是Key矩阵，形状为(m, d_k)
- V是Value矩阵，形状为(m, d_v)
- d_k是Key的维度
- n是Query序列的长度
- m是Key/Value序列的长度

### 详细计算步骤

1. **矩阵乘法**：计算QK^T，得到形状为(n, m)的注意力分数矩阵
2. **缩放操作**：将注意力分数除以√d_k，防止softmax函数进入饱和区域
3. **Softmax归一化**：对每一行应用softmax函数，得到注意力权重矩阵
4. **加权求和**：将注意力权重矩阵与V相乘，得到最终的输出

### 为什么需要缩放因子

当d_k较大时，点积QK^T的值也会相应增大，这会导致softmax函数的梯度变得非常小，影响模型的训练效果。通过除以√d_k，可以将点积的方差控制在1左右，使softmax函数工作在梯度较大的区域。

## 注意力分数的计算方法

### 点积注意力

点积注意力是最常用的注意力计算方法，它通过计算Query和Key的点积来衡量它们的相似度：

```
score(q, k) = q · k
```

点积注意力的优点是计算简单高效，但当向量维度较大时需要进行缩放。

### 加性注意力

加性注意力通过一个多层感知机来计算注意力分数：

```
score(q, k) = v^T tanh(W_q q + W_k k)
```

其中W_q、W_k和v是可学习的参数。加性注意力在理论上更强大，但计算复杂度较高。

### 余弦相似度注意力

余弦相似度注意力通过计算Query和Key之间的余弦相似度来衡量它们的相似度：

```
score(q, k) = (q · k) / (||q|| × ||k||)
```

余弦相似度注意力对向量的长度不敏感，只关注方向的一致性。

## Softmax归一化的作用

### 概率分布生成

Softmax函数将任意实数向量转换为概率分布，确保所有注意力权重的和为1：

```
softmax(x_i) = exp(x_i) / Σ_j exp(x_j)
```

### 梯度特性

Softmax函数具有良好的梯度特性，便于反向传播训练。当某个元素的值远大于其他元素时，其对应的概率接近1，其他元素的概率接近0。

### 温度参数

可以通过引入温度参数T来控制Softmax的"锐度"：

```
softmax(x_i/T) = exp(x_i/T) / Σ_j exp(x_j/T)
```

当T较小时，概率分布更加尖锐；当T较大时，概率分布更加平滑。

## 自注意力机制的实现

### 数据结构定义

在实现自注意力机制时，我们需要定义以下数据结构：

```java
public class SelfAttention {
    private int dModel;    // 模型维度
    private int dKey;      // Key维度
    private int dValue;    // Value维度
    
    // 可学习的投影矩阵
    private LinearLayer wQ;  // Query投影矩阵
    private LinearLayer wK;  // Key投影矩阵
    private LinearLayer wV;  // Value投影矩阵
    private LinearLayer wO;  // Output投影矩阵
}
```

### 前向计算过程

```java
public Variable forward(Variable input) {
    // 1. 线性投影
    Variable Q = wQ.forward(input);  // (batch, seq_len, d_model) -> (batch, seq_len, d_key)
    Variable K = wK.forward(input);  // (batch, seq_len, d_model) -> (batch, seq_len, d_key)
    Variable V = wV.forward(input);  // (batch, seq_len, d_model) -> (batch, seq_len, d_value)
    
    // 2. 计算注意力分数
    Variable scores = Q.dot(K.transpose(-2, -1));  // (batch, seq_len, seq_len)
    scores = scores.div(Math.sqrt(dKey));          // 缩放
    
    // 3. Softmax归一化
    Variable attentionWeights = scores.softmax(-1); // (batch, seq_len, seq_len)
    
    // 4. 加权求和
    Variable output = attentionWeights.dot(V);      // (batch, seq_len, d_value)
    
    // 5. 输出投影
    output = wO.forward(output);                   // (batch, seq_len, d_model)
    
    return output;
}
```

## 计算复杂度分析

### 时间复杂度

自注意力机制的时间复杂度为O(n²d)，其中n是序列长度，d是向量维度。主要计算开销来自于：

1. **QK^T计算**：O(n²d)
2. **Softmax计算**：O(n²)
3. **与V相乘**：O(n²d)

### 空间复杂度

自注意力机制的空间复杂度为O(n²)，主要来自于存储注意力权重矩阵。

### O(n²)问题的影响

随着序列长度的增加，自注意力机制的计算和存储开销会急剧增长，这限制了其在长序列任务中的应用。对于长度为1000的序列，需要存储100万个注意力权重；对于长度为10000的序列，则需要存储1亿个注意力权重。

## 优化策略

### 稀疏注意力

稀疏注意力通过只计算部分位置之间的注意力来减少计算量，如局部注意力、全局注意力等。

### 线性注意力

线性注意力通过数学变换将复杂度从O(n²)降低到O(n)，如Performer、Linear Transformer等。

### 内存优化

通过梯度检查点、分块计算等技术来减少内存占用。

## 本节小结

本节深入探讨了自注意力机制的数学原理和实现细节，我们学习了：

1. **Scaled Dot-Product Attention的数学推导**：理解了注意力计算的核心公式
2. **注意力分数的计算方法**：掌握了点积、加性、余弦相似度等不同计算方式
3. **Softmax归一化的作用**：理解了概率分布生成和梯度特性
4. **自注意力机制的实现**：掌握了基础的代码实现方法
5. **计算复杂度分析**：理解了O(n²)问题及其影响

在下一节中，我们将学习多头注意力机制，了解如何通过并行处理不同类型的注意力信息来提升模型性能。