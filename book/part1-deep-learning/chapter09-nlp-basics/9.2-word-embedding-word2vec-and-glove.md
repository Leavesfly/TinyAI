# 9.2 词嵌入技术：Word2Vec与GloVe

> **本节学习目标**：理解词嵌入的数学原理和实现方法，掌握Word2Vec和GloVe两种经典词嵌入模型

## 内容概览

在上一节中，我们学习了如何将文本转换为数值序列。然而，仅仅使用离散的索引无法表达词汇之间的语义关系。词嵌入（Word Embedding）技术通过将词汇映射到连续的向量空间中，使得语义相似的词汇在向量空间中距离较近，从而为NLP任务提供了强大的表示能力。

本节将深入探讨词嵌入的原理，并基于TinyAI框架实现Word2Vec和GloVe两种经典模型。

## 为什么需要词嵌入？

传统的独热编码（One-Hot Encoding）存在以下问题：

1. **维度灾难**：词汇表很大时，向量维度会非常高
2. **缺乏语义信息**：任意两个词汇的向量都是正交的，无法表达相似性
3. **计算效率低**：高维稀疏向量的计算开销大

词嵌入通过学习低维稠密向量来解决这些问题，使得语义相近的词汇在向量空间中距离较近。

## 9.2.1 词嵌入的基本概念

词嵌入是一种将离散的词汇映射到连续向量空间的技术。每个词汇被表示为一个d维实数向量，其中d远小于词汇表大小。

### 数学表示

设词汇表大小为V，词嵌入维度为d，则词嵌入可以表示为：
- 词汇表：V = {w₁, w₂, ..., wᵥ}
- 词嵌入矩阵：E ∈ ℝ^(V×d)
- 词汇wᵢ的嵌入向量：eᵢ ∈ ℝ^d

### 词嵌入的性质

1. **语义相似性**：语义相近的词汇在向量空间中距离较近
2. **类比推理**：可以进行词汇类比运算，如"king - man + woman ≈ queen"
3. **降维表示**：将高维离散空间映射到低维连续空间

## 9.2.2 Word2Vec模型详解

Word2Vec是由Google在2013年提出的词嵌入模型，包含两种架构：
1. **CBOW（Continuous Bag-of-Words）**：根据上下文预测中心词
2. **Skip-gram**：根据中心词预测上下文

### CBOW模型

CBOW模型通过上下文词汇预测中心词汇，其目标函数为：

$$P(w_c|w_{c-m},...,w_{c-1},w_{c+1},...,w_{c+m})$$

其中w_c是中心词，w_{c-m},...,w_{c+m}是上下文词汇。

#### CBOW实现

```java
/**
 * CBOW模型实现
 */
public class CBOWModel {
    private int vocabSize;      // 词汇表大小
    private int embeddingDim;   // 嵌入维度
    private double learningRate; // 学习率
    
    // 输入嵌入矩阵（上下文词向量）
    private double[][] inputEmbeddings;
    // 输出嵌入矩阵（中心词向量）
    private double[][] outputEmbeddings;
    
    /**
     * 构造函数
     */
    public CBOWModel(int vocabSize, int embeddingDim, double learningRate) {
        this.vocabSize = vocabSize;
        this.embeddingDim = embeddingDim;
        this.learningRate = learningRate;
        
        // 初始化嵌入矩阵
        this.inputEmbeddings = new double[vocabSize][embeddingDim];
        this.outputEmbeddings = new double[vocabSize][embeddingDim];
        
        // 随机初始化
        Random random = new Random(42);
        for (int i = 0; i < vocabSize; i++) {
            for (int j = 0; j < embeddingDim; j++) {
                inputEmbeddings[i][j] = (random.nextGaussian() * 0.1);
                outputEmbeddings[i][j] = (random.nextGaussian() * 0.1);
            }
        }
    }
    
    /**
     * 前向传播
     * @param contextWords 上下文词汇索引数组
     * @param targetWord 目标词汇索引
     * @return 预测概率
     */
    public double forwardPass(int[] contextWords, int targetWord) {
        // 1. 计算上下文词向量的平均值
        double[] hidden = new double[embeddingDim];
        for (int contextWord : contextWords) {
            for (int j = 0; j < embeddingDim; j++) {
                hidden[j] += inputEmbeddings[contextWord][j];
            }
        }
        
        // 取平均值
        for (int j = 0; j < embeddingDim; j++) {
            hidden[j] /= contextWords.length;
        }
        
        // 2. 计算输出层得分
        double[] scores = new double[vocabSize];
        double maxScore = Double.NEGATIVE_INFINITY;
        
        // 计算所有词汇的得分（带数值稳定性处理）
        for (int i = 0; i < vocabSize; i++) {
            double score = 0;
            for (int j = 0; j < embeddingDim; j++) {
                score += hidden[j] * outputEmbeddings[i][j];
            }
            scores[i] = score;
            if (score > maxScore) {
                maxScore = score;
            }
        }
        
        // 3. 计算softmax概率
        double sumExp = 0;
        for (int i = 0; i < vocabSize; i++) {
            scores[i] = Math.exp(scores[i] - maxScore);
            sumExp += scores[i];
        }
        
        // 归一化
        for (int i = 0; i < vocabSize; i++) {
            scores[i] /= sumExp;
        }
        
        return scores[targetWord];
    }
    
    /**
     * 反向传播和参数更新
     * @param contextWords 上下文词汇索引数组
     * @param targetWord 目标词汇索引
     */
    public void backwardPass(int[] contextWords, int targetWord) {
        int contextSize = contextWords.length;
        
        // 1. 前向传播计算隐藏层
        double[] hidden = new double[embeddingDim];
        for (int contextWord : contextWords) {
            for (int j = 0; j < embeddingDim; j++) {
                hidden[j] += inputEmbeddings[contextWord][j];
            }
        }
        
        for (int j = 0; j < embeddingDim; j++) {
            hidden[j] /= contextSize;
        }
        
        // 2. 计算输出层得分和概率
        double[] scores = new double[vocabSize];
        double[] probs = new double[vocabSize];
        double maxScore = Double.NEGATIVE_INFINITY;
        
        for (int i = 0; i < vocabSize; i++) {
            double score = 0;
            for (int j = 0; j < embeddingDim; j++) {
                score += hidden[j] * outputEmbeddings[i][j];
            }
            scores[i] = score;
            if (score > maxScore) {
                maxScore = score;
            }
        }
        
        double sumExp = 0;
        for (int i = 0; i < vocabSize; i++) {
            probs[i] = Math.exp(scores[i] - maxScore);
            sumExp += probs[i];
        }
        
        for (int i = 0; i < vocabSize; i++) {
            probs[i] /= sumExp;
        }
        
        // 3. 计算梯度
        // 输出层梯度
        double[] outputGrad = new double[vocabSize];
        System.arraycopy(probs, 0, outputGrad, 0, vocabSize);
        outputGrad[targetWord] -= 1;
        
        // 隐藏层梯度
        double[] hiddenGrad = new double[embeddingDim];
        for (int i = 0; i < vocabSize; i++) {
            for (int j = 0; j < embeddingDim; j++) {
                hiddenGrad[j] += outputGrad[i] * outputEmbeddings[i][j];
            }
        }
        
        // 4. 更新参数
        // 更新输出嵌入
        for (int i = 0; i < vocabSize; i++) {
            for (int j = 0; j < embeddingDim; j++) {
                outputEmbeddings[i][j] -= learningRate * outputGrad[i] * hidden[j];
            }
        }
        
        // 更新输入嵌入
        for (int contextWord : contextWords) {
            for (int j = 0; j < embeddingDim; j++) {
                inputEmbeddings[contextWord][j] -= learningRate * hiddenGrad[j] / contextSize;
            }
        }
    }
    
    /**
     * 获取词汇的嵌入向量
     * @param wordIndex 词汇索引
     * @return 嵌入向量
     */
    public double[] getEmbedding(int wordIndex) {
        return Arrays.copyOf(inputEmbeddings[wordIndex], embeddingDim);
    }
}
```

### Skip-gram模型

Skip-gram模型通过中心词汇预测上下文词汇，其目标函数为：

$$\prod_{-m \leq j \leq m, j \neq 0} P(w_{c+j}|w_c)$$

#### Skip-gram实现

```java
/**
 * Skip-gram模型实现
 */
public class SkipGramModel {
    private int vocabSize;      // 词汇表大小
    private int embeddingDim;   // 嵌入维度
    private double learningRate; // 学习率
    
    // 输入嵌入矩阵（中心词向量）
    private double[][] inputEmbeddings;
    // 输出嵌入矩阵（上下文词向量）
    private double[][] outputEmbeddings;
    
    /**
     * 构造函数
     */
    public SkipGramModel(int vocabSize, int embeddingDim, double learningRate) {
        this.vocabSize = vocabSize;
        this.embeddingDim = embeddingDim;
        this.learningRate = learningRate;
        
        // 初始化嵌入矩阵
        this.inputEmbeddings = new double[vocabSize][embeddingDim];
        this.outputEmbeddings = new double[vocabSize][embeddingDim];
        
        // 随机初始化
        Random random = new Random(42);
        for (int i = 0; i < vocabSize; i++) {
            for (int j = 0; j < embeddingDim; j++) {
                inputEmbeddings[i][j] = (random.nextGaussian() * 0.1);
                outputEmbeddings[i][j] = (random.nextGaussian() * 0.1);
            }
        }
    }
    
    /**
     * 前向传播（单个上下文词）
     * @param centerWord 中心词汇索引
     * @param contextWord 上下文词汇索引
     * @return 预测概率
     */
    public double forwardPass(int centerWord, int contextWord) {
        // 1. 获取中心词向量
        double[] centerVector = inputEmbeddings[centerWord];
        
        // 2. 计算输出层得分
        double[] scores = new double[vocabSize];
        double maxScore = Double.NEGATIVE_INFINITY;
        
        // 计算所有词汇的得分（带数值稳定性处理）
        for (int i = 0; i < vocabSize; i++) {
            double score = 0;
            for (int j = 0; j < embeddingDim; j++) {
                score += centerVector[j] * outputEmbeddings[i][j];
            }
            scores[i] = score;
            if (score > maxScore) {
                maxScore = score;
            }
        }
        
        // 3. 计算softmax概率
        double sumExp = 0;
        for (int i = 0; i < vocabSize; i++) {
            scores[i] = Math.exp(scores[i] - maxScore);
            sumExp += scores[i];
        }
        
        // 归一化
        for (int i = 0; i < vocabSize; i++) {
            scores[i] /= sumExp;
        }
        
        return scores[contextWord];
    }
    
    /**
     * 反向传播和参数更新（单个上下文词）
     * @param centerWord 中心词汇索引
     * @param contextWord 上下文词汇索引
     */
    public void backwardPass(int centerWord, int contextWord) {
        // 1. 获取中心词向量
        double[] centerVector = inputEmbeddings[centerWord];
        
        // 2. 计算输出层得分和概率
        double[] scores = new double[vocabSize];
        double[] probs = new double[vocabSize];
        double maxScore = Double.NEGATIVE_INFINITY;
        
        for (int i = 0; i < vocabSize; i++) {
            double score = 0;
            for (int j = 0; j < embeddingDim; j++) {
                score += centerVector[j] * outputEmbeddings[i][j];
            }
            scores[i] = score;
            if (score > maxScore) {
                maxScore = score;
            }
        }
        
        double sumExp = 0;
        for (int i = 0; i < vocabSize; i++) {
            probs[i] = Math.exp(scores[i] - maxScore);
            sumExp += probs[i];
        }
        
        for (int i = 0; i < vocabSize; i++) {
            probs[i] /= sumExp;
        }
        
        // 3. 计算梯度
        // 输出层梯度
        double[] outputGrad = new double[vocabSize];
        System.arraycopy(probs, 0, outputGrad, 0, vocabSize);
        outputGrad[contextWord] -= 1;
        
        // 中心词向量梯度
        double[] centerGrad = new double[embeddingDim];
        for (int i = 0; i < vocabSize; i++) {
            for (int j = 0; j < embeddingDim; j++) {
                centerGrad[j] += outputGrad[i] * outputEmbeddings[i][j];
            }
        }
        
        // 4. 更新参数
        // 更新输出嵌入
        for (int i = 0; i < vocabSize; i++) {
            for (int j = 0; j < embeddingDim; j++) {
                outputEmbeddings[i][j] -= learningRate * outputGrad[i] * centerVector[j];
            }
        }
        
        // 更新中心词嵌入
        for (int j = 0; j < embeddingDim; j++) {
            inputEmbeddings[centerWord][j] -= learningRate * centerGrad[j];
        }
    }
    
    /**
     * 批量训练（处理一个中心词的所有上下文词）
     * @param centerWord 中心词汇索引
     * @param contextWords 上下文词汇索引数组
     */
    public void trainBatch(int centerWord, int[] contextWords) {
        for (int contextWord : contextWords) {
            // 前向传播
            forwardPass(centerWord, contextWord);
            // 反向传播
            backwardPass(centerWord, contextWord);
        }
    }
    
    /**
     * 获取词汇的嵌入向量
     * @param wordIndex 词汇索引
     * @return 嵌入向量
     */
    public double[] getEmbedding(int wordIndex) {
        return Arrays.copyOf(inputEmbeddings[wordIndex], embeddingDim);
    }
}
```

## 9.2.3 GloVe模型详解

GloVe（Global Vectors for Word Representation）是由斯坦福大学提出的词嵌入模型，它结合了全局矩阵分解和局部上下文窗口的优点。

### GloVe的目标函数

GloVe通过最小化以下目标函数来学习词向量：

$$J = \sum_{i,j=1}^{V} f(X_{ij})(w_i^T\tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2$$

其中：
- Xᵢⱼ是词汇i和词汇j的共现次数
- f是权重函数
- wᵢ是词汇i的词向量
- $\tilde{w}_j$是词汇j的上下文向量
- bᵢ和$\tilde{b}_j$是偏置项

### GloVe实现

```java
/**
 * GloVe模型实现
 */
public class GloveModel {
    private int vocabSize;      // 词汇表大小
    private int embeddingDim;   // 嵌入维度
    private double learningRate; // 学习率
    private double xmax;        // 共现次数阈值
    private double alpha;       // 权重函数参数
    
    // 词向量和上下文向量
    private double[][] wordVectors;
    private double[][] contextVectors;
    // 偏置项
    private double[] wordBiases;
    private double[] contextBiases;
    
    /**
     * 构造函数
     */
    public GloveModel(int vocabSize, int embeddingDim, double learningRate, 
                     double xmax, double alpha) {
        this.vocabSize = vocabSize;
        this.embeddingDim = embeddingDim;
        this.learningRate = learningRate;
        this.xmax = xmax;
        this.alpha = alpha;
        
        // 初始化参数
        this.wordVectors = new double[vocabSize][embeddingDim];
        this.contextVectors = new double[vocabSize][embeddingDim];
        this.wordBiases = new double[vocabSize];
        this.contextBiases = new double[vocabSize];
        
        // 随机初始化
        Random random = new Random(42);
        for (int i = 0; i < vocabSize; i++) {
            for (int j = 0; j < embeddingDim; j++) {
                wordVectors[i][j] = (random.nextGaussian() * 0.1);
                contextVectors[i][j] = (random.nextGaussian() * 0.1);
            }
        }
    }
    
    /**
     * 权重函数
     * @param cooccurrence 共现次数
     * @return 权重值
     */
    private double weightFunction(double cooccurrence) {
        if (cooccurrence == 0) {
            return 0;
        }
        
        if (cooccurrence / xmax < 1.0) {
            return Math.pow(cooccurrence / xmax, alpha);
        } else {
            return 1.0;
        }
    }
    
    /**
     * 计算损失函数值
     * @param cooccurrence 共现次数
     * @param wordIndex 词汇索引
     * @param contextIndex 上下文词汇索引
     * @return 损失值
     */
    public double computeLoss(double cooccurrence, int wordIndex, int contextIndex) {
        if (cooccurrence == 0) {
            return 0;
        }
        
        // 计算点积
        double dotProduct = 0;
        for (int i = 0; i < embeddingDim; i++) {
            dotProduct += wordVectors[wordIndex][i] * contextVectors[contextIndex][i];
        }
        
        // 计算预测值
        double prediction = dotProduct + wordBiases[wordIndex] + contextBiases[contextIndex];
        
        // 计算误差
        double diff = prediction - Math.log(cooccurrence);
        
        // 计算加权损失
        double weight = weightFunction(cooccurrence);
        
        return weight * diff * diff;
    }
    
    /**
     * 计算梯度并更新参数
     * @param cooccurrence 共现次数
     * @param wordIndex 词汇索引
     * @param contextIndex 上下文词汇索引
     */
    public void updateParameters(double cooccurrence, int wordIndex, int contextIndex) {
        if (cooccurrence == 0) {
            return;
        }
        
        // 计算点积
        double dotProduct = 0;
        for (int i = 0; i < embeddingDim; i++) {
            dotProduct += wordVectors[wordIndex][i] * contextVectors[contextIndex][i];
        }
        
        // 计算预测值
        double prediction = dotProduct + wordBiases[wordIndex] + contextBiases[contextIndex];
        
        // 计算误差
        double diff = prediction - Math.log(cooccurrence);
        
        // 计算权重
        double weight = weightFunction(cooccurrence);
        
        // 计算梯度（乘以学习率）
        double gradient = learningRate * weight * diff;
        
        // 更新词向量
        for (int i = 0; i < embeddingDim; i++) {
            double temp = wordVectors[wordIndex][i];
            wordVectors[wordIndex][i] -= gradient * contextVectors[contextIndex][i];
            contextVectors[contextIndex][i] -= gradient * temp;
        }
        
        // 更新偏置项
        wordBiases[wordIndex] -= learningRate * weight * diff;
        contextBiases[contextIndex] -= learningRate * weight * diff;
    }
    
    /**
     * 获取词汇的最终嵌入向量（词向量和上下文向量的平均）
     * @param wordIndex 词汇索引
     * @return 最终嵌入向量
     */
    public double[] getFinalEmbedding(int wordIndex) {
        double[] finalEmbedding = new double[embeddingDim];
        for (int i = 0; i < embeddingDim; i++) {
            finalEmbedding[i] = (wordVectors[wordIndex][i] + contextVectors[wordIndex][i]) / 2.0;
        }
        return finalEmbedding;
    }
}
```

## 9.2.4 词嵌入的训练和使用

让我们通过一个完整的示例来演示如何使用这些词嵌入模型：

```java
/**
 * 词嵌入训练和使用示例
 */
public class WordEmbeddingExample {
    public static void main(String[] args) {
        // 示例词汇表
        String[] vocab = {"the", "quick", "brown", "fox", "jumps", "over", "lazy", "dog"};
        Map<String, Integer> wordToIndex = new HashMap<>();
        for (int i = 0; i < vocab.length; i++) {
            wordToIndex.put(vocab[i], i);
        }
        int vocabSize = vocab.length;
        
        // 示例训练数据（句子）
        String[] sentences = {
            "the quick brown fox jumps over the lazy dog",
            "the brown fox is quick",
            "the dog is lazy"
        };
        
        System.out.println("=== Word2Vec CBOW 示例 ===");
        demonstrateCBOW(wordToIndex, sentences);
        
        System.out.println("\n=== Word2Vec Skip-gram 示例 ===");
        demonstrateSkipGram(wordToIndex, sentences);
        
        System.out.println("\n=== GloVe 示例 ===");
        demonstrateGloVe(wordToIndex, sentences);
    }
    
    /**
     * CBOW模型演示
     */
    private static void demonstrateCBOW(Map<String, Integer> wordToIndex, String[] sentences) {
        int vocabSize = wordToIndex.size();
        int embeddingDim = 10;
        double learningRate = 0.01;
        
        CBOWModel cbow = new CBOWModel(vocabSize, embeddingDim, learningRate);
        
        // 构造训练数据
        List<int[]> trainingData = generateTrainingData(wordToIndex, sentences, 2);
        
        // 训练几个epoch
        int epochs = 100;
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalLoss = 0;
            for (int[] data : trainingData) {
                int targetWord = data[0];
                int[] contextWords = Arrays.copyOfRange(data, 1, data.length);
                
                double prob = cbow.forwardPass(contextWords, targetWord);
                // 简单的损失计算（负对数似然）
                double loss = -Math.log(prob + 1e-10);
                totalLoss += loss;
                
                cbow.backwardPass(contextWords, targetWord);
            }
            
            if (epoch % 20 == 0) {
                System.out.printf("Epoch %d, Average Loss: %.4f%n", epoch, totalLoss / trainingData.size());
            }
        }
        
        // 查看词向量
        System.out.println("词向量示例:");
        for (Map.Entry<String, Integer> entry : wordToIndex.entrySet()) {
            String word = entry.getKey();
            int index = entry.getValue();
            double[] embedding = cbow.getEmbedding(index);
            System.out.printf("%s: %s%n", word, Arrays.toString(Arrays.copyOf(embedding, 3)));
        }
    }
    
    /**
     * Skip-gram模型演示
     */
    private static void demonstrateSkipGram(Map<String, Integer> wordToIndex, String[] sentences) {
        int vocabSize = wordToIndex.size();
        int embeddingDim = 10;
        double learningRate = 0.01;
        
        SkipGramModel skipGram = new SkipGramModel(vocabSize, embeddingDim, learningRate);
        
        // 构造训练数据
        List<int[]> trainingData = generateTrainingData(wordToIndex, sentences, 2);
        
        // 训练几个epoch
        int epochs = 100;
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalLoss = 0;
            for (int[] data : trainingData) {
                int targetWord = data[0];
                int[] contextWords = Arrays.copyOfRange(data, 1, data.length);
                
                // 对每个上下文词进行训练
                for (int contextWord : contextWords) {
                    double prob = skipGram.forwardPass(targetWord, contextWord);
                    double loss = -Math.log(prob + 1e-10);
                    totalLoss += loss;
                    
                    skipGram.backwardPass(targetWord, contextWord);
                }
            }
            
            if (epoch % 20 == 0) {
                System.out.printf("Epoch %d, Average Loss: %.4f%n", epoch, totalLoss / trainingData.size());
            }
        }
        
        // 查看词向量
        System.out.println("词向量示例:");
        for (Map.Entry<String, Integer> entry : wordToIndex.entrySet()) {
            String word = entry.getKey();
            int index = entry.getValue();
            double[] embedding = skipGram.getEmbedding(index);
            System.out.printf("%s: %s%n", word, Arrays.toString(Arrays.copyOf(embedding, 3)));
        }
    }
    
    /**
     * GloVe模型演示
     */
    private static void demonstrateGloVe(Map<String, Integer> wordToIndex, String[] sentences) {
        int vocabSize = wordToIndex.size();
        int embeddingDim = 10;
        double learningRate = 0.01;
        double xmax = 100.0;
        double alpha = 0.75;
        
        GloveModel glove = new GloveModel(vocabSize, embeddingDim, learningRate, xmax, alpha);
        
        // 构造共现矩阵（简化示例）
        double[][] cooccurrenceMatrix = buildCooccurrenceMatrix(wordToIndex, sentences, 2);
        
        // 训练几个epoch
        int epochs = 100;
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalLoss = 0;
            int count = 0;
            
            for (int i = 0; i < vocabSize; i++) {
                for (int j = 0; j < vocabSize; j++) {
                    double cooccur = cooccurrenceMatrix[i][j];
                    if (cooccur > 0) {
                        double loss = glove.computeLoss(cooccur, i, j);
                        totalLoss += loss;
                        count++;
                        
                        glove.updateParameters(cooccur, i, j);
                    }
                }
            }
            
            if (epoch % 20 == 0) {
                System.out.printf("Epoch %d, Average Loss: %.4f%n", epoch, totalLoss / count);
            }
        }
        
        // 查看词向量
        System.out.println("词向量示例:");
        for (Map.Entry<String, Integer> entry : wordToIndex.entrySet()) {
            String word = entry.getKey();
            int index = entry.getValue();
            double[] embedding = glove.getFinalEmbedding(index);
            System.out.printf("%s: %s%n", word, Arrays.toString(Arrays.copyOf(embedding, 3)));
        }
    }
    
    /**
     * 生成训练数据
     */
    private static List<int[]> generateTrainingData(Map<String, Integer> wordToIndex, 
                                                   String[] sentences, int windowSize) {
        List<int[]> trainingData = new ArrayList<>();
        
        for (String sentence : sentences) {
            String[] words = sentence.split(" ");
            int[] wordIndices = new int[words.length];
            
            for (int i = 0; i < words.length; i++) {
                wordIndices[i] = wordToIndex.getOrDefault(words[i], 
                    wordToIndex.get("<UNK>"));
            }
            
            // 生成训练样本
            for (int i = 0; i < wordIndices.length; i++) {
                List<Integer> context = new ArrayList<>();
                
                // 收集上下文词汇
                for (int j = Math.max(0, i - windowSize); 
                     j < Math.min(wordIndices.length, i + windowSize + 1); j++) {
                    if (j != i) {
                        context.add(wordIndices[j]);
                    }
                }
                
                if (!context.isEmpty()) {
                    int[] sample = new int[context.size() + 1];
                    sample[0] = wordIndices[i]; // 目标词
                    for (int k = 0; k < context.size(); k++) {
                        sample[k + 1] = context.get(k);
                    }
                    trainingData.add(sample);
                }
            }
        }
        
        return trainingData;
    }
    
    /**
     * 构建共现矩阵
     */
    private static double[][] buildCooccurrenceMatrix(Map<String, Integer> wordToIndex, 
                                                     String[] sentences, int windowSize) {
        int vocabSize = wordToIndex.size();
        double[][] matrix = new double[vocabSize][vocabSize];
        
        for (String sentence : sentences) {
            String[] words = sentence.split(" ");
            int[] wordIndices = new int[words.length];
            
            for (int i = 0; i < words.length; i++) {
                wordIndices[i] = wordToIndex.getOrDefault(words[i], 
                    wordToIndex.get("<UNK>"));
            }
            
            // 计算共现次数
            for (int i = 0; i < wordIndices.length; i++) {
                int centerWord = wordIndices[i];
                
                for (int j = Math.max(0, i - windowSize); 
                     j < Math.min(wordIndices.length, i + windowSize + 1); j++) {
                    if (j != i) {
                        int contextWord = wordIndices[j];
                        // 使用距离衰减
                        double distanceDecay = 1.0 / Math.abs(i - j);
                        matrix[centerWord][contextWord] += distanceDecay;
                    }
                }
            }
        }
        
        return matrix;
    }
}
```

## 9.2.5 预训练词向量的使用

在实际应用中，我们通常会使用预训练的词向量，而不是从头训练。下面是一个使用预训练词向量的示例：

```java
/**
 * 预训练词向量工具类
 */
public class PretrainedEmbeddings {
    private Map<String, double[]> wordVectors;
    private int dimension;
    
    /**
     * 从文件加载预训练词向量
     * @param filePath 词向量文件路径
     */
    public void loadFromFile(String filePath) throws IOException {
        wordVectors = new HashMap<>();
        
        try (BufferedReader reader = Files.newBufferedReader(Paths.get(filePath))) {
            String line = reader.readLine(); // 第一行通常是元信息
            
            while ((line = reader.readLine()) != null) {
                String[] parts = line.trim().split("\\s+");
                if (parts.length < 2) continue;
                
                String word = parts[0];
                double[] vector = new double[parts.length - 1];
                
                for (int i = 1; i < parts.length; i++) {
                    vector[i - 1] = Double.parseDouble(parts[i]);
                }
                
                wordVectors.put(word, vector);
                dimension = vector.length;
            }
        }
        
        System.out.println("加载了 " + wordVectors.size() + " 个词向量，维度: " + dimension);
    }
    
    /**
     * 获取词汇的向量表示
     * @param word 词汇
     * @return 向量，如果词汇不存在则返回null
     */
    public double[] getVector(String word) {
        return wordVectors.get(word);
    }
    
    /**
     * 计算两个词汇向量之间的余弦相似度
     * @param word1 词汇1
     * @param word2 词汇2
     * @return 余弦相似度
     */
    public double cosineSimilarity(String word1, String word2) {
        double[] vec1 = getVector(word1);
        double[] vec2 = getVector(word2);
        
        if (vec1 == null || vec2 == null) {
            return -1; // 表示无法计算
        }
        
        return cosineSimilarity(vec1, vec2);
    }
    
    /**
     * 计算两个向量之间的余弦相似度
     * @param vec1 向量1
     * @param vec2 向量2
     * @return 余弦相似度
     */
    private double cosineSimilarity(double[] vec1, double[] vec2) {
        double dotProduct = 0.0;
        double norm1 = 0.0;
        double norm2 = 0.0;
        
        for (int i = 0; i < vec1.length; i++) {
            dotProduct += vec1[i] * vec2[i];
            norm1 += vec1[i] * vec1[i];
            norm2 += vec2[i] * vec2[i];
        }
        
        if (norm1 == 0 || norm2 == 0) {
            return 0;
        }
        
        return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
    }
    
    /**
     * 查找与给定词汇最相似的词汇
     * @param word 目标词汇
     * @param topK 返回前K个最相似的词汇
     * @return 最相似的词汇列表
     */
    public List<Map.Entry<String, Double>> findMostSimilar(String word, int topK) {
        double[] targetVector = getVector(word);
        if (targetVector == null) {
            return new ArrayList<>();
        }
        
        PriorityQueue<Map.Entry<String, Double>> similarityQueue = 
            new PriorityQueue<>(Map.Entry.comparingByValue());
        
        for (Map.Entry<String, double[]> entry : wordVectors.entrySet()) {
            String candidateWord = entry.getKey();
            if (candidateWord.equals(word)) continue;
            
            double[] candidateVector = entry.getValue();
            double similarity = cosineSimilarity(targetVector, candidateVector);
            
            if (similarityQueue.size() < topK) {
                similarityQueue.offer(new AbstractMap.SimpleEntry<>(candidateWord, similarity));
            } else if (similarity > similarityQueue.peek().getValue()) {
                similarityQueue.poll();
                similarityQueue.offer(new AbstractMap.SimpleEntry<>(candidateWord, similarity));
            }
        }
        
        List<Map.Entry<String, Double>> result = new ArrayList<>();
        while (!similarityQueue.isEmpty()) {
            result.add(0, similarityQueue.poll()); // 反向添加以获得降序排列
        }
        
        return result;
    }
    
    /**
     * 获取词汇表大小
     */
    public int getVocabSize() {
        return wordVectors.size();
    }
    
    /**
     * 获取向量维度
     */
    public int getDimension() {
        return dimension;
    }
}
```

## 本节小结

在本节中，我们深入学习了词嵌入技术的核心原理和实现方法：

1. **词嵌入基础**：理解了词嵌入的概念、优势以及与传统编码方式的区别
2. **Word2Vec模型**：详细实现了CBOW和Skip-gram两种架构
3. **GloVe模型**：学习了基于全局共现统计的词嵌入方法
4. **实际应用**：演示了如何训练和使用词嵌入模型

通过TinyAI框架的实现，我们不仅掌握了理论知识，还获得了实际的编程经验。词嵌入技术是现代NLP的基础，为后续的深度学习模型提供了强大的语义表示能力。

## 下一步计划

在下一节中，我们将学习循环神经网络（RNN）及其变体LSTM和GRU，这些是处理序列数据的核心技术，在NLP任务中有着广泛应用。