# 10.3 Q-Learningç®—æ³•å®ç°

> **æœ¬èŠ‚å­¦ä¹ ç›®æ ‡**: å®ç°ç»å…¸çš„Q-Learningç®—æ³•å¹¶æ·±å…¥ç†è§£å…¶åŸç†,æŒæ¡Qè¡¨æ›´æ–°ã€Îµ-è´ªå¿ƒç­–ç•¥å’Œå®é™…åº”ç”¨æŠ€å·§

## å†…å®¹æ¦‚è§ˆ

Q-Learningæ˜¯å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„"æ˜æ˜Ÿç®—æ³•",å®ƒè®©è®¡ç®—æœºåƒäººç±»ä¸€æ ·é€šè¿‡è¯•é”™æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚æœ¬èŠ‚æˆ‘ä»¬å°†ç”¨ç”Ÿæ´»åŒ–çš„ä¾‹å­å¸®ä½ ç†è§£Q-Learningçš„ç²¾é«“,å¹¶å®ç°ä¸€ä¸ªå¯ä»¥å®é™…è¿è¡Œçš„Q-Learningç³»ç»Ÿã€‚

## 10.3.1 Q-Learningæ ¸å¿ƒæ€æƒ³

### ä»€ä¹ˆæ˜¯Q-Learning?

æƒ³è±¡ä½ åœ¨å­¦ä¹ æ‰“æ¸¸æˆ:
- **Qå€¼** = åœ¨æŸä¸ªå…³å¡(çŠ¶æ€)é€‰æ‹©æŸä¸ªæ“ä½œ(åŠ¨ä½œ)çš„"é¢„æœŸå¾—åˆ†"
- **Q-Learning** = é€šè¿‡ä¸æ–­å°è¯•,å­¦ä¹ æ¯ä¸ª"çŠ¶æ€-åŠ¨ä½œ"ç»„åˆçš„å¾—åˆ†

```mermaid
graph TB
    subgraph Q-Learningæ ¸å¿ƒæ€æƒ³
        S[å½“å‰çŠ¶æ€<br/>æˆ‘åœ¨ç¬¬3å…³] --> A[é€‰æ‹©åŠ¨ä½œ<br/>å‘å·¦èµ°è¿˜æ˜¯å‘å³?]
        A --> Q[æŸ¥Qè¡¨<br/>å‘å·¦Q=8åˆ†<br/>å‘å³Q=5åˆ†]
        Q --> D[å†³ç­–<br/>é€‰Qå€¼é«˜çš„:å‘å·¦]
        D --> E[æ‰§è¡Œå¹¶è·å¾—åé¦ˆ<br/>å®é™…å¾—åˆ°10åˆ†]
        E --> U[æ›´æ–°Qè¡¨<br/>å‘å·¦çš„Qå€¼â†‘]
    end
    
    style S fill:#e3f2fd
    style A fill:#f3e5f5
    style Q fill:#fff3e0
    style D fill:#e8f5e9
    style E fill:#ffe0b2
    style U fill:#c8e6c9
```

### ç”Ÿæ´»ç±»æ¯”: é¤å…é€‰æ‹©

å‡è®¾ä½ åœ¨å­¦ä¹ é€‰é¤å…:

**Qè¡¨è®°å½•ç»éªŒ**:
```
çŠ¶æ€(è‚šå­é¥¿ç¨‹åº¦) | åŠ¨ä½œ(é¤å…) | Qå€¼(é¢„æœŸæ»¡æ„åº¦)
----------------------------------------------
éå¸¸é¥¿          | å¿«é¤åº—    | 7.5åˆ†
éå¸¸é¥¿          | é«˜æ¡£é¤å…  | 6.0åˆ† (å¤ªæ…¢)
ä¸€èˆ¬é¥¿          | å¿«é¤åº—    | 5.0åˆ†
ä¸€èˆ¬é¥¿          | é«˜æ¡£é¤å…  | 9.0åˆ†
```

**å­¦ä¹ è¿‡ç¨‹**:
1. éå¸¸é¥¿æ—¶å»äº†é«˜æ¡£é¤å…,ç­‰å¤ªä¹…å¾ˆéš¾å— â†’ Qå€¼ä»6.0é™åˆ°5.5
2. ä¸€èˆ¬é¥¿æ—¶å»é«˜æ¡£é¤å…,ä½“éªŒå¾ˆæ£’ â†’ Qå€¼ä»9.0å‡åˆ°9.5
3. é€æ¸å­¦ä¼š"éå¸¸é¥¿é€‰å¿«é¤,æ‚ é—²æ—¶é€‰é«˜æ¡£é¤å…"çš„ç­–ç•¥

### Q-Learningçš„æ ¸å¿ƒå…¬å¼

$$Q(s,a) â† Q(s,a) + Î±[r + Î³ \max_{a'} Q(s',a') - Q(s,a)]$$

**ç™½è¯ç¿»è¯‘**:
```
æ–°Qå€¼ = æ—§Qå€¼ + å­¦ä¹ ç‡ Ã— [å®é™…ä½“éªŒ - æ—§ä¼°è®¡]
                        â””â”€â”€â”€â”€â”€TDè¯¯å·®â”€â”€â”€â”€â”˜

å®é™…ä½“éªŒ = å³æ—¶å¥–åŠ± + æŠ˜æ‰£ Ã— ä¸‹ä¸ªçŠ¶æ€æœ€å¥½çš„Qå€¼
```

### å½¢è±¡ç†è§£æ›´æ–°å…¬å¼

```mermaid
graph LR
    O[æ—§Qå€¼: 7åˆ†] -->|å®é™…ä½“éªŒ| N[æ–°Qå€¼: 7.5åˆ†]
    
    R[å³æ—¶å¥–åŠ±: 5åˆ†] --> E[å®é™…ä½“éªŒ: 5 + 0.9Ã—3 = 7.7åˆ†]
    F[æœªæ¥æœ€ä½³: 3åˆ†] --> E
    
    E --> TD[TDè¯¯å·®: 7.7-7 = 0.7]
    TD --> U[æ›´æ–°: 7 + 0.1Ã—0.7 = 7.07]
    
    style O fill:#ffccbc
    style N fill:#c8e6c9
    style E fill:#fff3e0
    style TD fill:#e1bee7
    style U fill:#bbdefb
```

### Q-Learningçš„ä¸‰å¤§ç‰¹ç‚¹

```mermaid
graph TB
    QL[Q-Learningç‰¹ç‚¹]
    
    QL --> F1[æ— æ¨¡å‹ Model-Free<br/>ä¸éœ€è¦çŸ¥é“ç¯å¢ƒè§„åˆ™]
    QL --> F2[ç¦»çº¿å­¦ä¹  Off-Policy<br/>å­¦ä¹ çš„ç­–ç•¥â‰ è¡ŒåŠ¨çš„ç­–ç•¥]
    QL --> F3[æ”¶æ•›ä¿è¯<br/>æœ€ç»ˆä¼šæ‰¾åˆ°æœ€ä¼˜ç­–ç•¥]
    
    F1 --> E1[å°±åƒå­¦æ¸¸æˆ<br/>ä¸éœ€è¦è¯»è¯´æ˜ä¹¦]
    F2 --> E2[å¯ä»¥çœ‹åˆ«äººç©<br/>ä»ä¸­å­¦ä¹ ]
    F3 --> E3[ç»ƒä¹ è¶³å¤Ÿä¹…<br/>ä¸€å®šèƒ½æˆä¸ºé«˜æ‰‹]
    
    style QL fill:#e1bee7
    style F1 fill:#bbdefb
    style F2 fill:#c5e1a5
    style F3 fill:#ffe0b2
```

## 10.3.2 Qè¡¨: è®°å½•ç»éªŒçš„"å°æœ¬æœ¬"

### Qè¡¨æ˜¯ä»€ä¹ˆ?

Qè¡¨å°±æ˜¯ä¸€ä¸ªäºŒç»´è¡¨æ ¼,è®°å½•äº†æ¯ä¸ª"çŠ¶æ€-åŠ¨ä½œ"ç»„åˆçš„ä»·å€¼è¯„åˆ†ã€‚

**å½¢è±¡æ¯”å–»**: 
- å­¦ç”Ÿçš„**é”™é¢˜æœ¬** â†’ è®°å½•å“ªäº›é¢˜å®¹æ˜“é”™
- Q-Learningçš„**Qè¡¨** â†’ è®°å½•å“ªäº›åŠ¨ä½œä»·å€¼é«˜

### Qè¡¨ç»“æ„

```mermaid
graph TB
    subgraph Qè¡¨ç»“æ„ç¤ºä¾‹
        T[Qè¡¨ = çŠ¶æ€ Ã— åŠ¨ä½œ]
        
        R1[çŠ¶æ€0: 8.5, 3.2, 5.7, 9.1]
        R2[çŠ¶æ€1: 2.3, 7.8, 4.1, 6.5]
        R3[çŠ¶æ€2: 9.2, 1.5, 8.3, 3.9]
        R4[çŠ¶æ€3: 4.7, 8.9, 2.1, 7.6]
        
        T --> R1
        T --> R2
        T --> R3
        T --> R4
        
        C1[åŠ¨ä½œ0] --> R1
        C2[åŠ¨ä½œ1] --> R1
        C3[åŠ¨ä½œ2] --> R1
        C4[åŠ¨ä½œ3] --> R1
    end
    
    style T fill:#e1bee7
    style R1 fill:#e3f2fd
    style R2 fill:#f3e5f5
    style R3 fill:#e8f5e9
    style R4 fill:#fff3e0
```

### ç®€åŒ–å®ç°

```java
/**
 * Qè¡¨ - è®°å½•æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„ä»·å€¼
 */
public class QTable {
    private double[][] qValues; // Q[çŠ¶æ€][åŠ¨ä½œ] = ä»·å€¼
    
    public QTable(int numStates, int numActions) {
        this.qValues = new double[numStates][numActions];
        // åˆå§‹åŒ–ä¸ºå°éšæœºå€¼,æ‰“ç ´å¯¹ç§°æ€§
        initializeRandomly();
    }
    
    /**
     * Q-Learningæ ¸å¿ƒæ›´æ–°
     */
    public void update(int state, int action, double reward, 
                      int nextState, double alpha, double gamma) {
        // å½“å‰Qå€¼
        double currentQ = qValues[state][action];
        
        // ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
        double maxNextQ = getMaxQValue(nextState);
        
        // Q-Learningæ›´æ–°å…¬å¼
        double tdError = reward + gamma * maxNextQ - currentQ;
        qValues[state][action] = currentQ + alpha * tdError;
    }
    
    /**
     * è·å–çŠ¶æ€ä¸‹æœ€å¤§çš„Qå€¼
     */
    public double getMaxQValue(int state) {
        return Arrays.stream(qValues[state]).max().orElse(0.0);
    }
    
    /**
     * è·å–æœ€ä¼˜åŠ¨ä½œ(Qå€¼æœ€å¤§çš„åŠ¨ä½œ)
     */
    public int getBestAction(int state) {
        int bestAction = 0;
        for (int a = 1; a < qValues[state].length; a++) {
            if (qValues[state][a] > qValues[state][bestAction]) {
                bestAction = a;
            }
        }
        return bestAction;
    }
}
```

## 10.3.3 Îµ-è´ªå¿ƒç­–ç•¥: æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡

### ä¸ºä»€ä¹ˆéœ€è¦Îµ-è´ªå¿ƒ?

å¦‚æœåªé€‰Qå€¼æœ€é«˜çš„åŠ¨ä½œ(çº¯è´ªå¿ƒ):
- âŒ å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜
- âŒ é”™è¿‡æ›´å¥½çš„é€‰æ‹©
- âŒ åƒåªåƒä¸€ç§èœçš„ç¾é£Ÿå®¶

```mermaid
graph TB
    D[å†³ç­–å›°å¢ƒ]
    
    D --> G[çº¯è´ªå¿ƒ Greedy<br/>åªé€‰æœ€å¥½çš„]
    D --> R[çº¯éšæœº Random<br/>å…¨é è¿æ°”]
    D --> E[Îµ-è´ªå¿ƒ Îµ-Greedy<br/>å¤§éƒ¨åˆ†è´ªå¿ƒ+å°‘é‡æ¢ç´¢]
    
    G --> GP[é—®é¢˜: å¯èƒ½é”™è¿‡æ›´å¥½é€‰æ‹©]
    R --> RP[é—®é¢˜: æ•ˆç‡å¤ªä½]
    E --> EP[ä¼˜åŠ¿: å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨]
    
    style D fill:#ffebee
    style G fill:#ffccbc
    style R fill:#ffe0b2
    style E fill:#c8e6c9
    style EP fill:#81c784
```

### Îµ-è´ªå¿ƒç­–ç•¥åŸç†

**ç­–ç•¥æè¿°**:
- **Îµæ¦‚ç‡**(å¦‚10%): éšæœºé€‰æ‹©åŠ¨ä½œ â†’ æ¢ç´¢æ–°å¯èƒ½
- **1-Îµæ¦‚ç‡**(å¦‚90%): é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ â†’ åˆ©ç”¨å·²çŸ¥æœ€ä¼˜

**ç”Ÿæ´»ä¾‹å­**: é€‰ç”µå½±
- 90%æ—¶é—´: çœ‹è¯„åˆ†æœ€é«˜çš„ç”µå½±(åˆ©ç”¨)
- 10%æ—¶é—´: éšæœºå°è¯•æ–°ç±»å‹(æ¢ç´¢)

### Îµè¡°å‡ç­–ç•¥

éšç€å­¦ä¹ è¿›å±•,é€æ¸å‡å°‘æ¢ç´¢:

```mermaid
graph LR
    E1[åˆæœŸ<br/>Îµ=1.0<br/>100%æ¢ç´¢] --> E2[å­¦ä¹ ä¸­<br/>Îµ=0.5<br/>50%æ¢ç´¢]
    E2 --> E3[å­¦ä¹ åæœŸ<br/>Îµ=0.1<br/>10%æ¢ç´¢]
    E3 --> E4[æˆç†Ÿé˜¶æ®µ<br/>Îµ=0.01<br/>1%æ¢ç´¢]
    
    style E1 fill:#ffccbc
    style E2 fill:#ffe0b2
    style E3 fill:#fff9c4
    style E4 fill:#c8e6c9
```

**ç±»æ¯”**: 
- æ–°æ‰‹å¸æœº: å¤šå°è¯•ä¸åŒè·¯çº¿(é«˜æ¢ç´¢ç‡)
- è€å¸æœº: å›ºå®šèµ°æœ€ç†Ÿæ‚‰çš„è·¯(ä½æ¢ç´¢ç‡)

### ä»£ç å®ç°

```java
/**
 * Îµ-è´ªå¿ƒç­–ç•¥
 */
public class EpsilonGreedy {
    private double epsilon;       // æ¢ç´¢æ¦‚ç‡
    private double epsilonDecay;  // è¡°å‡ç‡
    private double minEpsilon;    // æœ€å°æ¢ç´¢ç‡
    
    public EpsilonGreedy(double epsilon) {
        this.epsilon = epsilon;
        this.epsilonDecay = 0.995;
        this.minEpsilon = 0.01;
    }
    
    /**
     * é€‰æ‹©åŠ¨ä½œ
     */
    public int selectAction(QTable qTable, int state, int numActions) {
        // Îµæ¦‚ç‡éšæœºæ¢ç´¢
        if (Math.random() < epsilon) {
            return (int)(Math.random() * numActions);
        }
        // 1-Îµæ¦‚ç‡è´ªå¿ƒåˆ©ç”¨
        return qTable.getBestAction(state);
    }
    
    /**
     * è¡°å‡Îµ
     */
    public void decay() {
        epsilon = Math.max(minEpsilon, epsilon * epsilonDecay);
    }
}
```

## 10.3.4 å®Œæ•´Q-Learningç®—æ³•

### ç®—æ³•æµç¨‹å›¾

```mermaid
graph TB
    Start[å¼€å§‹] --> Init[åˆå§‹åŒ–Qè¡¨]
    Init --> Episode[å¼€å§‹æ–°å›åˆ]
    Episode --> Reset[é‡ç½®ç¯å¢ƒ]
    Reset --> State[è§‚å¯ŸçŠ¶æ€s]
    
    State --> Action[Îµ-è´ªå¿ƒé€‰æ‹©åŠ¨ä½œa]
    Action --> Exec[æ‰§è¡ŒåŠ¨ä½œ]
    Exec --> Observe[è§‚å¯Ÿ: å¥–åŠ±r, æ–°çŠ¶æ€s']
    
    Observe --> Update[æ›´æ–°Qè¡¨<br/>Q-Learningå…¬å¼]
    Update --> Check{åˆ°è¾¾<br/>ç»ˆæ­¢çŠ¶æ€?}
    
    Check -->|å¦| State2[s â† s']
    State2 --> State
    
    Check -->|æ˜¯| Decay[è¡°å‡Îµ]
    Decay --> CheckEpisode{å®Œæˆ<br/>è¶³å¤Ÿå›åˆ?}
    
    CheckEpisode -->|å¦| Episode
    CheckEpisode -->|æ˜¯| End[ç»“æŸ]
    
    style Start fill:#e8f5e9
    style Init fill:#e3f2fd
    style Update fill:#fff3e0
    style Decay fill:#f3e5f5
    style End fill:#c8e6c9
```

### ä¼ªä»£ç 

```
ç®—æ³•: Q-Learning

åˆå§‹åŒ–:
  - Qè¡¨æ‰€æœ‰å€¼è®¾ä¸º0(æˆ–å°éšæœºå€¼)
  - è®¾ç½®å­¦ä¹ ç‡Î±, æŠ˜æ‰£å› å­Î³, æ¢ç´¢ç‡Îµ

é‡å¤ (æ¯ä¸ªå›åˆ):
  åˆå§‹åŒ–çŠ¶æ€ s
  
  é‡å¤ (æ¯ä¸ªæ—¶é—´æ­¥):
    ç”¨Îµ-è´ªå¿ƒä»Qè¡¨é€‰æ‹©åŠ¨ä½œ a
    æ‰§è¡ŒåŠ¨ä½œ a, è§‚å¯Ÿå¥–åŠ± r å’Œæ–°çŠ¶æ€ s'
    
    æ›´æ–°Qè¡¨:
      Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
    
    s â† s'
  
  ç›´åˆ° s æ˜¯ç»ˆæ­¢çŠ¶æ€
  
  è¡°å‡ Îµ
  
ç›´åˆ° æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§å›åˆæ•°
```

### å®Œæ•´å®ç°ç¤ºä¾‹

```java
/**
 * Q-Learningå®Œæ•´å®ç°
 */
public class QLearning {
    private QTable qTable;
    private EpsilonGreedy policy;
    private double alpha = 0.1;    // å­¦ä¹ ç‡
    private double gamma = 0.99;   // æŠ˜æ‰£å› å­
    
    /**
     * è®­ç»ƒQ-Learningæ™ºèƒ½ä½“
     */
    public void train(Environment env, int numEpisodes) {
        for (int episode = 0; episode < numEpisodes; episode++) {
            int state = env.reset();
            double totalReward = 0;
            
            while (!env.isTerminal()) {
                // 1. é€‰æ‹©åŠ¨ä½œ
                int action = policy.selectAction(qTable, state, env.getNumActions());
                
                // 2. æ‰§è¡ŒåŠ¨ä½œ
                int nextState = env.step(action);
                double reward = env.getReward();
                
                // 3. æ›´æ–°Qè¡¨
                qTable.update(state, action, reward, nextState, alpha, gamma);
                
                // 4. è½¬ç§»åˆ°æ–°çŠ¶æ€
                state = nextState;
                totalReward += reward;
            }
            
            // 5. è¡°å‡æ¢ç´¢ç‡
            policy.decay();
            
            // æ‰“å°è®­ç»ƒè¿›åº¦
            if (episode % 100 == 0) {
                System.out.printf("Episode %d, Reward: %.2f, Îµ: %.3f%n", 
                    episode, totalReward, policy.getEpsilon());
            }
        }
    }
}
```

## 10.3.5 å‚æ•°è°ƒä¼˜è‰ºæœ¯

### å…³é”®å‚æ•°çš„å½±å“

```mermaid
graph TB
    Params[Q-Learningå‚æ•°]
    
    Params --> Alpha[å­¦ä¹ ç‡ Î±]
    Params --> Gamma[æŠ˜æ‰£å› å­ Î³]
    Params --> Epsilon[æ¢ç´¢ç‡ Îµ]
    
    Alpha --> A1[å¤ªå¤§: ä¸ç¨³å®š, éœ‡è¡]
    Alpha --> A2[å¤ªå°: å­¦ä¹ å¤ªæ…¢]
    Alpha --> A3[æ¨è: 0.01-0.1]
    
    Gamma --> G1[æ¥è¿‘1: é‡è§†é•¿è¿œ]
    Gamma --> G2[æ¥è¿‘0: é‡è§†çœ¼å‰]
    Gamma --> G3[æ¨è: 0.9-0.99]
    
    Epsilon --> E1[åˆå§‹å€¼: 1.0]
    Epsilon --> E2[è¡°å‡ç‡: 0.995]
    Epsilon --> E3[æœ€å°å€¼: 0.01]
    
    style Params fill:#e1bee7
    style Alpha fill:#bbdefb
    style Gamma fill:#c5e1a5
    style Epsilon fill:#ffe0b2
```

### å‚æ•°é€‰æ‹©å»ºè®®

| å‚æ•° | æ¨èèŒƒå›´ | ä½œç”¨ | è°ƒä¼˜æç¤º |
|------|---------|------|----------|
| å­¦ä¹ ç‡Î± | 0.01-0.1 | æ§åˆ¶æ›´æ–°å¹…åº¦ | é—®é¢˜å¤æ‚â†’è°ƒå° |
| æŠ˜æ‰£å› å­Î³ | 0.9-0.99 | æœªæ¥æƒé‡ | é•¿æœŸä»»åŠ¡â†’æ¥è¿‘1 |
| åˆå§‹Îµ | 0.5-1.0 | åˆå§‹æ¢ç´¢ | ç¯å¢ƒå¤æ‚â†’è°ƒå¤§ |
| Îµè¡°å‡ç‡ | 0.99-0.999 | æ¢ç´¢å‡å°‘é€Ÿåº¦ | å­¦ä¹ æ…¢â†’è°ƒå¤§ |
| æœ€å°Îµ | 0.01-0.05 | ä¿æŒæ¢ç´¢ | ä¿æŒå¥½å¥‡å¿ƒ |

## 10.3.6 å®æˆ˜æ¡ˆä¾‹: èµ°è¿·å®«

### é—®é¢˜æè¿°

5Ã—5ç½‘æ ¼è¿·å®«,ä»èµ·ç‚¹èµ°åˆ°ç»ˆç‚¹:

```mermaid
graph TB
    subgraph è¿·å®«å¸ƒå±€
        S[Sèµ·ç‚¹<br/>0,0] --> R1[ ] --> R2[ ] --> R3[ ] --> G[Gç›®æ ‡<br/>4,4]
        S --> W1[â–ˆå¢™] --> R4[ ] --> R5[ ] --> R6[ ]
        W1 --> R7[ ] --> W2[â–ˆå¢™] --> R8[ ] --> R9[ ]
        R7 --> R10[ ] --> R11[ ] --> W3[â–ˆå¢™] --> R12[ ]
        R10 --> R13[ ] --> R14[ ] --> R15[ ] --> G
    end
    
    style S fill:#e3f2fd
    style G fill:#c8e6c9
    style W1 fill:#424242
    style W2 fill:#424242
    style W3 fill:#424242
```

**å¥–åŠ±è®¾è®¡**:
- åˆ°è¾¾ç›®æ ‡: +100
- æ’å¢™: -10
- æ¯æ­¥: -1 (é¼“åŠ±å¿«é€Ÿåˆ°è¾¾)

### å­¦ä¹ è¿‡ç¨‹å¯è§†åŒ–

```mermaid
graph LR
    P1[åˆæœŸ<br/>ä¹±èµ°] --> P2[ä¸­æœŸ<br/>å‘ç°è·¯å¾„]
    P2 --> P3[åæœŸ<br/>ä¼˜åŒ–è·¯å¾„]
    P3 --> P4[æˆç†Ÿ<br/>æœ€ä¼˜è·¯å¾„]
    
    P1 -.å¹³å‡æ­¥æ•°: 200.-> P1
    P2 -.å¹³å‡æ­¥æ•°: 50.-> P2
    P3 -.å¹³å‡æ­¥æ•°: 20.-> P3
    P4 -.å¹³å‡æ­¥æ•°: 8.-> P4
    
    style P1 fill:#ffccbc
    style P2 fill:#ffe0b2
    style P3 fill:#fff9c4
    style P4 fill:#c8e6c9
```

### è®­ç»ƒä»£ç 

```java
/**
 * è¿·å®«Q-Learningç¤ºä¾‹
 */
public class MazeQLearning {
    public static void main(String[] args) {
        // åˆ›å»ºè¿·å®«ç¯å¢ƒ
        MazeEnvironment env = new MazeEnvironment(5, 5);
        env.setStart(0, 0);
        env.setGoal(4, 4);
        env.addWall(1, 0);
        env.addWall(2, 2);
        env.addWall(3, 3);
        
        // åˆ›å»ºQ-Learningæ™ºèƒ½ä½“
        int numStates = 25;  // 5Ã—5ç½‘æ ¼
        int numActions = 4;  // ä¸Šä¸‹å·¦å³
        QTable qTable = new QTable(numStates, numActions);
        EpsilonGreedy policy = new EpsilonGreedy(1.0);
        QLearning agent = new QLearning(qTable, policy);
        
        // è®­ç»ƒ
        System.out.println("å¼€å§‹è®­ç»ƒ...");
        agent.train(env, 1000);
        
        // æµ‹è¯•æœ€ä¼˜ç­–ç•¥
        System.out.println("\næµ‹è¯•å­¦åˆ°çš„ç­–ç•¥:");
        testPolicy(env, qTable);
    }
    
    /**
     * æµ‹è¯•å­¦åˆ°çš„ç­–ç•¥
     */
    private static void testPolicy(MazeEnvironment env, QTable qTable) {
        int state = env.reset();
        int steps = 0;
        
        while (!env.isTerminal() && steps < 100) {
            int action = qTable.getBestAction(state);
            state = env.step(action);
            env.render();  // å¯è§†åŒ–å½“å‰ä½ç½®
            steps++;
        }
        
        System.out.printf("åˆ°è¾¾ç›®æ ‡ç”¨äº† %d æ­¥%n", steps);
    }
}
```

## 10.3.7 Q-Learningçš„ä¼˜åŠ¿ä¸å±€é™

### ä¼˜åŠ¿ âœ…

```mermaid
graph TB
    Adv[Q-Learningä¼˜åŠ¿]
    
    Adv --> A1[ç®€å•ç›´è§‚<br/>æ˜“äºç†è§£å’Œå®ç°]
    Adv --> A2[æ— éœ€æ¨¡å‹<br/>ä¸éœ€è¦çŸ¥é“ç¯å¢ƒåŠ¨åŠ›å­¦]
    Adv --> A3[ç¦»çº¿å­¦ä¹ <br/>å¯ä»ä»»æ„æ•°æ®å­¦ä¹ ]
    Adv --> A4[æ”¶æ•›ä¿è¯<br/>ç†è®ºä¸Šä¿è¯æ”¶æ•›]
    
    style Adv fill:#c8e6c9
    style A1 fill:#81c784
    style A2 fill:#66bb6a
    style A3 fill:#4caf50
    style A4 fill:#43a047
```

### å±€é™ âš ï¸

```mermaid
graph TB
    Lim[Q-Learningå±€é™]
    
    Lim --> L1[ç»´åº¦çˆ†ç‚¸<br/>çŠ¶æ€å¤šæ—¶Qè¡¨å¤ªå¤§]
    Lim --> L2[æ³›åŒ–èƒ½åŠ›å¼±<br/>æœªè§è¿‡çš„çŠ¶æ€æ— æ³•å¤„ç†]
    Lim --> L3[ç¦»æ•£åŠ¨ä½œ<br/>ä¸é€‚åˆè¿ç»­åŠ¨ä½œç©ºé—´]
    Lim --> L4[æ”¶æ•›æ…¢<br/>éœ€è¦å¤§é‡è®­ç»ƒæ ·æœ¬]
    
    style Lim fill:#ffccbc
    style L1 fill:#ff8a65
    style L2 fill:#ff7043
    style L3 fill:#ff5722
    style L4 fill:#f4511e
```

### è§£å†³æ–¹æ¡ˆ

| å±€é™ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| ç»´åº¦çˆ†ç‚¸ | â†’ ä½¿ç”¨æ·±åº¦Qç½‘ç»œ(DQN) |
| æ³›åŒ–èƒ½åŠ›å¼± | â†’ å‡½æ•°è¿‘ä¼¼,ç¥ç»ç½‘ç»œ |
| ç¦»æ•£åŠ¨ä½œ | â†’ ç­–ç•¥æ¢¯åº¦æ–¹æ³• |
| æ”¶æ•›æ…¢ | â†’ ç»éªŒå›æ”¾,ç›®æ ‡ç½‘ç»œ |

**ä¸‹ä¸€èŠ‚é¢„å‘Š**: æˆ‘ä»¬å°†å­¦ä¹ DQN,ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼Qå‡½æ•°,çªç ´Qè¡¨çš„ç»´åº¦é™åˆ¶!

## æœ¬èŠ‚å°ç»“

### çŸ¥è¯†ç»“æ„

```mermaid
graph TB
    Root[Q-Learningç®—æ³•]
    
    Root --> Core[æ ¸å¿ƒæ€æƒ³<br/>å­¦ä¹ Qå€¼è¡¨]
    Root --> Update[æ›´æ–°è§„åˆ™<br/>TDæ–¹æ³•]
    Root --> Explore[æ¢ç´¢ç­–ç•¥<br/>Îµ-è´ªå¿ƒ]
    Root --> Apply[å®é™…åº”ç”¨<br/>è¿·å®«æ±‚è§£]
    
    Core --> C1[Qè¡¨å­˜å‚¨ç»éªŒ]
    Core --> C2[æŸ¥è¡¨å†³ç­–]
    
    Update --> U1[Q-Learningå…¬å¼]
    Update --> U2[åœ¨çº¿æ›´æ–°]
    
    Explore --> E1[Îµæ¦‚ç‡æ¢ç´¢]
    Explore --> E2[1-Îµåˆ©ç”¨]
    
    Apply --> A1[å‚æ•°è°ƒä¼˜]
    Apply --> A2[æ€§èƒ½è¯„ä¼°]
    
    style Root fill:#e1bee7
    style Core fill:#bbdefb
    style Update fill:#c5e1a5
    style Explore fill:#ffe0b2
    style Apply fill:#f8bbd0
```

### æ ¸å¿ƒè¦ç‚¹

1. **Q-Learningæœ¬è´¨**: é€šè¿‡è¯•é”™å­¦ä¹ "çŠ¶æ€-åŠ¨ä½œ"çš„ä»·å€¼è¯„åˆ†
2. **æ›´æ–°æœºåˆ¶**: ç”¨TDæ–¹æ³•åœ¨çº¿æ›´æ–°Qå€¼,æ— éœ€ç­‰å›åˆç»“æŸ
3. **æ¢ç´¢åˆ©ç”¨**: Îµ-è´ªå¿ƒç­–ç•¥å¹³è¡¡æ¢ç´¢æ–°å¯èƒ½å’Œåˆ©ç”¨å·²çŸ¥æœ€ä¼˜
4. **å‚æ•°è°ƒä¼˜**: å­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ã€æ¢ç´¢ç‡éœ€è¦ä»”ç»†è°ƒæ•´
5. **åº”ç”¨åœºæ™¯**: é€‚åˆçŠ¶æ€ç©ºé—´ä¸å¤ªå¤§çš„ç¦»æ•£å†³ç­–é—®é¢˜

### å®è·µå»ºè®®

ğŸ’¡ **å…ˆä»ç®€å•é—®é¢˜å…¥æ‰‹**: å¦‚ç½‘æ ¼ä¸–ç•Œã€äº•å­—æ£‹ç­‰  
ğŸ’¡ **å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹**: è§‚å¯ŸQå€¼å˜åŒ–å’Œç­–ç•¥æ”¹è¿›  
ğŸ’¡ **è°ƒè¯•æŠ€å·§**: æ‰“å°Qè¡¨,æ£€æŸ¥æ˜¯å¦åˆç†æ”¶æ•›  
ğŸ’¡ **æ€§èƒ½ç›‘æ§**: è®°å½•æ¯å›åˆå¥–åŠ±,ç»˜åˆ¶å­¦ä¹ æ›²çº¿

### ç”Ÿæ´»å¯ç¤º

Q-Learningæ•™ä¼šæˆ‘ä»¬:
- ç»éªŒç§¯ç´¯å¾ˆé‡è¦(Qè¡¨)
- è¦æ•¢äºå°è¯•æ–°äº‹ç‰©(æ¢ç´¢)
- ä½†ä¹Ÿè¦å–„ç”¨å·²çŸ¥æœ€ä¼˜æ–¹æ¡ˆ(åˆ©ç”¨)
- ä»é”™è¯¯ä¸­å­¦ä¹ ,ä¸æ–­è°ƒæ•´ç­–ç•¥

ä¸‹ä¸€èŠ‚,æˆ‘ä»¬å°†å­¦ä¹ DQNç®—æ³•,çœ‹çœ‹å¦‚ä½•ç”¨æ·±åº¦å­¦ä¹ çªç ´Q-Learningçš„é™åˆ¶!

---

**ç»ƒä¹ ä»»åŠ¡**:
1. å®ç°ä¸€ä¸ª3Ã—3çš„äº•å­—æ£‹Q-Learningæ™ºèƒ½ä½“
2. è°ƒæ•´ä¸åŒçš„Îµè¡°å‡ç­–ç•¥,è§‚å¯Ÿå¯¹å­¦ä¹ é€Ÿåº¦çš„å½±å“
3. æ€è€ƒ: ä¸ºä»€ä¹ˆQ-Learningè¢«ç§°ä¸º"off-policy"ç®—æ³•?
