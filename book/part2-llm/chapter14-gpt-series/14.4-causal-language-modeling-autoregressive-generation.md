# 14.4 因果语言建模：自回归生成原理

> **设计思想**：深入理解因果语言建模的数学原理和实现方法，掌握自回归生成的核心技术

## 本节概述

因果语言建模（Causal Language Modeling）是GPT系列模型的核心训练目标，它通过自回归的方式预测序列中的下一个token。想象你在玩"文字接龙"游戏——只能看到前面的字,猜下一个字是什么。这就是因果语言建模的核心思想。

与BERT等模型使用的掩码语言建模不同，因果语言建模只使用历史信息来预测未来（像算命先生只能根据过去预测未来），这使得模型能够自然地生成连贯的文本序列。这也是为什么GPT擅长文本生成，而BERT擅长文本理解。

本节将深入探讨因果语言建模的数学原理、实现细节、训练策略以及自回归生成的技术要点。

## 学习目标

完成本节学习后，你将：

- ✅ **掌握因果语言建模的数学原理**：理解自回归生成的概率模型
- ✅ **学会因果掩码的实现**：掌握防止位置关注后续位置的技术
- ✅ **理解训练目标的设计**：掌握最大似然估计在语言建模中的应用
- ✅ **掌握自回归生成过程**：理解逐步生成和概率计算的实现
- ✅ **具备因果语言模型实现能力**：能够编写完整的因果语言模型代码

## 因果语言建模的数学原理："预测下一个字"的学问

### 概率模型：把句子看作"概率链"

**核心思想:**

一个句子的概率,等于每个词依次出现的概率相乘。就像多米诺骨牌,每一块倒下的概率取决于前面所有骨牌的状态。

**数学表达:**

```
P(x_1, x_2, ..., x_T) = Π_{t=1}^T P(x_t | x_1, x_2, ..., x_{t-1})
```

**通俗解释:**

这个公式看起来吓人,其实很简单:
- x_1, x_2, ..., x_T 是一个句子中的词(token)
- P(x_t | x_1, ..., x_{t-1}) 是"在看到前面的词后,下一个词是x_t的概率"
- 整个句子的概率 = 第1个词的概率 × 第2个词的概率 × ... × 最后一个词的概率

**生活类比:**

就像猜成语接龙:
- 第1个成语: "一言九鼎" (任意选,概率是1/所有成语数)
- 第2个成语: 必须以"鼎"开头,比如"鼎鼎大名" (概率是"符合规则的成语数/所有成语数")
- 第3个成语: 必须以"名"开头,比如"名副其实" (继续缩小范围)

每一步的选择都依赖于前面的选择,这就是"因果"和"自回归"的含义。

### 最大似然估计：让模型说"人话"

**训练目标:**

训练语言模型的目标是让它给"正常的句子"打高分,给"奇怪的句子"打低分。用数学说,就是最小化这个损失函数:

```
L = -Σ_{t=1}^T log P(x_t | x_1, x_2, ..., x_{t-1})
```

**直观理解:**

- 对于正确的下一个词,模型应该预测出高概率
- log概率越大(越接近0),损失越小
- 负号的作用:把"最大化概率"变成"最小化损失"

**类比:**

就像训练学生猜成语接龙:
- 猜对了(概率高):表扬(损失小)
- 猜错了(概率低):批评(损失大)
- 通过大量练习,学生越来越会猜了

### 条件概率建模：用神经网络"打分"

在实际实现中,条件概率通过神经网络+softmax函数来计算:

```
P(x_t = v | x_1, ..., x_{t-1}) = softmax(f(x_1, ..., x_{t-1}))_v
```

**这个过程可以分解为三步:**

1. **神经网络处理**:f(x_1, ..., x_{t-1}) 
   - 把前面的词输入神经网络
   - 输出一个向量,每个位置对应一个候选词的"原始分数"

2. **Softmax归一化**:把"原始分数"转换成概率
   - 所有候选词的概率加起来等于1
   - 分数越高的词,概率越大

3. **选择词v的概率**:从概率分布中取出词v的概率

**形象比喻:**

就像选秀节目打分:
1. 评委(神经网络)给每个选手打分(可能是负数、很大的数等)
2. 主持人(Softmax)把分数转换成百分比,让总和=100%
3. 观众看到每个选手获胜的概率

## 因果掩码的实现："戴眼罩"防止作弊

### 掩码设计原理:确保"看不到未来"

**为什么需要因果掩码?**

想象你在做"完形填空",如果能看到后面的答案,那就是作弊了。因果掩码就像给模型戴上"眼罩",确保它在预测第i个词时,绝对看不到第i+1个及之后的词。

**视觉化理解:**

假设有句子"我 爱 吃 苹果":

```
预测"爱"时: 只能看到 "我" ✓
预测"吃"时: 只能看到 "我 爱" ✓
预测"苹果"时: 只能看到 "我 爱 吃" ✓
```

如果没有掩码,模型可能会这样"作弊":

```
预测"爱"时: 偷看到了"吃 苹果" ✗ (作弊!)
```

**掩码矩阵的样子:**

对于4个词的句子,掩码矩阵长这样(√表示允许关注,×表示禁止):

```
     我  爱  吃  苹果
我   √  ×  ×  ×      (预测"我"时只看"我")
爱   √  √  ×  ×      (预测"爱"时只看"我 爱")  
吃   √  √  √  ×      (预测"吃"时只看"我 爱 吃")
苹果 √  √  √  √      (预测"苹果"时看所有前面的词)
```

这是个下三角矩阵,对角线及以下是√(允许),以上是×(禁止)。


# 14.4 因果语言建模：自回归生成原理

> **设计思想**：深入理解因果语言建模的数学原理和实现方法，掌握自回归生成的核心技术

## 本节概述

因果语言建模（Causal Language Modeling）是GPT系列模型的核心训练目标，它通过自回归的方式预测序列中的下一个token。想象你在玩"文字接龙"游戏——只能看到前面的字,猜下一个字是什么。这就是因果语言建模的核心思想。

与BERT等模型使用的掩码语言建模不同，因果语言建模只使用历史信息来预测未来（像算命先生只能根据过去预测未来），这使得模型能够自然地生成连贯的文本序列。这也是为什么GPT擅长文本生成，而BERT擅长文本理解。

本节将深入探讨因果语言建模的数学原理、实现细节、训练策略以及自回归生成的技术要点。

## 学习目标

完成本节学习后，你将：

- ✅ **掌握因果语言建模的数学原理**：理解自回归生成的概率模型
- ✅ **学会因果掩码的实现**：掌握防止位置关注后续位置的技术
- ✅ **理解训练目标的设计**：掌握最大似然估计在语言建模中的应用
- ✅ **掌握自回归生成过程**：理解逐步生成和概率计算的实现
- ✅ **具备因果语言模型实现能力**：能够编写完整的因果语言模型代码

## 因果语言建模的数学原理："预测下一个字"的学问

### 概率模型：把句子看作"概率链"

**核心思想:**

一个句子的概率,等于每个词依次出现的概率相乘。就像多米诺骨牌,每一块倒下的概率取决于前面所有骨牌的状态。

**数学表达:**

```
P(x_1, x_2, ..., x_T) = Π_{t=1}^T P(x_t | x_1, x_2, ..., x_{t-1})
```

**通俗解释:**

这个公式看起来吓人,其实很简单:
- x_1, x_2, ..., x_T 是一个句子中的词(token)
- P(x_t | x_1, ..., x_{t-1}) 是"在看到前面的词后,下一个词是x_t的概率"
- 整个句子的概率 = 第1个词的概率 × 第2个词的概率 × ... × 最后一个词的概率

**生活类比:**

就像猜成语接龙:
- 第1个成语: "一言九鼎" (任意选,概率是1/所有成语数)
- 第2个成语: 必须以"鼎"开头,比如"鼎鼎大名" (概率是"符合规则的成语数/所有成语数")
- 第3个成语: 必须以"名"开头,比如"名副其实" (继续缩小范围)

每一步的选择都依赖于前面的选择,这就是"因果"和"自回归"的含义。

### 最大似然估计：让模型说"人话"

**训练目标:**

训练语言模型的目标是让它给"正常的句子"打高分,给"奇怪的句子"打低分。用数学说,就是最小化这个损失函数:

```
L = -Σ_{t=1}^T log P(x_t | x_1, x_2, ..., x_{t-1})
```

**直观理解:**

- 对于正确的下一个词,模型应该预测出高概率
- log概率越大(越接近0),损失越小
- 负号的作用:把"最大化概率"变成"最小化损失"

**类比:**

就像训练学生猜成语接龙:
- 猜对了(概率高):表扬(损失小)
- 猜错了(概率低):批评(损失大)
- 通过大量练习,学生越来越会猜了

### 条件概率建模：用神经网络"打分"

在实际实现中,条件概率通过神经网络+softmax函数来计算:

```
P(x_t = v | x_1, ..., x_{t-1}) = softmax(f(x_1, ..., x_{t-1}))_v
```

**这个过程可以分解为三步:**

1. **神经网络处理**:f(x_1, ..., x_{t-1}) 
   - 把前面的词输入神经网络
   - 输出一个向量,每个位置对应一个候选词的"原始分数"

2. **Softmax归一化**:把"原始分数"转换成概率
   - 所有候选词的概率加起来等于1
   - 分数越高的词,概率越大

3. **选择词v的概率**:从概率分布中取出词v的概率

**形象比喻:**

就像选秀节目打分:
1. 评委(神经网络)给每个选手打分(可能是负数、很大的数等)
2. 主持人(Softmax)把分数转换成百分比,让总和=100%
3. 观众看到每个选手获胜的概率

## 因果掩码的实现："戴眼罩"防止作弊

### 掩码设计原理:确保"看不到未来"

**为什么需要因果掩码?**

想象你在做"完形填空",如果能看到后面的答案,那就是作弊了。因果掩码就像给模型戴上"眼罩",确保它在预测第i个词时,绝对看不到第i+1个及之后的词。

**视觉化理解:**

假设有句子"我 爱 吃 苹果":

```
预测"爱"时: 只能看到 "我" ✓
预测"吃"时: 只能看到 "我 爱" ✓
预测"苹果"时: 只能看到 "我 爱 吃" ✓
```

如果没有掩码,模型可能会这样"作弊":

```
预测"爱"时: 偷看到了"吃 苹果" ✗ (作弊!)
```

**掩码矩阵的样子:**

对于4个词的句子,掩码矩阵长这样(√表示允许关注,×表示禁止):

```
     我  爱  吃  苹果
我   √  ×  ×  ×      (预测"我"时只看"我")
爱   √  √  ×  ×      (预测"爱"时只看"我 爱")  
吃   √  √  √  ×      (预测"吃"时只看"我 爱 吃")
苹果 √  √  √  √      (预测"苹果"时看所有前面的词)
```

这是个下三角矩阵,对角线及以下是√(允许),以上是×(禁止)。

```java
public class CausalMask {
    public Variable createCausalMask(int seqLength) {
        // 创建上三角矩阵，对角线及以下为0，以上为负无穷
        float[][] mask = new float[seqLength][seqLength];
        
        for (int i = 0; i < seqLength; i++) {
            for (int j = 0; j < seqLength; j++) {
                if (j > i) {
                    mask[i][j] = Float.NEGATIVE_INFINITY;  // 阻止关注未来位置
                } else {
                    mask[i][j] = 0.0f;  // 允许关注当前位置及之前位置
                }
            }
        }
        
        return new Variable(NdArray.of(mask));
    }
    
    public Variable createCausalMaskWithPadding(Variable attentionMask) {
        // attentionMask: (batch, seqLen) - 1表示有效token，0表示填充
        int batchSize = attentionMask.getShape().get(0);
        int seqLength = attentionMask.getShape().get(1);
        
        // 创建因果掩码
        Variable causalMask = createCausalMask(seqLength);
        
        // 扩展维度以匹配注意力权重
        causalMask = causalMask.unsqueeze(0).unsqueeze(0);  // (1, 1, seqLen, seqLen)
        causalMask = causalMask.expand(batchSize, 1, seqLength, seqLength);
        
        // 创建填充掩码
        Variable paddingMask = createPaddingMask(attentionMask);  // (batch, 1, 1, seqLen)
        paddingMask = paddingMask.expand(batchSize, 1, seqLength, seqLength);
        
        // 合并掩码
        return causalMask.add(paddingMask);
    }
    
    private Variable createPaddingMask(Variable attentionMask) {
        // 将0转换为负无穷，1保持为0
        return attentionMask.sub(1.0f).mul(Float.POSITIVE_INFINITY);
    }
}
```

### 掩码在注意力中的应用

```java
public class CausalAttention {
    private MultiHeadAttention attention;
    private CausalMask causalMaskGenerator;
    
    public Variable forward(Variable hiddenStates, Variable attentionMask) {
        // 创建因果掩码
        Variable causalMask = causalMaskGenerator
            .createCausalMaskWithPadding(attentionMask);
        
        // 应用注意力机制
        return attention.forward(
            hiddenStates,  // Query
            hiddenStates,  // Key
            hiddenStates,  // Value
            causalMask     // 因果掩码
        );
    }
}
```

## 训练目标实现:教模型"猜词"

### 损失函数设计:衡量"猜得准不准"

训练语言模型就像训练学生猜谜语,我们需要一个"评分标准"来判断猜得好不好。

**基础版本:交叉熵损失**

交叉熵损失衡量"预测的概率分布"和"真实答案"之间的差距:
- 如果模型给正确答案的概率是0.9,损失很小 ✓
- 如果模型给正确答案的概率是0.1,损失很大 ✗

```java
public class CausalLanguageModelingLoss {
    private boolean useLabelSmoothing;
    private double smoothingFactor;
    
    public Variable computeLoss(Variable logits, Variable labels) {
        // logits: (batch, seqLen, vocabSize)
        // labels: (batch, seqLen)
        
        int batchSize = logits.getShape().get(0);
        int seqLength = logits.getShape().get(1);
        int vocabSize = logits.getShape().get(2);
        
        // 重塑为二维矩阵
        Variable reshapedLogits = logits.reshape(-1, vocabSize);  // (batch*seqLen, vocabSize)
        Variable reshapedLabels = labels.reshape(-1);             // (batch*seqLen)
        
        if (useLabelSmoothing) {
            return computeLabelSmoothedLoss(reshapedLogits, reshapedLabels);
        } else {
            return computeCrossEntropyLoss(reshapedLogits, reshapedLabels);
        }
    }
    
    private Variable computeCrossEntropyLoss(Variable logits, Variable labels) {
        // 计算交叉熵损失
        return crossEntropyLoss(logits, labels);
    }
    
    private Variable computeLabelSmoothedLoss(Variable logits, Variable labels) {
        // 实现标签平滑
        int vocabSize = logits.getShape().get(-1);
        Variable logProbs = logits.logSoftmax(-1);
        
        // 创建平滑标签
        Variable oneHot = oneHot(labels, vocabSize);
        Variable smoothLabels = oneHot.mul(1.0 - smoothingFactor)
                                  .add(smoothingFactor / vocabSize);
        
        // 计算平滑损失
        Variable loss = logProbs.mul(smoothLabels).sum(-1).neg();
        return loss.mean();
    }
}
```

**进阶版本:标签平滑**

有时候"标准答案"也不是唯一的。比如:
- "今天天气真____", 答案可以是"好"、"棒"、"不错"等

标签平滑的思想是:不要100%确定某个答案,给其他合理答案也留点概率。

```

```

### 训练循环:反复练习,越来越准

训练过程就像让学生反复做练习题:

1. **出题**(准备数据): 给一批句子,遮住后面的词
2. **学生答题**(前向传播): 模型根据前面的词,预测下一个词
3. **老师打分**(计算损失): 看预测的概率分布和正确答案差多少
4. **学生改进**(反向传播): 根据错误调整"思维方式"(参数)
5. **重复练习**(迭代): 做成千上万道题,越做越好

```java
public class CausalLanguageModelTrainer {
    private CausalLanguageModel model;
    private CausalLanguageModelingLoss lossFunction;
    private Optimizer optimizer;
    private DataLoader dataLoader;
    
    public void train(int epochs) {
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalLoss = 0.0;
            int batchCount = 0;
            
            for (Batch batch : dataLoader) {
                // 获取输入和标签
                Variable inputIds = batch.getInputIds();      // (batch, seqLen)
                Variable labels = batch.getLabels();          // (batch, seqLen)
                Variable attentionMask = batch.getAttentionMask(); // (batch, seqLen)
                
                // 前向传播
                Variable logits = model.forward(inputIds, attentionMask);
                
                // 计算损失
                Variable loss = lossFunction.computeLoss(logits, labels);
                
                // 反向传播
                loss.backward();
                
                // 梯度裁剪
                clipGradients(model.getParameters(), 1.0);
                
                // 优化步骤
                optimizer.step();
                optimizer.zeroGrad();
                
                totalLoss += loss.getData().getFloat();
                batchCount++;
                
                // 定期记录日志
                if (batchCount % 100 == 0) {
                    System.out.printf("Epoch %d, Batch %d, Loss: %.4f%n", 
                                    epoch, batchCount, totalLoss / batchCount);
                }
            }
            
            System.out.printf("Epoch %d completed. Average Loss: %.4f%n", 
                            epoch, totalLoss / batchCount);
        }
    }
}
```

## 自回归生成过程:一个字一个字地"写作文"

### 逐步生成算法:"接龙式"创作

自回归生成就像写作文,一个字一个字往后写:

**步骤分解:**

1. **给个开头**(Prompt): "从前有座山"
2. **预测第1个新字**: 模型看"从前有座山",预测下一个字,比如"山"
3. **预测第2个新字**: 看"从前有座山山",预测"上", 得到"从前有座山山上"
4. **继续预测**: "从前有座山山上有"
5. **重复**: 直到写够长度或遇到结束符

**关键点:**

- 每次只生成1个词
- 新生成的词会加入上下文,影响下一个词的预测
- 就像多米诺骨牌,前面的影响后面的

**形象类比:**

1. **接龙游戏**: 你说一个成语,我接一个,不断延续
2. **盖楼**: 一层层往上盖,每层都依赖下面的层
3. **编故事**: 根据前面的情节,往后编,要前后连贯

```java
public class AutoregressiveGenerator {
    private CausalLanguageModel model;
    private Tokenizer tokenizer;
    
    public String generate(String prompt, GenerationConfig config) {
        // 1. 编码提示文本
        int[] inputIds = tokenizer.encode(prompt);
        List<Integer> generatedTokens = new ArrayList<>();
        
        // 将输入token添加到生成列表
        for (int tokenId : inputIds) {
            generatedTokens.add(tokenId);
        }
        
        // 2. 逐步生成
        for (int i = 0; i < config.getMaxTokens(); i++) {
            // 构建当前输入
            int[] currentInput = generatedTokens.stream()
                .mapToInt(Integer::intValue)
                .toArray();
            
            // 如果超过最大序列长度，移除最早的token
            if (currentInput.length > model.getConfig().getMaxPositionEmbeddings()) {
                currentInput = Arrays.copyOfRange(
                    currentInput, 
                    currentInput.length - model.getConfig().getMaxPositionEmbeddings(),
                    currentInput.length
                );
            }
            
            // 前向传播
            Variable inputTensor = new Variable(NdArray.of(new int[][]{currentInput}));
            Variable logits = model.forward(inputTensor);
            
            // 获取最后一个位置的logits
            Variable lastLogits = logits.slice(-1);
            
            // 根据配置选择下一个token
            int nextToken = selectNextToken(lastLogits, config);
            generatedTokens.add(nextToken);
            
            // 检查停止条件
            if (nextToken == tokenizer.getEosTokenId() || 
                isStopSequence(generatedTokens, config.getStopSequences())) {
                break;
            }
        }
        
        // 3. 解码生成的token
        int[] outputTokens = generatedTokens.stream()
            .mapToInt(Integer::intValue)
            .toArray();
            
        return tokenizer.decode(outputTokens);
    }
    
    private int selectNextToken(Variable logits, GenerationConfig config) {
        switch (config.getSamplingStrategy()) {
            case GREEDY:
                return greedySampling(logits);
            case TOP_K:
                return topKSampling(logits, config.getTopK());
            case TOP_P:
                return nucleusSampling(logits, config.getTopP());
            case TEMPERATURE:
                return temperatureSampling(logits, config.getTemperature());
            default:
                return greedySampling(logits);
        }
    }
}
```

### 采样策略:不要总说"最可能的话"

如果每次都选概率最高的词(贪心策略),生成的文本会很无聊:

**问题示例:**
```
输入: "今天天气"
贪心: "今天天气很好" (每次都选最常见的词)
创意: "今天天气有点反常,早上还晴朗,中午却..." (有变化,更自然)
```

**三种常见策略:**

1. **贪心采样**: 总是选概率最高的词
   - 优点: 语法正确,逻辑清晰
   - 缺点: 无聊,重复,像机器

2. **随机采样**: 按概率随机选
   - 优点: 有创意,多样化
   - 缺点: 可能选到低概率的奇怪词

3. **Top-k采样**: 只从概率最高的k个词中随机选
   - 平衡了质量和多样性
   - 是实际应用中最常用的方法

```java
private int greedySampling(Variable logits) {
    // 选择概率最高的token
    Variable probs = logits.softmax(-1);
    return probs.argmax(-1).getData().toInt();
}
```

```
private int temperatureSampling(Variable logits, double temperature) {
    // 应用温度参数调整分布
    Variable adjustedLogits = logits.div(temperature);
    Variable probs = adjustedLogits.softmax(-1);
    
    // 从调整后的分布中采样
    return sampleFromDistribution(probs);
}
```
```java
private int topKSampling(Variable logits, int k) {
    // 获取top-k的logits
    Variable topKLogits = getTopKLogits(logits, k);
    Variable probs = topKLogits.softmax(-1);
    
    // 从top-k分布中采样
    return sampleFromDistribution(probs);
}

private Variable getTopKLogits(Variable logits, int k) {
    // 对logits进行排序，保留top-k
    Variable sortedIndices = logits.argsort(-1, true);  // 降序排列
    Variable topKIndices = sortedIndices.slice(0, k);
    
    // 创建新的logits，只保留top-k位置的值
    Variable topKLogits = new Variable(logits.getShape());
    topKLogits = topKLogits.fill(Float.NEGATIVE_INFINITY);
    
    // 将top-k位置的值填充回去
    for (int i = 0; i < k; i++) {
        int index = topKIndices.getData().toIntArray()[i];
        topKLogits = topKLogits.setSlice(
            new int[]{0, index}, 
            new int[]{1, index + 1}, 
            logits.slice(0, 1).slice(index, index + 1)
        );
    }
    
    return topKLogits;
}
```

```java
private int nucleusSampling(Variable logits, double p) {
    // 计算累积概率
    Variable probs = logits.softmax(-1);
    Variable sortedProbs = probs.sort(-1, true);  // 降序排列
    Variable cumulativeProbs = sortedProbs.cumsum(-1);
    
    // 找到累积概率超过p的最小集合
    Variable mask = cumulativeProbs.gt(p);
    Variable filteredProbs = sortedProbs.where(mask, 0.0f);
    
    // 重新归一化
    Variable normalizedProbs = filteredProbs.div(filteredProbs.sum(-1, true));
    
    // 采样
    return sampleFromDistribution(normalizedProbs);
}
```

这些采样策略我们会在下一节详细讲解。

## 因果语言模型实现

### 模型架构

```java
public class CausalLanguageModel extends Model {
    private CausalLMConfig config;
    private EmbeddingLayer tokenEmbedding;
    private EmbeddingLayer positionEmbedding;
    private List<CausalTransformerBlock> transformerBlocks;
    private LayerNormalization finalLayerNorm;
    private LinearLayer lmHead;
    
    public CausalLanguageModel(CausalLMConfig config) {
        super("CausalLanguageModel");
        this.config = config;
        
        // 词嵌入层
        this.tokenEmbedding = new EmbeddingLayer(
            "token_embedding",
            config.getVocabSize(),
            config.getHiddenSize()
        );
        
        // 位置嵌入层
        this.positionEmbedding = new EmbeddingLayer(
            "position_embedding",
            config.getMaxPositionEmbeddings(),
            config.getHiddenSize()
        );
        
        // Transformer块
        this.transformerBlocks = new ArrayList<>();
        for (int i = 0; i < config.getNumLayers(); i++) {
            transformerBlocks.add(new CausalTransformerBlock(
                "block_" + i, config
            ));
        }
        
        // 最终层归一化
        this.finalLayerNorm = new LayerNormalization(
            "final_layer_norm",
            config.getHiddenSize(),
            config.getLayerNormEpsilon()
        );
        
        // 语言模型头部
        this.lmHead = new LinearLayer(
            "lm_head",
            config.getHiddenSize(),
            config.getVocabSize()
        );
        
        // 注意：在某些实现中，lmHead的权重可能与tokenEmbedding共享
        if (config.isTieWordEmbeddings()) {
            this.lmHead.setWeight(tokenEmbedding.getWeight().transpose());
        }
    }
    
    public Variable forward(Variable inputIds) {
        return forward(inputIds, null);
    }
    
    public Variable forward(Variable inputIds, Variable attentionMask) {
        int batchSize = inputIds.getShape().get(0);
        int seqLength = inputIds.getShape().get(1);
        
        // 创建位置ID
        Variable positionIds = createPositionIds(inputIds);
        
        // 词嵌入和位置嵌入
        Variable hiddenStates = tokenEmbedding.forward(inputIds);
        Variable positionEmbeds = positionEmbedding.forward(positionIds);
        hiddenStates = hiddenStates.add(positionEmbeds);
        
        // 应用嵌入层Dropout
        hiddenStates = new Dropout("embedding_dropout", config.getEmbeddingDropout())
                      .forward(hiddenStates);
        
        // 逐层处理
        for (CausalTransformerBlock block : transformerBlocks) {
            hiddenStates = block.forward(hiddenStates, attentionMask);
        }
        
        // 最终层归一化
        hiddenStates = finalLayerNorm.forward(hiddenStates);
        
        // 语言模型头部
        Variable logits = lmHead.forward(hiddenStates);
        
        return logits;
    }
    
    private Variable createPositionIds(Variable inputIds) {
        int batchSize = inputIds.getShape().get(0);
        int seqLength = inputIds.getShape().get(1);
        
        int[][] positionIds = new int[batchSize][seqLength];
        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < seqLength; j++) {
                positionIds[i][j] = j;
            }
        }
        
        return new Variable(NdArray.of(positionIds));
    }
}
```

### Transformer块实现

```java
public class CausalTransformerBlock extends Layer {
    private CausalLMConfig config;
    private LayerNormalization attentionLayerNorm;
    private CausalAttention selfAttention;
    private Dropout attentionDropout;
    private LayerNormalization feedForwardLayerNorm;
    private PositionwiseFeedForward feedForward;
    private Dropout feedForwardDropout;
    
    public CausalTransformerBlock(String name, CausalLMConfig config) {
        super(name);
        this.config = config;
        
        // 注意力层归一化
        this.attentionLayerNorm = new LayerNormalization(
            "attention_layer_norm",
            config.getHiddenSize(),
            config.getLayerNormEpsilon()
        );
        
        // 因果注意力
        this.selfAttention = new CausalAttention(
            "self_attention",
            config.getNumHeads(),
            config.getHiddenSize(),
            config.getAttentionDropout()
        );
        
        // 注意力Dropout
        this.attentionDropout = new Dropout(
            "attention_dropout",
            config.getAttentionDropout()
        );
        
        // 前馈网络层归一化
        this.feedForwardLayerNorm = new LayerNormalization(
            "feed_forward_layer_norm",
            config.getHiddenSize(),
            config.getLayerNormEpsilon()
        );
        
        // 前馈网络
        this.feedForward = new PositionwiseFeedForward(
            "feed_forward",
            config.getHiddenSize(),
            config.getIntermediateSize(),
            config.getHiddenDropout()
        );
        
        // 前馈网络Dropout
        this.feedForwardDropout = new Dropout(
            "feed_forward_dropout",
            config.getHiddenDropout()
        );
    }
    
    public Variable forward(Variable hiddenStates) {
        return forward(hiddenStates, null);
    }
    
    public Variable forward(Variable hiddenStates, Variable attentionMask) {
        // 自注意力块
        Variable attentionInput = attentionLayerNorm.forward(hiddenStates);
        Variable attentionOutput = selfAttention.forward(
            attentionInput, attentionMask
        );
        attentionOutput = attentionDropout.forward(attentionOutput);
        hiddenStates = hiddenStates.add(attentionOutput);
        
        // 前馈网络块
        Variable feedForwardInput = feedForwardLayerNorm.forward(hiddenStates);
        Variable feedForwardOutput = feedForward.forward(feedForwardInput);
        feedForwardOutput = feedForwardDropout.forward(feedForwardOutput);
        hiddenStates = hiddenStates.add(feedForwardOutput);
        
        return hiddenStates;
    }
}
```

## 性能优化技术

### 批量生成优化

```java
public class BatchedGenerator {
    public List<String> batchGenerate(List<String> prompts, 
                                    GenerationConfig config) {
        // 1. 批量编码
        List<int[]> inputIdsList = new ArrayList<>();
        for (String prompt : prompts) {
            inputIdsList.add(tokenizer.encode(prompt));
        }
        
        // 2. 动态批处理
        List<String> results = new ArrayList<>();
        int batchSize = config.getBatchSize();
        
        for (int i = 0; i < inputIdsList.size(); i += batchSize) {
            int end = Math.min(i + batchSize, inputIdsList.size());
            List<int[]> batchInputs = inputIdsList.subList(i, end);
            
            // 3. 批量生成
            List<String> batchResults = generateBatch(batchInputs, config);
            results.addAll(batchResults);
        }
        
        return results;
    }
    
    private List<String> generateBatch(List<int[]> batchInputs, 
                                     GenerationConfig config) {
        // 实现批量生成逻辑
        // ...
        return new ArrayList<>();
    }
}
```

## 本节小结

本节深入探讨了因果语言建模的原理和实现,我们学习了:

1. **数学原理**: 理解了"概率链"和"最大似然估计"的直观含义
2. **因果掩码**: 掌握了如何"戴眼罩"防止模型"作弊"
3. **训练目标**: 学会了如何"评分"和"改进"模型
4. **自回归生成**: 理解了"一个字一个字写作文"的过程
5. **代码实现**: 掌握了核心实现的技术要点

**关键要点回顾:**

- **因果** = 只看过去,不看未来
- **自回归** = 用自己生成的内容继续生成
- **掩码** = 防止信息泄露的"眼罩"
- **生成** = 逐步预测,像接龙游戏

因果语言建模是GPT系列模型的灵魂,理解了它,就理解了GPT为什么擅长文本生成。下一节我们将学习如何通过不同的采样策略,让生成的文本更加自然和有创意。
