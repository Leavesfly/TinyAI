# TinyAI Machine Learning æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¨¡å— (tinyai-dl-ml)

## æ¨¡å—æ¦‚è¿°

`tinyai-dl-ml` æ˜¯ TinyAI æ·±åº¦å­¦ä¹ æ¡†æ¶çš„æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¨¡å—ï¼Œæä¾›äº†å®Œæ•´çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œç®¡ç†åŠŸèƒ½ã€‚æœ¬æ¨¡å—æ˜¯æ¡†æ¶çš„æ ¸å¿ƒå¤§è„‘ï¼Œæ•´åˆäº†å…¶ä»–åŸºç¡€æ¨¡å—ï¼ˆndarrã€funcã€nnetï¼‰ï¼Œæ„å»ºäº†ä¸€ä¸ªé«˜åº¦å¯æ‰©å±•å’Œæ˜“äºä½¿ç”¨çš„æœºå™¨å­¦ä¹ å¹³å°ã€‚

## æ ¸å¿ƒæ¶æ„

### è®¾è®¡ç†å¿µ

æœ¬æ¨¡å—é‡‡ç”¨åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œé€šè¿‡æ¨¡å—åŒ–ç»„ä»¶æä¾›å®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥ä½œæµï¼š

- **Modelï¼ˆæ¨¡å‹ï¼‰**ï¼šæ¨¡å‹çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†
- **Trainerï¼ˆè®­ç»ƒå™¨ï¼‰**ï¼šè®­ç»ƒæµç¨‹æ§åˆ¶å’Œå¹¶è¡Œè®­ç»ƒæ”¯æŒ
- **DataSetï¼ˆæ•°æ®é›†ï¼‰**ï¼šæ•°æ®ç®¡ç†å’Œæ‰¹æ¬¡å¤„ç†
- **Optimizerï¼ˆä¼˜åŒ–å™¨ï¼‰**ï¼šå‚æ•°æ›´æ–°ç®—æ³•
- **Lossï¼ˆæŸå¤±å‡½æ•°ï¼‰**ï¼šæ¨¡å‹æ€§èƒ½è¯„ä¼°
- **Evaluatorï¼ˆè¯„ä¼°å™¨ï¼‰**ï¼šæ¨¡å‹æ•ˆæœè¯„ä¼°

```mermaid
graph TB
    subgraph "æ ¸å¿ƒæœºå™¨å­¦ä¹ æ¨¡å—"
        Model[Model æ¨¡å‹ç®¡ç†]
        Trainer[Trainer è®­ç»ƒå™¨]
        Monitor[Monitor ç›‘æ§å™¨]
        ModelSerializer[ModelSerializer åºåˆ—åŒ–]
        ParameterManager[ParameterManager å‚æ•°ç®¡ç†]
    end
    
    subgraph "æ•°æ®å¤„ç†å­ç³»ç»Ÿ"
        DataSet[DataSet æ•°æ®é›†]
        Batch[Batch æ‰¹æ¬¡]
        ArrayDataset[ArrayDataset æ•°ç»„æ•°æ®é›†]
        StreamDataset[StreamDataset æµæ•°æ®é›†]
    end
    
    subgraph "ä¼˜åŒ–å­ç³»ç»Ÿ"
        Optimizer[Optimizer ä¼˜åŒ–å™¨]
        SGD[SGD éšæœºæ¢¯åº¦ä¸‹é™]
        Adam[Adam è‡ªé€‚åº”ä¼˜åŒ–]
    end
    
    subgraph "æŸå¤±å‡½æ•°å­ç³»ç»Ÿ"
        Loss[Loss æŸå¤±å‡½æ•°]
        MSE[MeanSquaredLoss å‡æ–¹è¯¯å·®]
        CrossEntropy[SoftmaxCrossEntropy äº¤å‰ç†µ]
        MaskedCE[MaskedSoftmaxCELoss æ©ç äº¤å‰ç†µ]
    end
    
    subgraph "è¯„ä¼°å­ç³»ç»Ÿ"
        Evaluator[Evaluator è¯„ä¼°å™¨]
        Predictor[Predictor é¢„æµ‹å™¨]
        Translator[Translator è½¬æ¢å™¨]
    end
    
    subgraph "å¹¶è¡Œè®­ç»ƒå­ç³»ç»Ÿ"
        ParallelTrainingUtils[ParallelTrainingUtils å¹¶è¡Œå·¥å…·]
        GradientAggregator[GradientAggregator æ¢¯åº¦èšåˆ]
        ParallelBatchProcessor[ParallelBatchProcessor å¹¶è¡Œæ‰¹å¤„ç†]
    end
    
    Trainer --> Model
    Trainer --> DataSet
    Trainer --> Optimizer
    Trainer --> Loss
    Trainer --> Monitor
    Trainer --> ParallelTrainingUtils
    Model --> ModelSerializer
    Model --> ParameterManager
    DataSet --> Batch
```

### æ ¸å¿ƒç»„ä»¶

#### 1. æ¨¡å‹ç®¡ç†
- [`Model`](src/main/java/io/leavesfly/tinyai/ml/Model.java) - æ¨¡å‹æ ¸å¿ƒç±»ï¼Œå°è£…ç¥ç»ç½‘ç»œæ¶æ„
- [`ModelInfo`](src/main/java/io/leavesfly/tinyai/ml/ModelInfo.java) - æ¨¡å‹å…ƒæ•°æ®ç®¡ç†
- [`ModelSerializer`](src/main/java/io/leavesfly/tinyai/ml/ModelSerializer.java) - æ¨¡å‹åºåˆ—åŒ–ä¸ååºåˆ—åŒ–
- [`ParameterManager`](src/main/java/io/leavesfly/tinyai/ml/ParameterManager.java) - å‚æ•°ç®¡ç†å’Œä¼˜åŒ–

#### 2. è®­ç»ƒç³»ç»Ÿ
- [`Trainer`](src/main/java/io/leavesfly/tinyai/ml/Trainer.java) - è®­ç»ƒæµç¨‹æ§åˆ¶å™¨
- [`Monitor`](src/main/java/io/leavesfly/tinyai/ml/Monitor.java) - è®­ç»ƒè¿‡ç¨‹ç›‘æ§
- [`Plot`](src/main/java/io/leavesfly/tinyai/ml/Plot.java) - è®­ç»ƒå¯è§†åŒ–

## åŠŸèƒ½ç‰¹æ€§

### ğŸ§  æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†

#### æ¨¡å‹åˆ›å»ºä¸é…ç½®
```java
// åˆ›å»ºæ¨¡å‹
Model model = new Model("myModel", networkBlock);

// æ¨¡å‹ä¿¡æ¯ç®¡ç†
ModelInfo info = model.getModelInfo();
info.setDescription("æ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡å‹");
info.setVersion("1.0");
```

#### æ¨¡å‹åºåˆ—åŒ–
- **å®Œæ•´æ¨¡å‹ä¿å­˜**ï¼šä¿å­˜æ¨¡å‹ç»“æ„å’Œå‚æ•°
- **å‚æ•°ä¿å­˜**ï¼šä»…ä¿å­˜æ¨¡å‹å‚æ•°
- **æ£€æŸ¥ç‚¹ä¿å­˜**ï¼šä¿å­˜è®­ç»ƒçŠ¶æ€ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­
- **å‹ç¼©å­˜å‚¨**ï¼šæ”¯æŒæ¨¡å‹å‹ç¼©å­˜å‚¨

```java
// ä¿å­˜å®Œæ•´æ¨¡å‹
model.saveModel("model.bin");

// ä¿å­˜å‹ç¼©æ¨¡å‹
model.saveModelCompressed("model_compressed.bin");

// ä¿å­˜æ£€æŸ¥ç‚¹
model.saveCheckpoint("checkpoint.bin", epoch, loss);

// ä»…ä¿å­˜å‚æ•°
model.saveParameters("params.bin");
```

### ğŸš€ é«˜æ€§èƒ½è®­ç»ƒç³»ç»Ÿ

#### å•çº¿ç¨‹è®­ç»ƒ
ä¼ ç»Ÿçš„é¡ºåºè®­ç»ƒæ¨¡å¼ï¼Œé€‚åˆè°ƒè¯•å’Œå°è§„æ¨¡æ•°æ®ï¼š

```java
Trainer trainer = new Trainer(maxEpoch, monitor, evaluator);
trainer.init(dataSet, model, loss, optimizer);
trainer.singleThreadTrain(true); // æ‰“ä¹±æ•°æ®
```

#### å¹¶è¡Œè®­ç»ƒ
æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼Œæå‡è®­ç»ƒæ•ˆç‡ï¼š

```java
// å¯ç”¨å¹¶è¡Œè®­ç»ƒ
Trainer trainer = new Trainer(maxEpoch, monitor, evaluator, 
                              true, 4); // å¯ç”¨å¹¶è¡Œï¼Œ4ä¸ªçº¿ç¨‹
trainer.init(dataSet, model, loss, optimizer);
trainer.train(true); // è‡ªåŠ¨é€‰æ‹©å¹¶è¡Œè®­ç»ƒ
```

#### å¹¶è¡Œè®­ç»ƒç‰¹æ€§
- **è‡ªåŠ¨çº¿ç¨‹æ•°ä¼˜åŒ–**ï¼šæ ¹æ®CPUæ ¸å¿ƒæ•°å’Œæ‰¹æ¬¡æ•°é‡è‡ªåŠ¨è®¡ç®—
- **æ¢¯åº¦èšåˆ**ï¼šå¤šçº¿ç¨‹æ¢¯åº¦è‡ªåŠ¨èšåˆ
- **å†…å­˜ä¼˜åŒ–**ï¼šé«˜æ•ˆçš„æ¨¡å‹æ·±æ‹·è´æœºåˆ¶
- **å®¹é”™æœºåˆ¶**ï¼šè‡ªåŠ¨å›é€€åˆ°å•çº¿ç¨‹æ¨¡å¼

### ğŸ“Š æ•°æ®ç®¡ç†ç³»ç»Ÿ

#### æ•°æ®é›†ç±»å‹ (dataset)
- [`DataSet`](src/main/java/io/leavesfly/tinyai/ml/dataset/DataSet.java) - æ•°æ®é›†åŸºç±»
- [`ArrayDataset`](src/main/java/io/leavesfly/tinyai/ml/dataset/ArrayDataset.java) - æ•°ç»„æ•°æ®é›†
- [`StreamDataset`](src/main/java/io/leavesfly/tinyai/ml/dataset/StreamDataset.java) - æµå¼æ•°æ®é›†
- [`Batch`](src/main/java/io/leavesfly/tinyai/ml/dataset/Batch.java) - æ‰¹æ¬¡æ•°æ®ç®¡ç†

#### ç®€å•æ•°æ®é›† (simple)
é¢„å®šä¹‰çš„æ ‡å‡†æ•°æ®é›†ï¼Œä¾¿äºå¿«é€Ÿå¼€å§‹ï¼š
- èºæ—‹æ•°æ®é›†
- çº¿æ€§å›å½’æ•°æ®é›†
- åˆ†ç±»æ•°æ®é›†

```java
// ä½¿ç”¨æ•°æ®é›†
DataSet dataSet = new ArrayDataset(xData, yData);
dataSet.prepare();
dataSet.setBatchSize(32);

// æ‰¹æ¬¡å¤„ç†
List<Batch> batches = dataSet.getTrainDataSet().getBatches();
```

### âš¡ ä¼˜åŒ–ç®—æ³• (optimize)

#### æ”¯æŒçš„ä¼˜åŒ–å™¨
- [`SGD`](src/main/java/io/leavesfly/tinyai/ml/optimize/SGD.java) - éšæœºæ¢¯åº¦ä¸‹é™
- [`Adam`](src/main/java/io/leavesfly/tinyai/ml/optimize/Adam.java) - è‡ªé€‚åº”çŸ©ä¼°è®¡ä¼˜åŒ–å™¨

```java
// SGD ä¼˜åŒ–å™¨
Optimizer sgd = new SGD(0.01f); // å­¦ä¹ ç‡ 0.01

// Adam ä¼˜åŒ–å™¨
Optimizer adam = new Adam(0.001f, 0.9f, 0.999f); // å­¦ä¹ ç‡, beta1, beta2
```

### ğŸ“ˆ æŸå¤±å‡½æ•° (loss)

#### æ”¯æŒçš„æŸå¤±å‡½æ•°
- [`MeanSquaredLoss`](src/main/java/io/leavesfly/tinyai/ml/loss/MeanSquaredLoss.java) - å‡æ–¹è¯¯å·®æŸå¤±
- [`SoftmaxCrossEntropy`](src/main/java/io/leavesfly/tinyai/ml/loss/SoftmaxCrossEntropy.java) - Softmaxäº¤å‰ç†µæŸå¤±
- [`MaskedSoftmaxCELoss`](src/main/java/io/leavesfly/tinyai/ml/loss/MaskedSoftmaxCELoss.java) - å¸¦æ©ç çš„Softmaxäº¤å‰ç†µ
- [`Classify`](src/main/java/io/leavesfly/tinyai/ml/loss/Classify.java) - åˆ†ç±»æŸå¤±

```java
// å›å½’ä»»åŠ¡
Loss mse = new MeanSquaredLoss();

// åˆ†ç±»ä»»åŠ¡
Loss crossEntropy = new SoftmaxCrossEntropy();

// åºåˆ—ä»»åŠ¡ï¼ˆæ”¯æŒå˜é•¿åºåˆ—ï¼‰
Loss maskedCE = new MaskedSoftmaxCELoss();
```

### ğŸ” æ¨¡å‹è¯„ä¼° (evaluator)

#### è¯„ä¼°ç»„ä»¶
- [`Evaluator`](src/main/java/io/leavesfly/tinyai/ml/evaluator/Evaluator.java) - è¯„ä¼°å™¨åŸºç±»
- **å‡†ç¡®ç‡è¯„ä¼°**ï¼šåˆ†ç±»æ¨¡å‹å‡†ç¡®ç‡è®¡ç®—
- **å›å½’è¯„ä¼°**ï¼šå›å½’æ¨¡å‹è¯¯å·®åˆ†æ

### ğŸ”® æ¨¡å‹æ¨ç† (inference)

#### æ¨ç†ç»„ä»¶
- [`Predictor`](src/main/java/io/leavesfly/tinyai/ml/inference/Predictor.java) - æ¨¡å‹é¢„æµ‹å™¨
- [`Translator`](src/main/java/io/leavesfly/tinyai/ml/inference/Translator.java) - è¾“å‡ºè½¬æ¢å™¨

```java
// æ¨¡å‹æ¨ç†
Variable prediction = model.forward(inputVariable);

// ä½¿ç”¨é¢„æµ‹å™¨
Predictor predictor = new Predictor(model);
NdArray result = predictor.predict(inputData);
```

## æŠ€æœ¯ä¾èµ–

æœ¬æ¨¡å—ä¾èµ–ä»¥ä¸‹ TinyAI æ ¸å¿ƒæ¨¡å—ï¼š

- `tinyai-dl-func` - è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼Œæä¾›åå‘ä¼ æ’­æ”¯æŒ
- `tinyai-dl-ndarr` - å¤šç»´æ•°ç»„åŸºç¡€åº“ï¼Œæä¾›å¼ é‡è®¡ç®—
- `tinyai-dl-nnet` - ç¥ç»ç½‘ç»œå±‚ï¼Œæä¾›ç½‘ç»œæ„å»ºç»„ä»¶

å¤–éƒ¨ä¾èµ–ï¼š
- `jfreechart` - å›¾è¡¨å¯è§†åŒ–åº“ï¼Œç”¨äºè®­ç»ƒç›‘æ§
- `junit` - å•å…ƒæµ‹è¯•æ¡†æ¶

## ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹

```java
import io.leavesfly.tinyai.ml.*;
import io.leavesfly.tinyai.ml.dataset.ArrayDataset;
import io.leavesfly.tinyai.ml.loss.SoftmaxCrossEntropy;
import io.leavesfly.tinyai.ml.optimize.Adam;
import io.leavesfly.tinyai.nnet.block.MlpBlock;

// 1. å‡†å¤‡æ•°æ®
NdArray xData = ...; // è¾“å…¥æ•°æ®
NdArray yData = ...; // æ ‡ç­¾æ•°æ®
DataSet dataSet = new ArrayDataset(xData, yData);

// 2. åˆ›å»ºæ¨¡å‹
MlpBlock mlpBlock = new MlpBlock("classifier", 
                                 inputShape, 
                                 new int[]{128, 64, 10});
Model model = new Model("mnist_classifier", mlpBlock);

// 3. é…ç½®è®­ç»ƒç»„ä»¶
Loss loss = new SoftmaxCrossEntropy();
Optimizer optimizer = new Adam(0.001f);
Monitor monitor = new Monitor();
Evaluator evaluator = new ClassificationEvaluator();

// 4. åˆ›å»ºè®­ç»ƒå™¨ï¼ˆå¯ç”¨å¹¶è¡Œè®­ç»ƒï¼‰
Trainer trainer = new Trainer(100, monitor, evaluator, true, 4);
trainer.init(dataSet, model, loss, optimizer);

// 5. å¼€å§‹è®­ç»ƒ
trainer.train(true);

// 6. ä¿å­˜æ¨¡å‹
model.saveModel("trained_model.bin");
```

### æ¨¡å‹åŠ è½½å’Œæ¨ç†

```java
// åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
Model loadedModel = Model.loadModel("trained_model.bin");

// æ¨ç†
Variable input = new Variable(inputData);
Variable prediction = loadedModel.forward(input);

// è·å–é¢„æµ‹ç»“æœ
NdArray result = prediction.getValue();
```

### æ¨¡å‹ä¿¡æ¯ç®¡ç†

```java
// è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
ModelInfo info = model.getModelInfo();
System.out.println("æ¨¡å‹åç§°: " + info.getName());
System.out.println("å‚æ•°æ•°é‡: " + info.getTotalParameters());
System.out.println("æ¶æ„ç±»å‹: " + info.getArchitectureType());

// å¯¼å‡ºæ¨¡å‹ä¿¡æ¯
ModelInfoExporter exporter = new ModelInfoExporter();
exporter.exportToMarkdown(info, "model_info.md");
```

## å¹¶è¡Œè®­ç»ƒè¯¦è§£

### å¹¶è¡Œè®­ç»ƒæ¶æ„

TinyAI çš„å¹¶è¡Œè®­ç»ƒç³»ç»Ÿé‡‡ç”¨æ•°æ®å¹¶è¡Œç­–ç•¥ï¼š

1. **æ‰¹æ¬¡åˆ†å‰²**ï¼šå°†å¤§æ‰¹æ¬¡æ•°æ®åˆ†å‰²ç»™å¤šä¸ªçº¿ç¨‹
2. **å¹¶è¡Œå¤„ç†**ï¼šæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„æ¨¡å‹å‰¯æœ¬è®¡ç®—æ¢¯åº¦
3. **æ¢¯åº¦èšåˆ**ï¼šæ”¶é›†æ‰€æœ‰çº¿ç¨‹çš„æ¢¯åº¦å¹¶è¿›è¡Œå¹³å‡
4. **å‚æ•°æ›´æ–°**ï¼šä½¿ç”¨èšåˆåçš„æ¢¯åº¦æ›´æ–°ä¸»æ¨¡å‹å‚æ•°

### æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§

- **æ™ºèƒ½çº¿ç¨‹æ•°è®¡ç®—**ï¼šæ ¹æ®CPUæ ¸å¿ƒæ•°å’Œæ•°æ®é‡è‡ªåŠ¨ä¼˜åŒ–
- **å†…å­˜æ•ˆç‡**ï¼šé«˜æ•ˆçš„æ¨¡å‹æ·±æ‹·è´æœºåˆ¶
- **è´Ÿè½½å‡è¡¡**ï¼šåŠ¨æ€åˆ†é…æ‰¹æ¬¡åˆ°å„ä¸ªçº¿ç¨‹
- **å¼‚å¸¸å¤„ç†**ï¼šè‡ªåŠ¨å›é€€æœºåˆ¶ä¿è¯è®­ç»ƒç¨³å®šæ€§

## æµ‹è¯•è¦†ç›–

æ¨¡å—åŒ…å«å®Œæ•´çš„å•å…ƒæµ‹è¯•ï¼Œè¦†ç›–ï¼š
- æ¨¡å‹åºåˆ—åŒ–å’Œååºåˆ—åŒ–æµ‹è¯•
- å‚æ•°ç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•
- å¹¶è¡Œè®­ç»ƒæ€§èƒ½æµ‹è¯•
- å„ç§æŸå¤±å‡½æ•°æµ‹è¯•
- ä¼˜åŒ–å™¨æ”¶æ•›æ€§æµ‹è¯•

è¿è¡Œæµ‹è¯•ï¼š
```bash
cd /Users/yefei.yf/Qoder/TinyAI
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home
mvn test -pl tinyai-dl-ml
```

## æ¨¡å—ç‰¹è‰²

### ğŸ—ï¸ ä¼ä¸šçº§æ¶æ„
- å®Œæ•´çš„æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
- ç”Ÿäº§å°±ç»ªçš„åºåˆ—åŒ–æœºåˆ¶
- ä¸°å¯Œçš„ç›‘æ§å’Œæ—¥å¿—åŠŸèƒ½

### âš¡ é«˜æ€§èƒ½è®¡ç®—
- å¤šçº¿ç¨‹å¹¶è¡Œè®­ç»ƒæ”¯æŒ
- å†…å­˜ä¼˜åŒ–çš„æ•°æ®å¤„ç†
- GPUå…¼å®¹çš„å¼ é‡è®¡ç®—åŸºç¡€

### ğŸ”§ æ˜“ç”¨æ€§è®¾è®¡
- ç®€æ´çš„APIè®¾è®¡
- ä¸°å¯Œçš„é¢„ç½®ç»„ä»¶
- è¯¦ç»†çš„æ–‡æ¡£å’Œç¤ºä¾‹

### ğŸ›¡ï¸ å¯é æ€§ä¿è¯
- å¹¿æ³›çš„å•å…ƒæµ‹è¯•è¦†ç›–
- å¼‚å¸¸å¤„ç†å’Œå®¹é”™æœºåˆ¶
- æŒç»­çš„æ€§èƒ½ä¼˜åŒ–

## å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çš„ä¼˜åŒ–å™¨

```java
public class CustomOptimizer extends Optimizer {
    @Override
    public void update() {
        // å®ç°è‡ªå®šä¹‰ä¼˜åŒ–ç®—æ³•
    }
}
```

### æ·»åŠ æ–°çš„æŸå¤±å‡½æ•°

```java
public class CustomLoss extends Loss {
    @Override
    public Variable loss(Variable y, Variable predictY) {
        // å®ç°è‡ªå®šä¹‰æŸå¤±è®¡ç®—
        return customLossVariable;
    }
}
```

### è‡ªå®šä¹‰è¯„ä¼°å™¨

```java
public class CustomEvaluator extends Evaluator {
    @Override
    public void evaluate() {
        // å®ç°è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
    }
}
```

## ç‰ˆæœ¬ä¿¡æ¯

- **å½“å‰ç‰ˆæœ¬**: 1.0-SNAPSHOT
- **Java ç‰ˆæœ¬**: 17+
- **æ„å»ºå·¥å…·**: Maven 3.6+
- **å¹¶è¡Œè®­ç»ƒ**: æ”¯æŒå¤šçº¿ç¨‹æ•°æ®å¹¶è¡Œ

## ç›¸å…³æ¨¡å—

- [`tinyai-dl-nnet`](../tinyai-dl-nnet/README.md) - ç¥ç»ç½‘ç»œå±‚æ¨¡å—
- [`tinyai-dl-func`](../tinyai-dl-func/README.md) - è‡ªåŠ¨å¾®åˆ†å¼•æ“
- [`tinyai-dl-ndarr`](../tinyai-dl-ndarr/README.md) - å¤šç»´æ•°ç»„åŸºç¡€åº“
- [`tinyai-dl-case`](../tinyai-dl-case/README.md) - åº”ç”¨ç¤ºä¾‹æ¨¡å—

---

**TinyAI Machine Learning æ¨¡å—** - è®©æ·±åº¦å­¦ä¹ è®­ç»ƒå˜å¾—ç®€å•ã€é«˜æ•ˆã€å¯é  ğŸš€