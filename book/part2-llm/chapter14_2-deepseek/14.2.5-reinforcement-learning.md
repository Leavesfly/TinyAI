# 14.2.5 å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥

> **æ ¸å¿ƒä¸»é¢˜**ï¼šè®©AIä»"å¥–åŠ±"ä¸­å­¦ä¹ è¿›æ­¥

## å¼•è¨€ï¼šä»å¥–åŠ±ä¸­å­¦ä¹  ğŸ¯

æƒ³è±¡è®­ç»ƒä¸€åªå® ç‰©ï¼š
- åšå¯¹äº†â†’ç»™é›¶é£Ÿï¼ˆæ­£å¥–åŠ±ï¼‰
- åšé”™äº†â†’æ— å¥–åŠ±ï¼ˆè´Ÿåé¦ˆï¼‰
- ä¸æ–­é‡å¤â†’å­¦ä¼šæ­£ç¡®è¡Œä¸º

DeepSeekçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¹Ÿæ˜¯å¦‚æ­¤â€”â€”é€šè¿‡å¤šç»´åº¦å¥–åŠ±ä¿¡å·æŒ‡å¯¼æ¨¡å‹æŒç»­æ”¹è¿›ã€‚

## å­¦ä¹ ç›®æ ‡ ğŸ¯

- âœ… ç†è§£å¼ºåŒ–å­¦ä¹ åœ¨LLMä¸­çš„åº”ç”¨
- âœ… æŒæ¡å¤šç»´åº¦å¥–åŠ±ä¿¡å·è®¾è®¡
- âœ… å­¦ä¼šç­–ç•¥æ¢¯åº¦ä¼˜åŒ–æ–¹æ³•
- âœ… ç†è§£R1å’ŒV3çš„ä¸åŒè®­ç»ƒç­–ç•¥
- âœ… æŒæ¡è´Ÿè½½å‡è¡¡ä¼˜åŒ–

## ä¸€ã€å¼ºåŒ–å­¦ä¹ åŸºç¡€ ğŸ“š

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

**å¼ºåŒ–å­¦ä¹ ä¸‰è¦ç´ **ï¼š

```mermaid
graph LR
    Agent[æ™ºèƒ½ä½“/æ¨¡å‹] -->|æ‰§è¡ŒåŠ¨ä½œ| Environment[ç¯å¢ƒ/ä»»åŠ¡]
    Environment -->|è¿”å›å¥–åŠ±| Reward[å¥–åŠ±ä¿¡å·]
    Reward -->|æŒ‡å¯¼å­¦ä¹ | Agent
```

| è¦ç´  | LLMä¸­çš„å¯¹åº” | ç¤ºä¾‹ |
|------|-----------|------|
| æ™ºèƒ½ä½“ | DeepSeekæ¨¡å‹ | R1/V3 |
| åŠ¨ä½œ | ç”Ÿæˆæ¨ç†è¿‡ç¨‹ | å¤šæ­¥æ¨ç†é“¾ |
| çŠ¶æ€ | å½“å‰æ¨ç†çŠ¶æ€ | æ¨ç†æ­¥éª¤å‘é‡ |
| å¥–åŠ± | è´¨é‡è¯„åˆ† | å‡†ç¡®æ€§+æ¨ç†è´¨é‡ |
| ç­–ç•¥ | æ¨¡å‹å‚æ•° | ç¥ç»ç½‘ç»œæƒé‡ |

### 1.2 ç­–ç•¥æ¢¯åº¦æ–¹æ³•

**æ ¸å¿ƒæ€æƒ³**ï¼šè°ƒæ•´å‚æ•°ä½¿é«˜å¥–åŠ±åŠ¨ä½œæ¦‚ç‡å¢åŠ 

**æ•°å­¦è¡¨è¾¾**ï¼š

```
æ¢¯åº¦ = âˆ‡_Î¸ log Ï€_Î¸(a|s) Ã— (R - baseline)

å…¶ä¸­ï¼š
- Ï€_Î¸(a|s)ï¼šåœ¨çŠ¶æ€sä¸‹é€‰æ‹©åŠ¨ä½œaçš„æ¦‚ç‡
- Rï¼šè·å¾—çš„å¥–åŠ±
- baselineï¼šåŸºçº¿å€¼ï¼ˆå‡å°‘æ–¹å·®ï¼‰
```

**TinyAIå®ç°**ï¼ˆåŸºäº`RLTrainer.java`ï¼‰ï¼š

```java
// è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
Variable policyLoss = computePolicyLoss(
    modelOutput, 
    targetIds, 
    totalReward - runningBaseline  // ä¼˜åŠ¿å€¼
);
```

## äºŒã€R1çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ ğŸ“

### 2.1 å¤šç»´åº¦å¥–åŠ±è®¾è®¡

**å¥–åŠ±ç»„ä»¶**ï¼ˆæ¥è‡ª`RLTrainer.java`ï¼‰ï¼š

| å¥–åŠ±ç±»å‹ | æƒé‡ | è®¡ç®—æ–¹æ³• | ä½œç”¨ |
|---------|------|---------|------|
| å‡†ç¡®æ€§å¥–åŠ± | 40% | åŸºäºäº¤å‰ç†µ | ç¡®ä¿ç­”æ¡ˆæ­£ç¡® |
| æ¨ç†è´¨é‡å¥–åŠ± | 30% | åæ€æ¨¡å—è¯„åˆ† | æå‡æ¨ç†æ¸…æ™°åº¦ |
| åæ€å¥–åŠ± | 20% | æ˜¯å¦è§¦å‘æ”¹è¿› | é¼“åŠ±è‡ªæˆ‘ä¼˜åŒ– |
| ä¸€è‡´æ€§å¥–åŠ± | 10% | æ¨ç†æ­¥éª¤ä¸€è‡´æ€§ | ä¿è¯é€»è¾‘è¿è´¯ |

**æ ¸å¿ƒä»£ç **ï¼š

```java
public class RLTrainer extends Trainer {
    // å¥–åŠ±æƒé‡
    private float accuracyRewardWeight = 1.0f;
    private float reasoningQualityWeight = 0.3f;
    private float reflectionRewardWeight = 0.2f;
    private float consistencyRewardWeight = 0.1f;
    
    private RewardComponents computeRewardComponents(
        DeepSeekR1Result modelOutput, NdArray targetIds) {
        
        // 1. å‡†ç¡®æ€§å¥–åŠ±
        float accuracyReward = computeAccuracyReward(
            modelOutput.getLogits(), targetIds);
        
        // 2. æ¨ç†è´¨é‡å¥–åŠ±ï¼ˆæ¥è‡ªåæ€æ¨¡å—ï¼‰
        float reasoningQualityReward = 
            modelOutput.getReflectionResult().getQualityScore();
        
        // 3. åæ€å¥–åŠ±
        float reflectionReward = 
            modelOutput.getReflectionResult().needsRefinement() 
                ? 0.5f : 1.0f;
        
        // 4. ä¸€è‡´æ€§å¥–åŠ±
        float consistencyReward = computeConsistencyReward(modelOutput);
        
        return new RewardComponents(accuracyReward, 
            reasoningQualityReward, reflectionReward, consistencyReward);
    }
}
```

### 2.2 è®­ç»ƒæµç¨‹

**å®Œæ•´è®­ç»ƒæ­¥éª¤**ï¼š

```mermaid
flowchart TD
    Input[è¾“å…¥æ•°æ®] --> Forward[å‰å‘ä¼ æ’­]
    Forward --> Reasoning[æ¨ç†æ¨¡å—]
    Reasoning --> Reflection[åæ€æ¨¡å—]
    Reflection --> Compute[è®¡ç®—å¥–åŠ±]
    
    Compute --> R1[å‡†ç¡®æ€§å¥–åŠ±]
    Compute --> R2[æ¨ç†è´¨é‡å¥–åŠ±]
    Compute --> R3[åæ€å¥–åŠ±]
    Compute --> R4[ä¸€è‡´æ€§å¥–åŠ±]
    
    R1 --> Total[æ€»å¥–åŠ±]
    R2 --> Total
    R3 --> Total
    R4 --> Total
    
    Total --> Policy[ç­–ç•¥æ¢¯åº¦]
    Policy --> Backward[åå‘ä¼ æ’­]
    Backward --> Update[å‚æ•°æ›´æ–°]
```

**å…³é”®ä»£ç **ï¼š

```java
public Map<String, Float> trainRLStep(NdArray inputIds, NdArray targetIds) {
    // 1. å‰å‘ä¼ æ’­
    DeepSeekR1Result modelOutput = 
        deepseekModel.inferenceWithDetails(inputIds, null);
    
    // 2. è®¡ç®—å¥–åŠ±
    RewardComponents rewards = computeRewardComponents(modelOutput, targetIds);
    float totalReward = computeTotalReward(rewards);
    
    // 3. æ›´æ–°åŸºçº¿ï¼ˆå‡å°‘æ–¹å·®ï¼‰
    runningBaseline = baselineDecay * runningBaseline 
                    + (1 - baselineDecay) * totalReward;
    
    // 4. è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
    Variable policyLoss = computePolicyLoss(
        modelOutput, targetIds, totalReward - runningBaseline);
    
    // 5. è®¡ç®—ä»·å€¼å‡½æ•°æŸå¤±
    Variable valueLoss = computeValueLoss(totalReward);
    
    // 6. è®¡ç®—ç†µæŸå¤±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
    Variable entropyLoss = computeEntropyLoss(modelOutput.getLogits());
    
    // 7. ç»„åˆæ€»æŸå¤±
    Variable totalLoss = policyLoss
        .add(valueLoss.mulNum(valueCoefficient))
        .add(entropyLoss.mulNum(entropyCoefficient));
    
    // 8. åå‘ä¼ æ’­å’Œæ¢¯åº¦è£å‰ª
    totalLoss.backward();
    clipGradients(1.0f);
    
    // 9. æ›´æ–°å‚æ•°
    performParameterUpdate();
    
    return metrics;
}
```

### 2.3 åŸºçº¿ä¼°è®¡ä¸æ–¹å·®å‡å°‘

**ä¸ºä»€ä¹ˆéœ€è¦åŸºçº¿ï¼Ÿ**

```
æ— åŸºçº¿ï¼šæ¢¯åº¦ âˆ Rï¼ˆæ–¹å·®å¤§ï¼‰
æœ‰åŸºçº¿ï¼šæ¢¯åº¦ âˆ (R - baseline)ï¼ˆæ–¹å·®å°ï¼Œè®­ç»ƒç¨³å®šï¼‰
```

**æŒ‡æ•°ç§»åŠ¨å¹³å‡åŸºçº¿**ï¼š

```java
// baselineDecay = 0.99
runningBaseline = 0.99 * runningBaseline + 0.01 * totalReward;
```

**æ•ˆæœå¯¹æ¯”**ï¼š

| è®­ç»ƒæ­¥éª¤ | å¥–åŠ± | æ— åŸºçº¿æ¢¯åº¦ | æœ‰åŸºçº¿æ¢¯åº¦ |
|---------|------|-----------|-----------|
| 1 | 0.8 | 0.8 | 0.8 - 0 = 0.8 |
| 2 | 0.75 | 0.75 | 0.75 - 0.792 = -0.042 |
| 3 | 0.9 | 0.9 | 0.9 - 0.795 = 0.105 |

æœ‰åŸºçº¿åï¼Œæ¢¯åº¦æ›´ç¨³å®šï¼Œè®­ç»ƒæ•ˆç‡æå‡ã€‚

## ä¸‰ã€V3çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒ ğŸš€

### 3.1 é¢å¤–çš„ä»£ç è´¨é‡å¥–åŠ±

**V3ç‰¹æœ‰å¥–åŠ±**ï¼ˆåŸºäº`V3RLTrainer.java`ï¼‰ï¼š

| å¥–åŠ±ç±»å‹ | æƒé‡ | è¯´æ˜ |
|---------|------|------|
| å‡†ç¡®æ€§å¥–åŠ± | 30% | ä»»åŠ¡å®Œæˆæ­£ç¡®æ€§ |
| æ¨ç†è´¨é‡å¥–åŠ± | 30% | æ¨ç†æ·±åº¦å’Œé€»è¾‘æ€§ |
| ä»£ç è´¨é‡å¥–åŠ± | 20% | ä»£ç å¯è¯»æ€§å’Œæ­£ç¡®æ€§ |
| MoEæ•ˆç‡å¥–åŠ± | 20% | ä¸“å®¶ä½¿ç”¨æ•ˆç‡å’Œå¹³è¡¡æ€§ |

**ä»£ç è´¨é‡è¯„ä¼°**ï¼š

```java
private float computeCodeQualityReward(CodeGenerationResult codeResult) {
    float syntaxScore = codeResult.getSyntaxScore();
    float qualityScore = codeResult.getQualityScore();
    float confidence = codeResult.getCodeConfidence();
    
    // ç»¼åˆè¯„ä¼°
    return 0.4f * syntaxScore 
         + 0.4f * qualityScore 
         + 0.2f * confidence;
}
```

### 3.2 MoEè´Ÿè½½å‡è¡¡ä¼˜åŒ–

**è´Ÿè½½å‡è¡¡æƒ©ç½š**ï¼š

**é—®é¢˜**ï¼šæŸäº›ä¸“å®¶è¿‡åº¦ä½¿ç”¨ï¼Œå…¶ä»–ä¸“å®¶é—²ç½®

**è§£å†³**ï¼šKLæ•£åº¦æŸå¤±

```java
private float computeLoadBalanceLoss(NdArray routerProbs) {
    // è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡ä½¿ç”¨ç‡
    float[] expertUsage = computeExpertUsage(routerProbs);
    
    // ç›®æ ‡ï¼šå‡åŒ€åˆ†å¸ƒ (1/numExperts)
    float targetUsage = 1.0f / numExperts;
    
    // KLæ•£åº¦
    float klLoss = 0.0f;
    for (int i = 0; i < numExperts; i++) {
        float usage = expertUsage[i] + 1e-8f; // é¿å…log(0)
        klLoss += usage * Math.log(usage / targetUsage);
    }
    
    return klLoss;
}
```

**æ€»æŸå¤±**ï¼š

```
Total_Loss = Task_Loss + Î»_balance Ã— LoadBalance_Loss

å…¶ä¸­Î»_balanceé€šå¸¸ä¸º0.01
```

### 3.3 ä»»åŠ¡æ„ŸçŸ¥çš„å¥–åŠ±å¡‘å½¢

**æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´å¥–åŠ±æƒé‡**ï¼š

```java
private RewardWeights getTaskSpecificWeights(TaskType taskType) {
    switch (taskType) {
        case CODING:
            return new RewardWeights(
                0.2f,  // å‡†ç¡®æ€§
                0.2f,  // æ¨ç†è´¨é‡
                0.5f,  // ä»£ç è´¨é‡ï¼ˆæ›´é«˜æƒé‡ï¼‰
                0.1f   // MoEæ•ˆç‡
            );
        case REASONING:
            return new RewardWeights(
                0.3f,  // å‡†ç¡®æ€§
                0.5f,  // æ¨ç†è´¨é‡ï¼ˆæ›´é«˜æƒé‡ï¼‰
                0.1f,  // ä»£ç è´¨é‡
                0.1f   // MoEæ•ˆç‡
            );
        default:
            return new RewardWeights(0.3f, 0.3f, 0.2f, 0.2f);
    }
}
```

## å››ã€è®­ç»ƒä¼˜åŒ–æŠ€å·§ âš™ï¸

### 4.1 æ¢¯åº¦è£å‰ª

**é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸**ï¼š

```java
private void clipGradients(float maxNorm) {
    float totalNorm = 0.0f;
    
    // è®¡ç®—æ‰€æœ‰æ¢¯åº¦çš„L2èŒƒæ•°
    for (Parameter param : model.getParameters()) {
        if (param.getGrad() != null) {
            NdArray grad = param.getGrad();
            totalNorm += grad.l2Norm() * grad.l2Norm();
        }
    }
    totalNorm = (float) Math.sqrt(totalNorm);
    
    // å¦‚æœè¶…è¿‡é˜ˆå€¼ï¼Œç¼©æ”¾æ¢¯åº¦
    if (totalNorm > maxNorm) {
        float scale = maxNorm / (totalNorm + 1e-6f);
        for (Parameter param : model.getParameters()) {
            if (param.getGrad() != null) {
                param.getGrad().mulNum(scale);
            }
        }
    }
}
```

### 4.2 ç†µæ­£åˆ™åŒ–

**é¼“åŠ±æ¢ç´¢ï¼Œé¿å…è¿‡æ—©æ”¶æ•›**ï¼š

```
Entropy_Loss = -Î£ p(a) log p(a)

æ€»æŸå¤± = Policy_Loss + Î± Ã— Entropy_Loss
```

**ä»£ç å®ç°**ï¼š

```java
private Variable computeEntropyLoss(Variable logits) {
    // Softmaxæ¦‚ç‡
    NdArray probs = logits.getValue().softMax();
    
    // ç†µè®¡ç®—
    float entropy = 0.0f;
    for (int i = 0; i < probs.getShape().size(); i++) {
        float p = probs.getByLinearIndex(i) + 1e-8f;
        entropy -= p * Math.log(p);
    }
    
    return new Variable(NdArray.scalar(entropy));
}
```

### 4.3 ç»éªŒå›æ”¾ï¼ˆå¯é€‰ï¼‰

**å­˜å‚¨å†å²è½¨è¿¹ï¼Œé‡å¤å­¦ä¹ **ï¼š

```java
public class ExperienceReplayBuffer {
    private List<Experience> buffer;
    private int maxSize = 10000;
    
    public void add(Experience exp) {
        buffer.add(exp);
        if (buffer.size() > maxSize) {
            buffer.remove(0); // ç§»é™¤æœ€æ—§çš„
        }
    }
    
    public List<Experience> sample(int batchSize) {
        // éšæœºé‡‡æ ·
        return randomSample(buffer, batchSize);
    }
}
```

## äº”ã€è®­ç»ƒç›‘æ§ä¸è¯„ä¼° ğŸ“Š

### 5.1 å…³é”®æŒ‡æ ‡

**è®­ç»ƒè¿‡ç¨‹ç›‘æ§**ï¼š

| æŒ‡æ ‡ | è¯´æ˜ | æœŸæœ›è¶‹åŠ¿ |
|------|------|---------|
| å¹³å‡å¥–åŠ± | æ¨¡å‹è¡¨ç° | ä¸Šå‡ |
| æ¨ç†è´¨é‡åˆ†æ•° | æ¨ç†æ¸…æ™°åº¦ | ä¸Šå‡ |
| ç­–ç•¥æŸå¤± | ç­–ç•¥ä¼˜åŒ–è¿›åº¦ | ä¸‹é™åç¨³å®š |
| ç†µ | æ¢ç´¢ç¨‹åº¦ | é€æ¸ä¸‹é™ |
| ä¸“å®¶è´Ÿè½½å‡è¡¡ | ä¸“å®¶ä½¿ç”¨å‡è¡¡æ€§ | KLæ•£åº¦ä¸‹é™ |

**ä»£ç å®ç°**ï¼š

```java
private void updateTrainingMetrics(RewardComponents rewards, 
                                  Variable policyLoss,
                                  Variable valueLoss, 
                                  Variable entropyLoss,
                                  float totalReward) {
    trainingMetrics.put("total_reward", totalReward);
    trainingMetrics.put("policy_loss", policyLoss.getValue().getNumber());
    trainingMetrics.put("value_loss", valueLoss.getValue().getNumber());
    trainingMetrics.put("entropy_loss", entropyLoss.getValue().getNumber());
    trainingMetrics.put("avg_quality", rewards.getReasoningQualityReward());
    
    // è®°å½•å†å²
    rewardHistory.add(totalReward);
    qualityHistory.add(rewards.getReasoningQualityReward());
}
```

### 5.2 è®­ç»ƒæ›²çº¿åˆ†æ

**å…¸å‹è®­ç»ƒæ›²çº¿**ï¼š

```
å¥–åŠ±æ›²çº¿ï¼š
1.0 |                    ___________
    |               ____/
0.8 |          ____/
    |     ____/
0.6 | ___/
    |/
0.4 +--------------------------------
     0    500   1000  1500  2000  æ­¥æ•°

æ¨ç†è´¨é‡æ›²çº¿ï¼š
1.0 |                         -------
    |                   _____/
0.8 |              ____/
    |         ____/
0.6 |    ____/
    |___/
0.4 +--------------------------------
     0    500   1000  1500  2000  æ­¥æ•°
```

## å…­ã€å®æˆ˜ç¤ºä¾‹ ğŸ’¡

### 6.1 R1è®­ç»ƒç¤ºä¾‹

```java
// åˆ›å»ºè®­ç»ƒå™¨
RLTrainer trainer = new RLTrainer(
    maxEpoch,    // 100
    monitor,     // ç›‘æ§å™¨
    evaluator    // è¯„ä¼°å™¨
);

// è®¾ç½®å¥–åŠ±æƒé‡
trainer.setAccuracyRewardWeight(1.0f);
trainer.setReasoningQualityWeight(0.3f);
trainer.setReflectionRewardWeight(0.2f);
trainer.setConsistencyRewardWeight(0.1f);

// åˆå§‹åŒ–
trainer.init(dataset, deepseekR1Model, loss, optimizer);

// è®­ç»ƒå¾ªç¯
for (int epoch = 0; epoch < maxEpoch; epoch++) {
    for (Batch batch : dataset) {
        Map<String, Float> metrics = trainer.trainRLStep(
            batch.getInputIds(), 
            batch.getTargetIds()
        );
        
        // æ‰“å°æŒ‡æ ‡
        System.out.printf("Epoch %d - Reward: %.3f, Quality: %.3f\n",
            epoch, metrics.get("total_reward"), metrics.get("avg_quality"));
    }
}
```

### 6.2 V3è®­ç»ƒç¤ºä¾‹

```java
// V3è®­ç»ƒå™¨ï¼ˆåŒ…å«MoEè´Ÿè½½å‡è¡¡ï¼‰
V3RLTrainer v3Trainer = new V3RLTrainer(maxEpoch, monitor, evaluator);

// è®¾ç½®è´Ÿè½½å‡è¡¡æƒé‡
v3Trainer.setLoadBalanceWeight(0.01f);

// è®­ç»ƒ
v3Trainer.init(dataset, deepseekV3Model, loss, optimizer);

for (Batch batch : dataset) {
    V3TrainingMetrics metrics = v3Trainer.trainStep(
        batch.getInputIds(),
        batch.getTargetIds(),
        batch.getTaskType()  // ä»»åŠ¡ç±»å‹æ„ŸçŸ¥
    );
    
    System.out.printf(
        "Reward: %.3f, Code Quality: %.3f, Load Balance Loss: %.4f\n",
        metrics.getTotalReward(),
        metrics.getCodeQualityReward(),
        metrics.getLoadBalanceLoss()
    );
}
```

## ä¸ƒã€æŠ€æœ¯çªç ´æ€»ç»“ ğŸš€

**æ ¸å¿ƒåˆ›æ–°**ï¼š

1. **å¤šç»´åº¦å¥–åŠ±**ï¼šä¸ä»…ä¼˜åŒ–å‡†ç¡®æ€§ï¼Œè¿˜ä¼˜åŒ–æ¨ç†è´¨é‡
2. **åæ€å¥–åŠ±**ï¼šé¼“åŠ±æ¨¡å‹è‡ªæˆ‘æ”¹è¿›
3. **è´Ÿè½½å‡è¡¡**ï¼šç¡®ä¿MoEä¸“å®¶å‡è¡¡ä½¿ç”¨
4. **ä»»åŠ¡æ„ŸçŸ¥**ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´å¥–åŠ±æƒé‡
5. **åŸºçº¿ä¼°è®¡**ï¼šå‡å°‘æ¢¯åº¦æ–¹å·®ï¼Œç¨³å®šè®­ç»ƒ

**ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”**ï¼š

| ç»´åº¦ | ç›‘ç£å­¦ä¹  | DeepSeekå¼ºåŒ–å­¦ä¹  |
|------|---------|-----------------|
| ä¼˜åŒ–ç›®æ ‡ | å•ä¸€æŸå¤±å‡½æ•° | å¤šç»´åº¦å¥–åŠ± |
| åé¦ˆæœºåˆ¶ | æ ‡æ³¨æ•°æ® | è‡ªæˆ‘è¯„ä¼°+ç¯å¢ƒåé¦ˆ |
| æ¢ç´¢èƒ½åŠ› | âŒ æ—  | âœ… ç†µæ­£åˆ™åŒ– |
| è‡ªæˆ‘æ”¹è¿› | âŒ æ—  | âœ… åæ€æœºåˆ¶ |
| è´Ÿè½½å‡è¡¡ | âŒ æ—  | âœ… KLæ•£åº¦æƒ©ç½š |

## å…«ã€æœ¬èŠ‚æ€»ç»“ ğŸ“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **å¼ºåŒ–å­¦ä¹ åŸºç¡€**ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•å’Œå¥–åŠ±é©±åŠ¨å­¦ä¹ 
2. **R1è®­ç»ƒ**ï¼šå¤šç»´åº¦å¥–åŠ±ï¼ˆå‡†ç¡®æ€§+æ¨ç†+åæ€+ä¸€è‡´æ€§ï¼‰
3. **V3è®­ç»ƒ**ï¼šé¢å¤–çš„ä»£ç è´¨é‡å’ŒMoEæ•ˆç‡å¥–åŠ±
4. **è´Ÿè½½å‡è¡¡**ï¼šKLæ•£åº¦æƒ©ç½šç¡®ä¿ä¸“å®¶ä½¿ç”¨å‡è¡¡
5. **è®­ç»ƒä¼˜åŒ–**ï¼šæ¢¯åº¦è£å‰ªã€ç†µæ­£åˆ™åŒ–ã€åŸºçº¿ä¼°è®¡

**è®¾è®¡ç†å¿µ**ï¼š
- å…¨é¢è¯„ä¼°èƒœäºå•ä¸€æŒ‡æ ‡
- è¿‡ç¨‹è´¨é‡ä¸ç»“æœè´¨é‡åŒç­‰é‡è¦
- å¹³è¡¡å¤šä¸ªä¼˜åŒ–ç›®æ ‡

## æ€è€ƒé¢˜ ğŸ’­

1. ä¸ºä»€ä¹ˆéœ€è¦å¤šç»´åº¦å¥–åŠ±è€Œä¸æ˜¯å•ä¸€å‡†ç¡®æ€§ï¼Ÿ
2. åŸºçº¿ä¼°è®¡å¦‚ä½•å‡å°‘è®­ç»ƒæ–¹å·®ï¼Ÿ
3. è´Ÿè½½å‡è¡¡æƒ©ç½šçš„ä½œç”¨æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ

## ä¸‹ä¸€èŠ‚é¢„å‘Š â­ï¸

**14.2.6 ç»¼åˆé¡¹ç›®ï¼šæ™ºèƒ½æ¨ç†ç³»ç»Ÿ**
- æ•´åˆæ‰€å­¦çŸ¥è¯†æ„å»ºå®Œæ•´ç³»ç»Ÿ
- ä»»åŠ¡è·¯ç”±ã€æ¨ç†ã€åæ€çš„åä½œ
- å®é™…åº”ç”¨åœºæ™¯æ¼”ç¤º

ğŸ‘‰ [ç»§ç»­å­¦ä¹ ï¼š14.2.6 ç»¼åˆé¡¹ç›®](./14.2.6-comprehensive-project.md)
