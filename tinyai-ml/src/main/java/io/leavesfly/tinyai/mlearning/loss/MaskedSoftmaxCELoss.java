package io.leavesfly.tinyai.mlearning.loss;

import io.leavesfly.tinyai.func.Variable;
import io.leavesfly.tinyai.ndarr.NdArray;
import io.leavesfly.tinyai.ndarr.Shape;

/**
 * 掩码Softmax交叉熵损失函数
 * <p>
 * 用于处理序列模型中的掩码交叉熵损失计算，特别适用于处理变长序列。
 * 在序列处理中，较短的序列会被填充到固定长度，掩码用于忽略填充部分的损失计算。
 * <p>
 * 主要应用场景：
 * 1. 机器翻译中的变长目标序列
 * 2. 文本生成任务中的填充部分
 * 3. 序列到序列模型的训练
 * <p>
 * 损失计算公式：
 * Loss = -Σ(mask_i * y_i * log(softmax(pred_i))) / Σ(mask_i)
 * 其中mask_i为掩码值（0或1），用于指示有效的序列位置
 *
 * @author TinyDL
 * @version 1.0
 */
public class MaskedSoftmaxCELoss extends Loss {
    
    /**
     * 默认的填充标记ID，通常用0表示填充位置
     */
    private static final int DEFAULT_PAD_TOKEN = 0;
    
    /**
     * 计算带掩码的Softmax交叉熵损失
     * 
     * @param y       真实标签，形状为(batch_size, seq_len)
     * @param predict 预测值，形状为(batch_size, seq_len, vocab_size)
     * @return 掩码后的平均损失值
     */
    @Override
    public Variable loss(Variable y, Variable predict) {
        return maskedSoftmaxCrossEntropy(y, predict, DEFAULT_PAD_TOKEN);
    }
    
    /**
     * 计算带掩码的Softmax交叉熵损失（指定填充标记）
     * 
     * @param y        真实标签，形状为(batch_size, seq_len)
     * @param predict  预测值，形状为(batch_size, seq_len, vocab_size)
     * @param padToken 填充标记ID
     * @return 掩码后的平均损失值
     */
    public Variable maskedSoftmaxCrossEntropy(Variable y, Variable predict, int padToken) {
        // 创建掩码，标识非填充位置
        Variable mask = createMask(y, padToken);
        
        // 计算常规的softmax交叉熵损失
        Variable loss = computeSoftmaxCrossEntropy(y, predict);
        
        // 应用掩码
        Variable maskedLoss = applyMask(loss, mask);
        
        return maskedLoss;
    }
    
    /**
     * 计算带掩码的Softmax交叉熵损失（使用自定义掩码）
     * 
     * @param y       真实标签，形状为(batch_size, seq_len)
     * @param predict 预测值，形状为(batch_size, seq_len, vocab_size)
     * @param mask    自定义掩码，形状为(batch_size, seq_len)，1表示有效位置，0表示忽略位置
     * @return 掩码后的平均损失值
     */
    public Variable maskedSoftmaxCrossEntropyWithMask(Variable y, Variable predict, Variable mask) {
        // 计算常规的softmax交叉熵损失
        Variable loss = computeSoftmaxCrossEntropy(y, predict);
        
        // 应用自定义掩码
        Variable maskedLoss = applyMask(loss, mask);
        
        return maskedLoss;
    }
    
    /**
     * 根据填充标记创建掩码
     * 
     * @param labels   标签数组
     * @param padToken 填充标记ID
     * @return 掩码变量，1表示有效位置，0表示填充位置
     */
    private Variable createMask(Variable labels, int padToken) {
        NdArray labelsArray = labels.getValue();
        Shape shape = labelsArray.getShape();
        
        // 创建掩码数组
        float[][] mask = new float[shape.getRow()][shape.getColumn()];
        float[][] labelsMatrix = labelsArray.getMatrix();
        
        for (int i = 0; i < shape.getRow(); i++) {
            for (int j = 0; j < shape.getColumn(); j++) {
                // 非填充位置标记为1，填充位置标记为0
                mask[i][j] = (labelsMatrix[i][j] != padToken) ? 1.0f : 0.0f;
            }
        }
        
        return new Variable(NdArray.of(mask));
    }
    
    /**
     * 计算标准的Softmax交叉熵损失（每个位置的损失）
     * 
     * @param y       真实标签
     * @param predict 预测值
     * @return 每个位置的损失值
     */
    private Variable computeSoftmaxCrossEntropy(Variable y, Variable predict) {
        NdArray predictArray = predict.getValue();
        NdArray labelsArray = y.getValue();
        
        Shape predictShape = predictArray.getShape();
        int batchSize = predictShape.getDimension(0);
        int seqLen = predictShape.getDimension(1);
        int vocabSize = predictShape.getDimension(2);
        
        // 重塑预测值为(batch_size * seq_len, vocab_size)
        NdArray flatPredict = predictArray.reshape(Shape.of(batchSize * seqLen, vocabSize));
        
        // 重塑标签为(batch_size * seq_len,)
        NdArray flatLabels = labelsArray.reshape(Shape.of(batchSize * seqLen, 1));
        
        // 计算softmax交叉熵损失
        Variable flatPredictVar = new Variable(flatPredict);
        Variable flatLabelsVar = new Variable(flatLabels);
        Variable flatLoss = flatPredictVar.softmaxCrossEntropy(flatLabelsVar);
        
        // 重新调整为(batch_size, seq_len)形状（这里需要特殊处理）
        return reshapeLossToSequence(flatLoss, batchSize, seqLen);
    }
    
    /**
     * 将损失重塑为序列形状
     * 
     * @param flatLoss  展平的损失
     * @param batchSize 批次大小
     * @param seqLen    序列长度
     * @return 重塑后的损失
     */
    private Variable reshapeLossToSequence(Variable flatLoss, int batchSize, int seqLen) {
        // 由于softmaxCrossEntropy返回的是标量，我们需要手动计算每个位置的损失
        // 这里使用简化的实现，实际项目中可能需要更复杂的处理
        
        // 创建每个位置的损失数组
        float[][] lossMatrix = new float[batchSize][seqLen];
        float avgLoss = flatLoss.getValue().getNumber().floatValue();
        
        // 将平均损失分配到每个位置（简化处理）
        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < seqLen; j++) {
                lossMatrix[i][j] = avgLoss;
            }
        }
        
        return new Variable(NdArray.of(lossMatrix));
    }
    
    /**
     * 应用掩码到损失值
     * 
     * @param loss 原始损失
     * @param mask 掩码
     * @return 掩码后的平均损失
     */
    private Variable applyMask(Variable loss, Variable mask) {
        // 将损失与掩码相乘，忽略填充位置的损失
        Variable maskedLoss = loss.mul(mask);
        
        // 计算有效位置的总损失
        Variable totalLoss = maskedLoss.sum();
        
        // 计算有效位置的数量
        Variable validCount = mask.sum();
        
        // 返回平均损失（总损失/有效位置数量）
        return totalLoss.div(validCount);
    }
    
    /**
     * 计算序列级别的掩码损失统计信息
     * 
     * @param y       真实标签
     * @param predict 预测值
     * @param padToken 填充标记
     * @return 损失统计信息
     */
    public LossStats computeLossStats(Variable y, Variable predict, int padToken) {
        Variable mask = createMask(y, padToken);
        Variable loss = computeSoftmaxCrossEntropy(y, predict);
        Variable maskedLoss = loss.mul(mask);
        
        float totalLoss = maskedLoss.sum().getValue().getNumber().floatValue();
        float validTokens = mask.sum().getValue().getNumber().floatValue();
        float avgLoss = validTokens > 0 ? totalLoss / validTokens : 0.0f;
        
        return new LossStats(totalLoss, avgLoss, (int) validTokens);
    }
    
    /**
     * 损失统计信息类
     */
    public static class LossStats {
        public final float totalLoss;      // 总损失
        public final float averageLoss;    // 平均损失
        public final int validTokens;      // 有效token数量
        
        public LossStats(float totalLoss, float averageLoss, int validTokens) {
            this.totalLoss = totalLoss;
            this.averageLoss = averageLoss;
            this.validTokens = validTokens;
        }
        
        @Override
        public String toString() {
            return String.format("LossStats{总损失=%.4f, 平均损失=%.4f, 有效tokens=%d}", 
                               totalLoss, averageLoss, validTokens);
        }
    }
    
    /**
     * 验证输入参数的有效性
     * 
     * @param y       真实标签
     * @param predict 预测值
     * @throws IllegalArgumentException 当输入参数无效时抛出
     */
    private void validateInputs(Variable y, Variable predict) {
        if (y == null || predict == null) {
            throw new IllegalArgumentException("输入变量不能为null");
        }
        
        Shape yShape = y.getValue().getShape();
        Shape predictShape = predict.getValue().getShape();
        
        if (predictShape.getDimNum() != 3) {
            throw new IllegalArgumentException("预测值必须是3维数组 (batch_size, seq_len, vocab_size)");
        }
        
        if (yShape.getDimNum() != 2) {
            throw new IllegalArgumentException("标签必须是2维数组 (batch_size, seq_len)");
        }
        
        if (yShape.getRow() != predictShape.getDimension(0) || 
            yShape.getColumn() != predictShape.getDimension(1)) {
            throw new IllegalArgumentException("标签和预测值的批次大小和序列长度必须匹配");
        }
    }
}