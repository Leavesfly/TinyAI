# 14.4 因果语言建模：自回归生成原理

> **设计思想**：深入理解因果语言建模的数学原理和实现方法，掌握自回归生成的核心技术

## 本节概述

因果语言建模（Causal Language Modeling）是GPT系列模型的核心训练目标，它通过自回归的方式预测序列中的下一个token。与掩码语言建模不同，因果语言建模只使用历史信息来预测未来，这使得模型能够自然地生成连贯的文本序列。

本节将深入探讨因果语言建模的数学原理、实现细节、训练策略以及自回归生成的技术要点。

## 学习目标

完成本节学习后，你将：

- ✅ **掌握因果语言建模的数学原理**：理解自回归生成的概率模型
- ✅ **学会因果掩码的实现**：掌握防止位置关注后续位置的技术
- ✅ **理解训练目标的设计**：掌握最大似然估计在语言建模中的应用
- ✅ **掌握自回归生成过程**：理解逐步生成和概率计算的实现
- ✅ **具备因果语言模型实现能力**：能够编写完整的因果语言模型代码

## 因果语言建模的数学原理

### 概率模型

因果语言建模基于以下概率模型：

```
P(x_1, x_2, ..., x_T) = Π_{t=1}^T P(x_t | x_1, x_2, ..., x_{t-1})
```

其中：
- x_t 是序列中第t个位置的token
- P(x_t | x_1, x_2, ..., x_{t-1}) 是给定历史token预测当前token的条件概率

### 最大似然估计

训练目标是最小化负对数似然损失：

```
L = -Σ_{t=1}^T log P(x_t | x_1, x_2, ..., x_{t-1})
```

### 条件概率建模

在神经网络实现中，条件概率通过softmax函数建模：

```
P(x_t = v | x_1, ..., x_{t-1}) = softmax(f(x_1, ..., x_{t-1}))_v
```

其中f是神经网络函数，v是词汇表中的token。

## 因果掩码的实现

### 掩码设计原理

因果掩码（Causal Mask）确保每个位置只能关注到它之前的位置，防止信息泄露：

```java
public class CausalMask {
    public Variable createCausalMask(int seqLength) {
        // 创建上三角矩阵，对角线及以下为0，以上为负无穷
        float[][] mask = new float[seqLength][seqLength];
        
        for (int i = 0; i < seqLength; i++) {
            for (int j = 0; j < seqLength; j++) {
                if (j > i) {
                    mask[i][j] = Float.NEGATIVE_INFINITY;  // 阻止关注未来位置
                } else {
                    mask[i][j] = 0.0f;  // 允许关注当前位置及之前位置
                }
            }
        }
        
        return new Variable(NdArray.of(mask));
    }
    
    public Variable createCausalMaskWithPadding(Variable attentionMask) {
        // attentionMask: (batch, seqLen) - 1表示有效token，0表示填充
        int batchSize = attentionMask.getShape().get(0);
        int seqLength = attentionMask.getShape().get(1);
        
        // 创建因果掩码
        Variable causalMask = createCausalMask(seqLength);
        
        // 扩展维度以匹配注意力权重
        causalMask = causalMask.unsqueeze(0).unsqueeze(0);  // (1, 1, seqLen, seqLen)
        causalMask = causalMask.expand(batchSize, 1, seqLength, seqLength);
        
        // 创建填充掩码
        Variable paddingMask = createPaddingMask(attentionMask);  // (batch, 1, 1, seqLen)
        paddingMask = paddingMask.expand(batchSize, 1, seqLength, seqLength);
        
        // 合并掩码
        return causalMask.add(paddingMask);
    }
    
    private Variable createPaddingMask(Variable attentionMask) {
        // 将0转换为负无穷，1保持为0
        return attentionMask.sub(1.0f).mul(Float.POSITIVE_INFINITY);
    }
}
```

### 掩码在注意力中的应用

```java
public class CausalAttention {
    private MultiHeadAttention attention;
    private CausalMask causalMaskGenerator;
    
    public Variable forward(Variable hiddenStates, Variable attentionMask) {
        // 创建因果掩码
        Variable causalMask = causalMaskGenerator
            .createCausalMaskWithPadding(attentionMask);
        
        // 应用注意力机制
        return attention.forward(
            hiddenStates,  // Query
            hiddenStates,  // Key
            hiddenStates,  // Value
            causalMask     // 因果掩码
        );
    }
}
```

## 训练目标实现

### 损失函数设计

```java
public class CausalLanguageModelingLoss {
    private boolean useLabelSmoothing;
    private double smoothingFactor;
    
    public Variable computeLoss(Variable logits, Variable labels) {
        // logits: (batch, seqLen, vocabSize)
        // labels: (batch, seqLen)
        
        int batchSize = logits.getShape().get(0);
        int seqLength = logits.getShape().get(1);
        int vocabSize = logits.getShape().get(2);
        
        // 重塑为二维矩阵
        Variable reshapedLogits = logits.reshape(-1, vocabSize);  // (batch*seqLen, vocabSize)
        Variable reshapedLabels = labels.reshape(-1);             // (batch*seqLen)
        
        if (useLabelSmoothing) {
            return computeLabelSmoothedLoss(reshapedLogits, reshapedLabels);
        } else {
            return computeCrossEntropyLoss(reshapedLogits, reshapedLabels);
        }
    }
    
    private Variable computeCrossEntropyLoss(Variable logits, Variable labels) {
        // 计算交叉熵损失
        return crossEntropyLoss(logits, labels);
    }
    
    private Variable computeLabelSmoothedLoss(Variable logits, Variable labels) {
        // 实现标签平滑
        int vocabSize = logits.getShape().get(-1);
        Variable logProbs = logits.logSoftmax(-1);
        
        // 创建平滑标签
        Variable oneHot = oneHot(labels, vocabSize);
        Variable smoothLabels = oneHot.mul(1.0 - smoothingFactor)
                                  .add(smoothingFactor / vocabSize);
        
        // 计算平滑损失
        Variable loss = logProbs.mul(smoothLabels).sum(-1).neg();
        return loss.mean();
    }
}
```

### 训练循环实现

```java
public class CausalLanguageModelTrainer {
    private CausalLanguageModel model;
    private CausalLanguageModelingLoss lossFunction;
    private Optimizer optimizer;
    private DataLoader dataLoader;
    
    public void train(int epochs) {
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalLoss = 0.0;
            int batchCount = 0;
            
            for (Batch batch : dataLoader) {
                // 获取输入和标签
                Variable inputIds = batch.getInputIds();      // (batch, seqLen)
                Variable labels = batch.getLabels();          // (batch, seqLen)
                Variable attentionMask = batch.getAttentionMask(); // (batch, seqLen)
                
                // 前向传播
                Variable logits = model.forward(inputIds, attentionMask);
                
                // 计算损失
                Variable loss = lossFunction.computeLoss(logits, labels);
                
                // 反向传播
                loss.backward();
                
                // 梯度裁剪
                clipGradients(model.getParameters(), 1.0);
                
                // 优化步骤
                optimizer.step();
                optimizer.zeroGrad();
                
                totalLoss += loss.getData().getFloat();
                batchCount++;
                
                // 定期记录日志
                if (batchCount % 100 == 0) {
                    System.out.printf("Epoch %d, Batch %d, Loss: %.4f%n", 
                                    epoch, batchCount, totalLoss / batchCount);
                }
            }
            
            System.out.printf("Epoch %d completed. Average Loss: %.4f%n", 
                            epoch, totalLoss / batchCount);
        }
    }
}
```

## 自回归生成过程

### 逐步生成算法

```java
public class AutoregressiveGenerator {
    private CausalLanguageModel model;
    private Tokenizer tokenizer;
    
    public String generate(String prompt, GenerationConfig config) {
        // 1. 编码提示文本
        int[] inputIds = tokenizer.encode(prompt);
        List<Integer> generatedTokens = new ArrayList<>();
        
        // 将输入token添加到生成列表
        for (int tokenId : inputIds) {
            generatedTokens.add(tokenId);
        }
        
        // 2. 逐步生成
        for (int i = 0; i < config.getMaxTokens(); i++) {
            // 构建当前输入
            int[] currentInput = generatedTokens.stream()
                .mapToInt(Integer::intValue)
                .toArray();
            
            // 如果超过最大序列长度，移除最早的token
            if (currentInput.length > model.getConfig().getMaxPositionEmbeddings()) {
                currentInput = Arrays.copyOfRange(
                    currentInput, 
                    currentInput.length - model.getConfig().getMaxPositionEmbeddings(),
                    currentInput.length
                );
            }
            
            // 前向传播
            Variable inputTensor = new Variable(NdArray.of(new int[][]{currentInput}));
            Variable logits = model.forward(inputTensor);
            
            // 获取最后一个位置的logits
            Variable lastLogits = logits.slice(-1);
            
            // 根据配置选择下一个token
            int nextToken = selectNextToken(lastLogits, config);
            generatedTokens.add(nextToken);
            
            // 检查停止条件
            if (nextToken == tokenizer.getEosTokenId() || 
                isStopSequence(generatedTokens, config.getStopSequences())) {
                break;
            }
        }
        
        // 3. 解码生成的token
        int[] outputTokens = generatedTokens.stream()
            .mapToInt(Integer::intValue)
            .toArray();
            
        return tokenizer.decode(outputTokens);
    }
    
    private int selectNextToken(Variable logits, GenerationConfig config) {
        switch (config.getSamplingStrategy()) {
            case GREEDY:
                return greedySampling(logits);
            case TOP_K:
                return topKSampling(logits, config.getTopK());
            case TOP_P:
                return nucleusSampling(logits, config.getTopP());
            case TEMPERATURE:
                return temperatureSampling(logits, config.getTemperature());
            default:
                return greedySampling(logits);
        }
    }
}
```

### 贪心采样

```java
private int greedySampling(Variable logits) {
    // 选择概率最高的token
    Variable probs = logits.softmax(-1);
    return probs.argmax(-1).getData().toInt();
}
```

### 温度采样

```java
private int temperatureSampling(Variable logits, double temperature) {
    // 应用温度参数调整分布
    Variable adjustedLogits = logits.div(temperature);
    Variable probs = adjustedLogits.softmax(-1);
    
    // 从调整后的分布中采样
    return sampleFromDistribution(probs);
}
```

### Top-K采样

```java
private int topKSampling(Variable logits, int k) {
    // 获取top-k的logits
    Variable topKLogits = getTopKLogits(logits, k);
    Variable probs = topKLogits.softmax(-1);
    
    // 从top-k分布中采样
    return sampleFromDistribution(probs);
}

private Variable getTopKLogits(Variable logits, int k) {
    // 对logits进行排序，保留top-k
    Variable sortedIndices = logits.argsort(-1, true);  // 降序排列
    Variable topKIndices = sortedIndices.slice(0, k);
    
    // 创建新的logits，只保留top-k位置的值
    Variable topKLogits = new Variable(logits.getShape());
    topKLogits = topKLogits.fill(Float.NEGATIVE_INFINITY);
    
    // 将top-k位置的值填充回去
    for (int i = 0; i < k; i++) {
        int index = topKIndices.getData().toIntArray()[i];
        topKLogits = topKLogits.setSlice(
            new int[]{0, index}, 
            new int[]{1, index + 1}, 
            logits.slice(0, 1).slice(index, index + 1)
        );
    }
    
    return topKLogits;
}
```

### 核采样（Nucleus Sampling）

```java
private int nucleusSampling(Variable logits, double p) {
    // 计算累积概率
    Variable probs = logits.softmax(-1);
    Variable sortedProbs = probs.sort(-1, true);  // 降序排列
    Variable cumulativeProbs = sortedProbs.cumsum(-1);
    
    // 找到累积概率超过p的最小集合
    Variable mask = cumulativeProbs.gt(p);
    Variable filteredProbs = sortedProbs.where(mask, 0.0f);
    
    // 重新归一化
    Variable normalizedProbs = filteredProbs.div(filteredProbs.sum(-1, true));
    
    // 采样
    return sampleFromDistribution(normalizedProbs);
}
```

## 因果语言模型实现

### 模型架构

```java
public class CausalLanguageModel extends Model {
    private CausalLMConfig config;
    private EmbeddingLayer tokenEmbedding;
    private EmbeddingLayer positionEmbedding;
    private List<CausalTransformerBlock> transformerBlocks;
    private LayerNormalization finalLayerNorm;
    private LinearLayer lmHead;
    
    public CausalLanguageModel(CausalLMConfig config) {
        super("CausalLanguageModel");
        this.config = config;
        
        // 词嵌入层
        this.tokenEmbedding = new EmbeddingLayer(
            "token_embedding",
            config.getVocabSize(),
            config.getHiddenSize()
        );
        
        // 位置嵌入层
        this.positionEmbedding = new EmbeddingLayer(
            "position_embedding",
            config.getMaxPositionEmbeddings(),
            config.getHiddenSize()
        );
        
        // Transformer块
        this.transformerBlocks = new ArrayList<>();
        for (int i = 0; i < config.getNumLayers(); i++) {
            transformerBlocks.add(new CausalTransformerBlock(
                "block_" + i, config
            ));
        }
        
        // 最终层归一化
        this.finalLayerNorm = new LayerNormalization(
            "final_layer_norm",
            config.getHiddenSize(),
            config.getLayerNormEpsilon()
        );
        
        // 语言模型头部
        this.lmHead = new LinearLayer(
            "lm_head",
            config.getHiddenSize(),
            config.getVocabSize()
        );
        
        // 注意：在某些实现中，lmHead的权重可能与tokenEmbedding共享
        if (config.isTieWordEmbeddings()) {
            this.lmHead.setWeight(tokenEmbedding.getWeight().transpose());
        }
    }
    
    public Variable forward(Variable inputIds) {
        return forward(inputIds, null);
    }
    
    public Variable forward(Variable inputIds, Variable attentionMask) {
        int batchSize = inputIds.getShape().get(0);
        int seqLength = inputIds.getShape().get(1);
        
        // 创建位置ID
        Variable positionIds = createPositionIds(inputIds);
        
        // 词嵌入和位置嵌入
        Variable hiddenStates = tokenEmbedding.forward(inputIds);
        Variable positionEmbeds = positionEmbedding.forward(positionIds);
        hiddenStates = hiddenStates.add(positionEmbeds);
        
        // 应用嵌入层Dropout
        hiddenStates = new Dropout("embedding_dropout", config.getEmbeddingDropout())
                      .forward(hiddenStates);
        
        // 逐层处理
        for (CausalTransformerBlock block : transformerBlocks) {
            hiddenStates = block.forward(hiddenStates, attentionMask);
        }
        
        // 最终层归一化
        hiddenStates = finalLayerNorm.forward(hiddenStates);
        
        // 语言模型头部
        Variable logits = lmHead.forward(hiddenStates);
        
        return logits;
    }
    
    private Variable createPositionIds(Variable inputIds) {
        int batchSize = inputIds.getShape().get(0);
        int seqLength = inputIds.getShape().get(1);
        
        int[][] positionIds = new int[batchSize][seqLength];
        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < seqLength; j++) {
                positionIds[i][j] = j;
            }
        }
        
        return new Variable(NdArray.of(positionIds));
    }
}
```

### Transformer块实现

```java
public class CausalTransformerBlock extends Layer {
    private CausalLMConfig config;
    private LayerNormalization attentionLayerNorm;
    private CausalAttention selfAttention;
    private Dropout attentionDropout;
    private LayerNormalization feedForwardLayerNorm;
    private PositionwiseFeedForward feedForward;
    private Dropout feedForwardDropout;
    
    public CausalTransformerBlock(String name, CausalLMConfig config) {
        super(name);
        this.config = config;
        
        // 注意力层归一化
        this.attentionLayerNorm = new LayerNormalization(
            "attention_layer_norm",
            config.getHiddenSize(),
            config.getLayerNormEpsilon()
        );
        
        // 因果注意力
        this.selfAttention = new CausalAttention(
            "self_attention",
            config.getNumHeads(),
            config.getHiddenSize(),
            config.getAttentionDropout()
        );
        
        // 注意力Dropout
        this.attentionDropout = new Dropout(
            "attention_dropout",
            config.getAttentionDropout()
        );
        
        // 前馈网络层归一化
        this.feedForwardLayerNorm = new LayerNormalization(
            "feed_forward_layer_norm",
            config.getHiddenSize(),
            config.getLayerNormEpsilon()
        );
        
        // 前馈网络
        this.feedForward = new PositionwiseFeedForward(
            "feed_forward",
            config.getHiddenSize(),
            config.getIntermediateSize(),
            config.getHiddenDropout()
        );
        
        // 前馈网络Dropout
        this.feedForwardDropout = new Dropout(
            "feed_forward_dropout",
            config.getHiddenDropout()
        );
    }
    
    public Variable forward(Variable hiddenStates) {
        return forward(hiddenStates, null);
    }
    
    public Variable forward(Variable hiddenStates, Variable attentionMask) {
        // 自注意力块
        Variable attentionInput = attentionLayerNorm.forward(hiddenStates);
        Variable attentionOutput = selfAttention.forward(
            attentionInput, attentionMask
        );
        attentionOutput = attentionDropout.forward(attentionOutput);
        hiddenStates = hiddenStates.add(attentionOutput);
        
        // 前馈网络块
        Variable feedForwardInput = feedForwardLayerNorm.forward(hiddenStates);
        Variable feedForwardOutput = feedForward.forward(feedForwardInput);
        feedForwardOutput = feedForwardDropout.forward(feedForwardOutput);
        hiddenStates = hiddenStates.add(feedForwardOutput);
        
        return hiddenStates;
    }
}
```

## 性能优化技术

### 批量生成优化

```java
public class BatchedGenerator {
    public List<String> batchGenerate(List<String> prompts, 
                                    GenerationConfig config) {
        // 1. 批量编码
        List<int[]> inputIdsList = new ArrayList<>();
        for (String prompt : prompts) {
            inputIdsList.add(tokenizer.encode(prompt));
        }
        
        // 2. 动态批处理
        List<String> results = new ArrayList<>();
        int batchSize = config.getBatchSize();
        
        for (int i = 0; i < inputIdsList.size(); i += batchSize) {
            int end = Math.min(i + batchSize, inputIdsList.size());
            List<int[]> batchInputs = inputIdsList.subList(i, end);
            
            // 3. 批量生成
            List<String> batchResults = generateBatch(batchInputs, config);
            results.addAll(batchResults);
        }
        
        return results;
    }
    
    private List<String> generateBatch(List<int[]> batchInputs, 
                                     GenerationConfig config) {
        // 实现批量生成逻辑
        // ...
        return new ArrayList<>();
    }
}
```

## 本节小结

本节深入探讨了因果语言建模的数学原理和实现技术，我们学习了：

1. **因果语言建模的数学原理**：理解了自回归生成的概率模型和最大似然估计
2. **因果掩码的实现**：掌握了防止位置关注后续位置的技术
3. **训练目标的设计**：学会了交叉熵损失和标签平滑的实现
4. **自回归生成过程**：掌握了逐步生成和多种采样策略
5. **因果语言模型实现**：具备了完整的因果语言模型代码实现能力

因果语言建模是GPT系列模型的核心技术，它通过自回归的方式使模型能够生成连贯、自然的文本序列。理解这一技术对于开发和应用大语言模型具有重要意义。

在下一节中，我们将学习温度采样与Top-k采样策略，深入理解文本生成的控制技术。