# TinyAI Reinforcement Learning å¼ºåŒ–å­¦ä¹ æ¨¡å— (tinyai-dl-rl)

## æ¨¡å—æ¦‚è¿°

`tinyai-dl-rl` æ˜¯ TinyAI æ·±åº¦å­¦ä¹ æ¡†æ¶çš„å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¨¡å—ï¼Œæä¾›äº†å®Œæ•´çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°å’Œç¯å¢ƒç®¡ç†åŠŸèƒ½ã€‚æœ¬æ¨¡å—å®ç°äº†ä»ç»å…¸çš„å¤šè‡‚è€è™æœºé—®é¢˜åˆ°ç°ä»£æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å…¨å¥—è§£å†³æ–¹æ¡ˆï¼Œæ˜¯æ„å»ºæ™ºèƒ½å†³ç­–ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ã€‚

## æ ¸å¿ƒæ¶æ„

### è®¾è®¡ç†å¿µ

æœ¬æ¨¡å—é‡‡ç”¨æ ‡å‡†çš„å¼ºåŒ–å­¦ä¹ æ¶æ„è®¾è®¡ï¼Œéµå¾ª OpenAI Gym æ¥å£è§„èŒƒï¼Œé€šè¿‡æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’æ¡†æ¶æ„å»ºå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼š

- **Agentï¼ˆæ™ºèƒ½ä½“ï¼‰**ï¼šå†³ç­–åˆ¶å®šè€…ï¼Œè´Ÿè´£é€‰æ‹©åŠ¨ä½œå’Œå­¦ä¹ ç­–ç•¥
- **Environmentï¼ˆç¯å¢ƒï¼‰**ï¼šäº¤äº’å¯¹è±¡ï¼Œæä¾›çŠ¶æ€è½¬ç§»å’Œå¥–åŠ±ä¿¡å·
- **Policyï¼ˆç­–ç•¥ï¼‰**ï¼šåŠ¨ä½œé€‰æ‹©æœºåˆ¶ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
- **Experienceï¼ˆç»éªŒï¼‰**ï¼šäº¤äº’è®°å½•ï¼Œæ”¯æŒç»éªŒå›æ”¾å­¦ä¹ 
- **ReplayBufferï¼ˆç»éªŒç¼“å†²åŒºï¼‰**ï¼šç»éªŒå­˜å‚¨å’Œé‡‡æ ·ç®¡ç†

```mermaid
graph TB
    subgraph "å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¶æ„"
        Agent[Agent æ™ºèƒ½ä½“]
        Environment[Environment ç¯å¢ƒ]
        Policy[Policy ç­–ç•¥]
        Experience[Experience ç»éªŒ]
        ReplayBuffer[ReplayBuffer ç»éªŒç¼“å†²åŒº]
    end
    
    subgraph "æ™ºèƒ½ä½“å®ç°"
        DQNAgent[DQNAgent æ·±åº¦Qç½‘ç»œ]
        REINFORCEAgent[REINFORCEAgent ç­–ç•¥æ¢¯åº¦]
        BanditAgent[BanditAgent å¤šè‡‚è€è™æœº]
        EpsilonGreedyBandit[EpsilonGreedyBanditAgent Îµ-è´ªå¿ƒè€è™æœº]
        UCBBandit[UCBBanditAgent UCBè€è™æœº]
        ThompsonBandit[ThompsonSamplingBanditAgent æ±¤æ™®æ£®é‡‡æ ·]
    end
    
    subgraph "ç¯å¢ƒå®ç°"
        CartPole[CartPoleEnvironment å€’ç«‹æ‘†]
        GridWorld[GridWorldEnvironment ç½‘æ ¼ä¸–ç•Œ]
        MultiArmedBandit[MultiArmedBanditEnvironment å¤šè‡‚è€è™æœº]
    end
    
    subgraph "ç­–ç•¥å®ç°"
        EpsilonGreedy[EpsilonGreedyPolicy Îµ-è´ªå¿ƒç­–ç•¥]
    end
    
    Agent --> Policy
    Agent --> ReplayBuffer
    Agent --> Experience
    Environment --> Experience
    
    Agent <--> Environment
    
    DQNAgent --> Agent
    REINFORCEAgent --> Agent
    BanditAgent --> Agent
    EpsilonGreedyBandit --> BanditAgent
    UCBBandit --> BanditAgent
    ThompsonBandit --> BanditAgent
    
    CartPole --> Environment
    GridWorld --> Environment
    MultiArmedBandit --> Environment
    
    EpsilonGreedy --> Policy
```

### æ ¸å¿ƒç»„ä»¶

#### 1. åŸºç¡€æŠ½è±¡ç±»
- [`Agent`](src/main/java/io/leavesfly/tinyai/rl/Agent.java) - æ™ºèƒ½ä½“æŠ½è±¡åŸºç±»
- [`Environment`](src/main/java/io/leavesfly/tinyai/rl/Environment.java) - ç¯å¢ƒæŠ½è±¡åŸºç±»
- [`Policy`](src/main/java/io/leavesfly/tinyai/rl/Policy.java) - ç­–ç•¥æŠ½è±¡åŸºç±»
- [`Experience`](src/main/java/io/leavesfly/tinyai/rl/Experience.java) - ç»éªŒæ•°æ®ç»“æ„
- [`ReplayBuffer`](src/main/java/io/leavesfly/tinyai/rl/ReplayBuffer.java) - ç»éªŒå›æ”¾ç¼“å†²åŒº

## åŠŸèƒ½ç‰¹æ€§

### ğŸ¤– å¤šæ ·åŒ–æ™ºèƒ½ä½“ç®—æ³•

#### æ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ (agent)

##### DQN æ·±åº¦Qç½‘ç»œ
[`DQNAgent`](src/main/java/io/leavesfly/tinyai/rl/agent/DQNAgent.java) - æ·±åº¦Qç½‘ç»œç®—æ³•å®ç°

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ä½¿ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘Qå‡½æ•°
- ç»éªŒå›æ”¾æœºåˆ¶æé«˜æ•°æ®åˆ©ç”¨ç‡  
- ç›®æ ‡ç½‘ç»œç¨³å®šè®­ç»ƒè¿‡ç¨‹
- Îµ-è´ªå©ªç­–ç•¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨

```java
// åˆ›å»ºDQNæ™ºèƒ½ä½“
DQNAgent dqnAgent = new DQNAgent(
    "CartPole-DQN",     // æ™ºèƒ½ä½“åç§°
    4,                  // çŠ¶æ€ç©ºé—´ç»´åº¦
    2,                  // åŠ¨ä½œç©ºé—´ç»´åº¦  
    new int[]{128, 128}, // éšè—å±‚å°ºå¯¸
    0.001f,             // å­¦ä¹ ç‡
    1.0f,               // åˆå§‹æ¢ç´¢ç‡
    0.99f,              // æŠ˜æ‰£å› å­
    32,                 // æ‰¹æ¬¡å¤§å°
    10000,              // ç»éªŒç¼“å†²åŒºå¤§å°
    100                 // ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
);
```

##### REINFORCE ç­–ç•¥æ¢¯åº¦
[`REINFORCEAgent`](src/main/java/io/leavesfly/tinyai/rl/agent/REINFORCEAgent.java) - ç­–ç•¥æ¢¯åº¦ç®—æ³•å®ç°

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ç›´æ¥ä¼˜åŒ–ç­–ç•¥å‡½æ•°
- æ”¯æŒè¿ç»­å’Œç¦»æ•£åŠ¨ä½œç©ºé—´
- åŸºçº¿å‡½æ•°å‡å°‘æ–¹å·®
- è’™ç‰¹å¡æ´›é‡‡æ ·ä¼°è®¡æ¢¯åº¦

#### å¤šè‡‚è€è™æœºæ™ºèƒ½ä½“

##### åŸºç¡€è€è™æœºæ™ºèƒ½ä½“
[`BanditAgent`](src/main/java/io/leavesfly/tinyai/rl/agent/BanditAgent.java) - å¤šè‡‚è€è™æœºåŸºç±»

##### Îµ-è´ªå¿ƒç­–ç•¥
[`EpsilonGreedyBanditAgent`](src/main/java/io/leavesfly/tinyai/rl/agent/EpsilonGreedyBanditAgent.java) - Îµ-è´ªå¿ƒè€è™æœº

**ç®—æ³•åŸç†ï¼š**
- ä»¥æ¦‚ç‡ Îµ éšæœºæ¢ç´¢
- ä»¥æ¦‚ç‡ (1-Îµ) é€‰æ‹©å½“å‰æœ€ä¼˜è‡‚
- ç®€å•æœ‰æ•ˆçš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ç­–ç•¥

##### UCB ä¸Šç½®ä¿¡åŒºé—´
[`UCBBanditAgent`](src/main/java/io/leavesfly/tinyai/rl/agent/UCBBanditAgent.java) - UCBç®—æ³•å®ç°

**ç®—æ³•åŸç†ï¼š**
- åŸºäºä¸Šç½®ä¿¡åŒºé—´çš„é€‰æ‹©ç­–ç•¥
- è€ƒè™‘å‡å€¼ä¼°è®¡å’Œä¸ç¡®å®šæ€§
- ç†è®ºä¸Šæœ‰æœ€ä¼˜çš„é—æ†¾ç•Œé™

##### æ±¤æ™®æ£®é‡‡æ ·
[`ThompsonSamplingBanditAgent`](src/main/java/io/leavesfly/tinyai/rl/agent/ThompsonSamplingBanditAgent.java) - è´å¶æ–¯é‡‡æ ·ç®—æ³•

**ç®—æ³•åŸç†ï¼š**
- åŸºäºè´å¶æ–¯æ¨ç†çš„é‡‡æ ·ç­–ç•¥
- ç»´æŠ¤æ¯ä¸ªè‡‚çš„åéªŒåˆ†å¸ƒ
- æ ¹æ®åéªŒåˆ†å¸ƒé‡‡æ ·è¿›è¡Œå†³ç­–

### ğŸŒ å¤šæ ·åŒ–ç¯å¢ƒå®ç°

#### ç»å…¸æ§åˆ¶ç¯å¢ƒ

##### CartPole å€’ç«‹æ‘†ç¯å¢ƒ
[`CartPoleEnvironment`](src/main/java/io/leavesfly/tinyai/rl/environment/CartPoleEnvironment.java) - ç»å…¸æ§åˆ¶é—®é¢˜

**ç¯å¢ƒç‰¹æ€§ï¼š**
- 4ç»´è¿ç»­çŠ¶æ€ç©ºé—´ï¼ˆä½ç½®ã€é€Ÿåº¦ã€è§’åº¦ã€è§’é€Ÿåº¦ï¼‰
- 2ç»´ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆå·¦æ¨ã€å³æ¨ï¼‰
- ç›®æ ‡ï¼šä¿æŒæ†å­å¹³è¡¡å°½å¯èƒ½é•¿æ—¶é—´
- é€‚åˆæµ‹è¯•æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•

```java
// åˆ›å»ºCartPoleç¯å¢ƒ
CartPoleEnvironment env = new CartPoleEnvironment();
Variable state = env.reset();

// ç¯å¢ƒäº¤äº’å¾ªç¯
while (!env.isDone()) {
    Variable action = agent.selectAction(state);
    Environment.StepResult result = env.step(action);
    
    agent.learn(new Experience(state, action, result.getNextState(), 
                              result.getReward(), result.isDone()));
    state = result.getNextState();
}
```

##### GridWorld ç½‘æ ¼ä¸–ç•Œç¯å¢ƒ
[`GridWorldEnvironment`](src/main/java/io/leavesfly/tinyai/rl/environment/GridWorldEnvironment.java) - ç¦»æ•£çŠ¶æ€ç©ºé—´ç¯å¢ƒ

**ç¯å¢ƒç‰¹æ€§ï¼š**
- ç¦»æ•£ç½‘æ ¼çŠ¶æ€ç©ºé—´
- 4æ–¹å‘ç§»åŠ¨åŠ¨ä½œï¼ˆä¸Šã€ä¸‹ã€å·¦ã€å³ï¼‰
- å¯é…ç½®å¥–åŠ±å’Œéšœç¢ç‰©
- é€‚åˆæµ‹è¯•åŸºç¡€å¼ºåŒ–å­¦ä¹ ç®—æ³•

##### MultiArmedBandit å¤šè‡‚è€è™æœºç¯å¢ƒ
[`MultiArmedBanditEnvironment`](src/main/java/io/leavesfly/tinyai/rl/environment/MultiArmedBanditEnvironment.java) - ç»å…¸å†³ç­–é—®é¢˜

**ç¯å¢ƒç‰¹æ€§ï¼š**
- å¤šä¸ªè€è™æœºè‡‚ï¼ˆåŠ¨ä½œé€‰æ‹©ï¼‰
- æ¯ä¸ªè‡‚æœ‰ä¸åŒçš„å¥–åŠ±åˆ†å¸ƒ
- æ¢ç´¢-åˆ©ç”¨æƒè¡¡çš„å…¸å‹åœºæ™¯
- é€‚åˆæµ‹è¯•è€è™æœºç®—æ³•

### ğŸ¯ ç­–ç•¥æœºåˆ¶ (policy)

#### Îµ-è´ªå¿ƒç­–ç•¥
[`EpsilonGreedyPolicy`](src/main/java/io/leavesfly/tinyai/rl/policy/EpsilonGreedyPolicy.java) - ç»å…¸æ¢ç´¢ç­–ç•¥

**ç­–ç•¥ç‰¹æ€§ï¼š**
- å¯é…ç½®çš„æ¢ç´¢ç‡ Îµ
- è‡ªåŠ¨æ¢ç´¢ç‡è¡°å‡
- æ”¯æŒä¸åŒçš„è¡°å‡ç­–ç•¥
- ç®€å•é«˜æ•ˆçš„å®ç°

```java
// åˆ›å»ºÎµ-è´ªå¿ƒç­–ç•¥
EpsilonGreedyPolicy policy = new EpsilonGreedyPolicy(
    stateDim, actionDim, 0.1f,  // çŠ¶æ€ç»´åº¦ã€åŠ¨ä½œç»´åº¦ã€æ¢ç´¢ç‡
    state -> model.forward(state) // Qå€¼å‡½æ•°
);

// é€‰æ‹©åŠ¨ä½œ
Variable action = policy.selectAction(currentState);
```

### ğŸ’¾ ç»éªŒç®¡ç†ç³»ç»Ÿ

#### ç»éªŒå›æ”¾ç¼“å†²åŒº
[`ReplayBuffer`](src/main/java/io/leavesfly/tinyai/rl/ReplayBuffer.java) - é«˜æ•ˆçš„ç»éªŒå­˜å‚¨å’Œé‡‡æ ·

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- å›ºå®šå¤§å°çš„å¾ªç¯ç¼“å†²åŒº
- éšæœºé‡‡æ ·é˜²æ­¢æ•°æ®ç›¸å…³æ€§
- é«˜æ•ˆçš„å†…å­˜ç®¡ç†
- çµæ´»çš„é‡‡æ ·ç­–ç•¥

```java
// åˆ›å»ºç»éªŒç¼“å†²åŒº
ReplayBuffer buffer = new ReplayBuffer(10000);

// å­˜å‚¨ç»éªŒ
buffer.push(experience);

// æ‰¹é‡é‡‡æ ·å­¦ä¹ 
if (buffer.canSample(batchSize)) {
    Experience[] batch = buffer.sample(batchSize);
    agent.learnBatch(batch);
}
```

#### ç»éªŒæ•°æ®ç»“æ„
[`Experience`](src/main/java/io/leavesfly/tinyai/rl/Experience.java) - æ ‡å‡†åŒ–çš„ç»éªŒè¡¨ç¤º

**æ•°æ®å­—æ®µï¼š**
- çŠ¶æ€ (State)
- åŠ¨ä½œ (Action) 
- ä¸‹ä¸€çŠ¶æ€ (Next State)
- å¥–åŠ± (Reward)
- æ˜¯å¦ç»“æŸ (Done)

## æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’æ¨¡å¼

### æ ‡å‡†äº¤äº’æµç¨‹

```mermaid
sequenceDiagram
    participant Env as ç¯å¢ƒ
    participant Agent as æ™ºèƒ½ä½“
    participant Buffer as ç»éªŒç¼“å†²åŒº
    participant Network as ç¥ç»ç½‘ç»œ
    
    loop è®­ç»ƒå¾ªç¯
        Env->>Agent : å½“å‰çŠ¶æ€
        Agent->>Agent : é€‰æ‹©åŠ¨ä½œ (Îµ-è´ªå¿ƒ)
        Agent->>Env : æ‰§è¡ŒåŠ¨ä½œ
        Env->>Agent : ä¸‹ä¸€çŠ¶æ€, å¥–åŠ±, æ˜¯å¦ç»“æŸ
        Agent->>Buffer : å­˜å‚¨ç»éªŒ
        
        alt ç»éªŒè¶³å¤Ÿ
            Buffer->>Agent : é‡‡æ ·æ‰¹æ¬¡
            Agent->>Network : è®¡ç®—ç›®æ ‡Qå€¼
            Agent->>Network : è®¡ç®—å½“å‰Qå€¼
            Network->>Agent : è®¡ç®—æŸå¤±
            Agent->>Network : åå‘ä¼ æ’­æ›´æ–°
            
            alt è¾¾åˆ°æ›´æ–°é¢‘ç‡
                Network->>Network : æ›´æ–°ç›®æ ‡ç½‘ç»œ
            end
        end
    end
```

## æŠ€æœ¯ä¾èµ–

æœ¬æ¨¡å—ä¾èµ–ä»¥ä¸‹ TinyAI æ ¸å¿ƒæ¨¡å—ï¼š

- `tinyai-dl-ml` - æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¨¡å—ï¼Œæä¾›æ¨¡å‹è®­ç»ƒå’Œä¼˜åŒ–æ”¯æŒ

å¤–éƒ¨ä¾èµ–ï¼š
- `jfreechart` - å›¾è¡¨å¯è§†åŒ–åº“ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹ç›‘æ§
- `junit` - å•å…ƒæµ‹è¯•æ¡†æ¶

## ä½¿ç”¨ç¤ºä¾‹

### DQNç®—æ³•å®Œæ•´ç¤ºä¾‹

```java
import io.leavesfly.tinyai.rl.agent.DQNAgent;
import io.leavesfly.tinyai.rl.environment.CartPoleEnvironment;
import io.leavesfly.tinyai.rl.Experience;

// 1. åˆ›å»ºç¯å¢ƒ
CartPoleEnvironment env = new CartPoleEnvironment();

// 2. åˆ›å»ºDQNæ™ºèƒ½ä½“
DQNAgent agent = new DQNAgent(
    "CartPole-DQN",           // åç§°
    env.getStateDim(),        // çŠ¶æ€ç»´åº¦ (4)
    env.getActionDim(),       // åŠ¨ä½œç»´åº¦ (2)
    new int[]{128, 128},      // éšè—å±‚
    0.001f,                   // å­¦ä¹ ç‡
    1.0f,                     // åˆå§‹æ¢ç´¢ç‡
    0.99f,                    // æŠ˜æ‰£å› å­
    32,                       // æ‰¹æ¬¡å¤§å°
    10000,                    // ç¼“å†²åŒºå¤§å°
    100                       // ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
);

// 3. è®­ç»ƒå¾ªç¯
int episodes = 1000;
for (int episode = 0; episode < episodes; episode++) {
    Variable state = env.reset();
    float episodeReward = 0f;
    
    while (!env.isDone()) {
        // é€‰æ‹©åŠ¨ä½œ
        Variable action = agent.selectAction(state);
        
        // æ‰§è¡ŒåŠ¨ä½œ
        Environment.StepResult result = env.step(action);
        
        // åˆ›å»ºç»éªŒ
        Experience experience = new Experience(
            state, action, result.getNextState(),
            result.getReward(), result.isDone()
        );
        
        // å­¦ä¹ 
        agent.learn(experience);
        
        // æ›´æ–°çŠ¶æ€
        state = result.getNextState();
        episodeReward += result.getReward();
    }
    
    // æ¢ç´¢ç‡è¡°å‡
    agent.decayEpsilon(0.995f);
    
    // æ‰“å°è®­ç»ƒä¿¡æ¯
    if (episode % 100 == 0) {
        System.out.printf("Episode %d: Reward = %.2f, Epsilon = %.3f%n",
                         episode, episodeReward, agent.getEpsilon());
    }
}
```

### å¤šè‡‚è€è™æœºç®—æ³•ç¤ºä¾‹

```java
import io.leavesfly.tinyai.rl.agent.UCBBanditAgent;
import io.leavesfly.tinyai.rl.environment.MultiArmedBanditEnvironment;

// 1. åˆ›å»ºå¤šè‡‚è€è™æœºç¯å¢ƒ
MultiArmedBanditEnvironment env = new MultiArmedBanditEnvironment(
    new float[]{0.1f, 0.4f, 0.8f, 0.3f}  // æ¯ä¸ªè‡‚çš„å¥–åŠ±æœŸæœ›
);

// 2. åˆ›å»ºUCBæ™ºèƒ½ä½“
UCBBanditAgent agent = new UCBBanditAgent(
    "UCB-Bandit",            // åç§°
    env.getActionDim(),      // è‡‚çš„æ•°é‡
    2.0f                     // UCBå‚æ•°
);

// 3. å­¦ä¹ å¾ªç¯
int steps = 10000;
for (int step = 0; step < steps; step++) {
    Variable state = env.getCurrentState();
    Variable action = agent.selectAction(state);
    
    Environment.StepResult result = env.step(action);
    
    Experience experience = new Experience(
        state, action, result.getNextState(),
        result.getReward(), false
    );
    
    agent.learn(experience);
}

// 4. è¾“å‡ºç»“æœ
System.out.println("æœ€ç»ˆç­–ç•¥åˆ†å¸ƒï¼š");
for (int i = 0; i < env.getActionDim(); i++) {
    System.out.printf("è‡‚ %d: å¹³å‡å¥–åŠ± = %.3f%n", i, agent.getAverageReward(i));
}
```

### è‡ªå®šä¹‰ç¯å¢ƒç¤ºä¾‹

```java
public class CustomEnvironment extends Environment {
    
    public CustomEnvironment() {
        super(customStateDim, customActionDim, maxSteps);
    }
    
    @Override
    public Variable reset() {
        // é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
        currentState = generateInitialState();
        done = false;
        currentStep = 0;
        return currentState;
    }
    
    @Override
    public StepResult step(Variable action) {
        // çŠ¶æ€è½¬ç§»é€»è¾‘
        Variable nextState = computeNextState(currentState, action);
        float reward = computeReward(currentState, action, nextState);
        boolean done = isTerminal(nextState) || currentStep >= maxSteps;
        
        currentState = nextState;
        currentStep++;
        this.done = done;
        
        return new StepResult(nextState, reward, done, getInfo());
    }
    
    @Override
    public Variable sampleAction() {
        // éšæœºåŠ¨ä½œé‡‡æ ·
        return new Variable(NdArray.of(random.nextInt(actionDim)));
    }
    
    @Override
    public boolean isValidAction(Variable action) {
        // åŠ¨ä½œæœ‰æ•ˆæ€§æ£€æŸ¥
        int actionValue = (int) action.getValue().getNumber().floatValue();
        return actionValue >= 0 && actionValue < actionDim;
    }
}
```

## ç®—æ³•å¯¹æ¯”åˆ†æ

### å¤šè‡‚è€è™æœºç®—æ³•å¯¹æ¯”

| ç®—æ³• | æ¢ç´¢ç­–ç•¥ | ç†è®ºä¿è¯ | è®¡ç®—å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|------------|----------|
| **Îµ-è´ªå¿ƒ** | å›ºå®šæ¦‚ç‡æ¢ç´¢ | ç®€å•é—æ†¾ç•Œ | O(1) | åœ¨çº¿å­¦ä¹ ã€å¿«é€Ÿå†³ç­– |
| **UCB** | ç½®ä¿¡åŒºé—´æ¢ç´¢ | æœ€ä¼˜é—æ†¾ç•Œ | O(1) | ç†è®ºæœ€ä¼˜ã€ç¨³å®šç¯å¢ƒ |
| **æ±¤æ™®æ£®é‡‡æ ·** | è´å¶æ–¯é‡‡æ · | æœ€ä¼˜é—æ†¾ç•Œ | O(k) | è´å¶æ–¯ä¼˜åŒ–ã€ä¸ç¡®å®šç¯å¢ƒ |

### æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹æ¯”

| ç®—æ³• | ç±»å‹ | çŠ¶æ€ç©ºé—´ | åŠ¨ä½œç©ºé—´ | æ ·æœ¬æ•ˆç‡ | ç¨³å®šæ€§ |
|------|------|----------|----------|----------|---------|
| **DQN** | å€¼å‡½æ•° | è¿ç»­ | ç¦»æ•£ | ä¸­ç­‰ | è¾ƒå¥½ |
| **REINFORCE** | ç­–ç•¥æ¢¯åº¦ | è¿ç»­ | è¿ç»­/ç¦»æ•£ | è¾ƒä½ | ä¸€èˆ¬ |

## æµ‹è¯•è¦†ç›–

æ¨¡å—åŒ…å«å®Œæ•´çš„å•å…ƒæµ‹è¯•ï¼Œè¦†ç›–ï¼š
- å„ç§æ™ºèƒ½ä½“ç®—æ³•çš„æ­£ç¡®æ€§æµ‹è¯•
- ç¯å¢ƒäº¤äº’é€»è¾‘éªŒè¯
- ç»éªŒç¼“å†²åŒºåŠŸèƒ½æµ‹è¯•
- ç­–ç•¥æœºåˆ¶æœ‰æ•ˆæ€§éªŒè¯
- ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•

è¿è¡Œæµ‹è¯•ï¼š
```bash
cd /Users/yefei.yf/Qoder/TinyAI
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home
mvn test -pl tinyai-dl-rl
```

## æ¨¡å—ç‰¹è‰²

### ğŸ—ï¸ æ ‡å‡†åŒ–è®¾è®¡
- éµå¾ª OpenAI Gym æ¥å£è§„èŒƒ
- ç»Ÿä¸€çš„æ™ºèƒ½ä½“-ç¯å¢ƒäº¤äº’æ¨¡å¼
- å¯æ‰©å±•çš„ç®—æ³•å®ç°æ¡†æ¶

### ğŸ§  ç®—æ³•ä¸°å¯Œæ€§
- ä»ç»å…¸è€è™æœºåˆ°ç°ä»£æ·±åº¦å¼ºåŒ–å­¦ä¹ 
- å¤šç§æ¢ç´¢ç­–ç•¥å’Œå­¦ä¹ ç®—æ³•
- ç†è®ºä¸å®è·µç›¸ç»“åˆçš„å®ç°

### âš¡ é«˜æ€§èƒ½å®ç°
- é«˜æ•ˆçš„ç»éªŒå›æ”¾æœºåˆ¶
- ä¼˜åŒ–çš„å†…å­˜ç®¡ç†
- æ”¯æŒæ‰¹é‡å­¦ä¹ å’Œå¹¶è¡Œè®­ç»ƒ

### ğŸ”§ æ˜“ç”¨æ€§è®¾è®¡
- ç®€æ´çš„APIæ¥å£
- ä¸°å¯Œçš„é¢„ç½®ç¯å¢ƒå’Œç®—æ³•
- è¯¦ç»†çš„æ–‡æ¡£å’Œç¤ºä¾‹

## å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çš„æ™ºèƒ½ä½“ç®—æ³•

```java
public class CustomAgent extends Agent {
    
    public CustomAgent(String name, int stateDim, int actionDim, 
                      float learningRate, float epsilon, float gamma) {
        super(name, stateDim, actionDim, learningRate, epsilon, gamma);
        // åˆå§‹åŒ–è‡ªå®šä¹‰å‚æ•°
    }
    
    @Override
    public Variable selectAction(Variable state) {
        // å®ç°åŠ¨ä½œé€‰æ‹©é€»è¾‘
        return customActionSelection(state);
    }
    
    @Override
    public void learn(Experience experience) {
        // å®ç°å­¦ä¹ æ›´æ–°é€»è¾‘
        customLearningUpdate(experience);
    }
    
    @Override
    public void learnBatch(Experience[] experiences) {
        // å®ç°æ‰¹é‡å­¦ä¹ é€»è¾‘
        customBatchLearning(experiences);
    }
    
    @Override
    public void storeExperience(Experience experience) {
        // å®ç°ç»éªŒå­˜å‚¨é€»è¾‘
        customExperienceStorage(experience);
    }
}
```

### æ·»åŠ æ–°çš„ç¯å¢ƒ

```java
public class NewEnvironment extends Environment {
    
    public NewEnvironment() {
        super(stateDim, actionDim, maxSteps);
        // ç¯å¢ƒç‰¹å®šçš„åˆå§‹åŒ–
    }
    
    @Override
    public Variable reset() {
        // å®ç°ç¯å¢ƒé‡ç½®é€»è¾‘
        return initialState;
    }
    
    @Override
    public StepResult step(Variable action) {
        // å®ç°çŠ¶æ€è½¬ç§»é€»è¾‘
        return new StepResult(nextState, reward, done, info);
    }
    
    @Override
    public Variable sampleAction() {
        // å®ç°éšæœºåŠ¨ä½œé‡‡æ ·
        return randomAction;
    }
    
    @Override
    public boolean isValidAction(Variable action) {
        // å®ç°åŠ¨ä½œæœ‰æ•ˆæ€§æ£€æŸ¥
        return isValid;
    }
}
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒç¨³å®šæ€§ä¼˜åŒ–
- **ç»éªŒå›æ”¾**ï¼šä½¿ç”¨è¶³å¤Ÿå¤§çš„ç¼“å†²åŒºæ‰“ç ´æ•°æ®ç›¸å…³æ€§
- **ç›®æ ‡ç½‘ç»œ**ï¼šå®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œç¨³å®šè®­ç»ƒ
- **æ¢ç´¢ç­–ç•¥**ï¼šåˆç†è®¾ç½®æ¢ç´¢ç‡å’Œè¡°å‡ç­–ç•¥
- **å¥–åŠ±è®¾è®¡**ï¼šè®¾è®¡åˆç†çš„å¥–åŠ±å‡½æ•°å¼•å¯¼å­¦ä¹ 

### è¶…å‚æ•°è°ƒä¼˜
- **å­¦ä¹ ç‡**ï¼šä» 0.001 å¼€å§‹ï¼Œæ ¹æ®æ”¶æ•›æƒ…å†µè°ƒæ•´
- **æ‰¹æ¬¡å¤§å°**ï¼š32-128ï¼Œå¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ¢¯åº¦ç¨³å®šæ€§
- **ç¼“å†²åŒºå¤§å°**ï¼š10000-100000ï¼Œæ ¹æ®å†…å­˜é™åˆ¶é€‰æ‹©
- **æ¢ç´¢ç‡**ï¼šä» 1.0 è¡°å‡åˆ° 0.01ï¼Œä¿æŒé€‚åº¦æ¢ç´¢

## ç‰ˆæœ¬ä¿¡æ¯

- **å½“å‰ç‰ˆæœ¬**: 1.0-SNAPSHOT
- **Java ç‰ˆæœ¬**: 17+
- **æ„å»ºå·¥å…·**: Maven 3.6+
- **ç®—æ³•æ”¯æŒ**: DQNã€REINFORCEã€å¤šè‡‚è€è™æœºç³»åˆ—

## ç›¸å…³æ¨¡å—

- [`tinyai-dl-ml`](../tinyai-dl-ml/README.md) - æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¨¡å—
- [`tinyai-dl-case`](../tinyai-dl-case/README.md) - åº”ç”¨ç¤ºä¾‹æ¨¡å—
- [`tinyai-dl-nnet`](../tinyai-dl-nnet/README.md) - ç¥ç»ç½‘ç»œå±‚æ¨¡å—

---

**TinyAI Reinforcement Learning æ¨¡å—** - è®©æ™ºèƒ½å†³ç­–å˜å¾—ç®€å•ã€é«˜æ•ˆã€å¯é  ğŸ¯