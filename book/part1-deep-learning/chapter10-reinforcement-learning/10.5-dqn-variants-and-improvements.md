# 10.5 DQN变体和改进

> **本节学习目标**：了解DQN的改进版本和先进技术，掌握Double DQN、Dueling DQN等变体的原理和实现

## 内容概览

虽然DQN在许多强化学习任务中表现出色，但它仍然存在一些问题，如Q值过高估计等。研究者们提出了多种DQN的改进版本来解决这些问题。本节将深入学习Double DQN、Dueling DQN、Prioritized Experience Replay等先进技术。

## 10.5.1 Double DQN

Double DQN通过解耦动作选择和价值评估来解决Q值过高估计问题。

### 问题分析

在标准DQN中，目标Q值计算为：
$$y = r + \gamma \max_{a'} Q(s',a';\theta^-)$$

这种计算方式容易导致Q值的过高估计，因为同一个网络既用于选择最优动作，又用于评估该动作的价值。

### 解决方案

Double DQN将动作选择和价值评估分离：
$$y = r + \gamma Q(s', \arg\max_{a'} Q(s',a';\theta);\theta^-)$$

其中θ是主网络参数，θ^-是目标网络参数。

```java
/**
 * Double DQN实现
 */
public class DoubleDQN extends DQN {
    /**
     * 构造函数
     */
    public DoubleDQN(int stateSize, int actionSize, int hiddenSize,
                     double learningRate, double discountFactor,
                     int bufferSize, int batchSize, int targetUpdateFrequency,
                     double initialEpsilon, double epsilonDecay, double minEpsilon) {
        super(stateSize, actionSize, hiddenSize, learningRate, discountFactor,
              bufferSize, batchSize, targetUpdateFrequency,
              initialEpsilon, epsilonDecay, minEpsilon);
    }
    
    /**
     * 重写训练方法，使用Double DQN的目标计算
     */
    @Override
    public void train() {
        // 检查是否可以采样
        if (!getReplayBuffer().canSample(getBatchSize())) {
            return;
        }
        
        // 采样一批经验
        List<Experience> batch = getReplayBuffer().sample(getBatchSize());
        
        // 计算Double DQN目标Q值
        double[][] targetQValues = computeDoubleDQNTargets(batch);
        
        // 更新网络参数（简化实现）
        getOptimizer().updateParameters(getMainNetwork(), batch, getTargetManager(), getDiscountFactor());
        
        // 更新目标网络
        getTargetManager().updateTargetNetwork();
        
        // 衰减ε
        getPolicy().decayEpsilon();
    }
    
    /**
     * 计算Double DQN目标Q值
     */
    private double[][] computeDoubleDQNTargets(List<Experience> batch) {
        double[][] targetQValues = new double[batch.size()][getActionSize()];
        double discountFactor = getDiscountFactor();
        
        for (int i = 0; i < batch.size(); i++) {
            Experience exp = batch.get(i);
            
            // 获取当前Q值
            double[] currentQValues = getMainNetwork().getQValues(exp.getState());
            System.arraycopy(currentQValues, 0, targetQValues[i], 0, getActionSize());
            
            // 计算Double DQN目标
            double target = exp.getReward();
            if (!exp.isDone()) {
                // 使用主网络选择动作
                int bestAction = getMainNetwork().getOptimalAction(exp.getNextState());
                // 使用目标网络评估动作价值
                double nextQValue = getTargetManager().getTargetNetwork().getQValue(exp.getNextState(), bestAction);
                target += discountFactor * nextQValue;
            }
            
            // 更新目标Q值
            targetQValues[i][exp.getAction()] = target;
        }
        
        return targetQValues;
    }
    
    /**
     * 获取动作数量
     */
    private int getActionSize() {
        // 这里需要访问父类的actionSize字段，简化实现
        return 2; // 假设为CartPole环境
    }
    
    /**
     * 获取折扣因子
     */
    private double getDiscountFactor() {
        // 这里需要访问父类的discountFactor字段，简化实现
        return 0.99;
    }
    
    /**
     * 获取批次大小
     */
    private int getBatchSize() {
        // 这里需要访问父类的batchSize字段，简化实现
        return 32;
    }
    
    /**
     * 获取优化器
     */
    private SimpleOptimizer getOptimizer() {
        // 这里需要访问父类的optimizer字段，简化实现
        return new SimpleOptimizer(0.001);
    }
    
    /**
     * 获取主网络
     */
    private SimpleQNetwork getMainNetwork() {
        // 这里需要访问父类的mainNetwork字段，简化实现
        return new SimpleQNetwork(4, 64, 2);
    }
}

/**
 * Double DQN训练器
 */
public class DoubleDQNTrainer extends DQNTrainer {
    private DoubleDQN doubleDQN;
    
    /**
     * 构造函数
     */
    public DoubleDQNTrainer(DoubleDQN doubleDQN, Environment environment, TrainingConfig config) {
        super(doubleDQN, environment, config);
        this.doubleDQN = doubleDQN;
    }
    
    /**
     * 比较DQN和Double DQN性能
     */
    public void compareWithDQN(DQN standardDQN) {
        System.out.println("=== DQN vs Double DQN 性能比较 ===");
        
        // 评估标准DQN
        System.out.println("标准DQN评估:");
        EvaluationResult dqnResult = evaluate(100);
        
        // 评估Double DQN
        System.out.println("Double DQN评估:");
        EvaluationResult doubleDQNResult = evaluate(100);
        
        // 比较结果
        System.out.println("性能比较:");
        System.out.printf("平均奖励提升: %.2f%n", doubleDQNResult.getAverageReward() - dqnResult.getAverageReward());
        System.out.printf("成功率提升: %.2f%%%n", (doubleDQNResult.getSuccessRate() - dqnResult.getSuccessRate()) * 100);
        System.out.printf("平均步数变化: %.1f%n", doubleDQNResult.getAverageSteps() - dqnResult.getAverageSteps());
    }
}
```

## 10.5.2 Dueling DQN

Dueling DQN通过将Q值分解为状态价值和优势函数来提高学习效率。

### 网络架构

Dueling DQN将网络分为两个流：
- **状态价值流**：输出V(s)
- **优势函数流**：输出A(s,a)

最终Q值计算为：
$$Q(s,a) = V(s) + A(s,a) - \frac{1}{|A|}\sum_{a'} A(s,a')$$

```java
/**
 * Dueling DQN网络架构
 */
public class DuelingQNetwork {
    private int inputSize;      // 输入维度
    private int hiddenSize;     // 共享隐藏层大小
    private int valueHiddenSize; // 价值流隐藏层大小
    private int advantageHiddenSize; // 优势流隐藏层大小
    private int outputSize;     // 输出维度（动作数量）
    
    // 共享层权重
    private double[][] sharedW1;
    private double[] sharedB1;
    
    // 价值流权重
    private double[][] valueW1;
    private double[] valueB1;
    private double[][] valueW2;
    private double[] valueB2;
    
    // 优势流权重
    private double[][] advantageW1;
    private double[] advantageB1;
    private double[][] advantageW2;
    private double[] advantageB2;
    
    // 激活函数
    private ActivationFunction activation;
    private Random random;
    
    /**
     * 构造函数
     */
    public DuelingQNetwork(int inputSize, int hiddenSize, 
                          int valueHiddenSize, int advantageHiddenSize, int outputSize) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.valueHiddenSize = valueHiddenSize;
        this.advantageHiddenSize = advantageHiddenSize;
        this.outputSize = outputSize;
        
        // 初始化权重
        initializeWeights();
        
        // 初始化激活函数
        this.activation = new ReLUActivation();
        this.random = new Random(42);
    }
    
    /**
     * 初始化权重
     */
    private void initializeWeights() {
        // 共享层
        this.sharedW1 = initializeMatrix(inputSize, hiddenSize);
        this.sharedB1 = new double[hiddenSize];
        
        // 价值流
        this.valueW1 = initializeMatrix(hiddenSize, valueHiddenSize);
        this.valueB1 = new double[valueHiddenSize];
        this.valueW2 = initializeMatrix(valueHiddenSize, 1);
        this.valueB2 = new double[1];
        
        // 优势流
        this.advantageW1 = initializeMatrix(hiddenSize, advantageHiddenSize);
        this.advantageB1 = new double[advantageHiddenSize];
        this.advantageW2 = initializeMatrix(advantageHiddenSize, outputSize);
        this.advantageB2 = new double[outputSize];
    }
    
    /**
     * 初始化权重矩阵
     */
    private double[][] initializeMatrix(int rows, int cols) {
        double[][] matrix = new double[rows][cols];
        double scale = Math.sqrt(2.0 / rows); // He初始化
        
        for (int i = 0; i < rows; i++) {
            for (int j = 0; j < cols; j++) {
                matrix[i][j] = random.nextGaussian() * scale;
            }
        }
        
        return matrix;
    }
    
    /**
     * 前向传播
     */
    public double[] forward(double[] state) {
        // 共享层
        double[] sharedHidden = new double[hiddenSize];
        for (int j = 0; j < hiddenSize; j++) {
            for (int i = 0; i < inputSize; i++) {
                sharedHidden[j] += state[i] * sharedW1[i][j];
            }
            sharedHidden[j] += sharedB1[j];
        }
        sharedHidden = activation.forward(sharedHidden);
        
        // 价值流
        double[] valueHidden = new double[valueHiddenSize];
        for (int j = 0; j < valueHiddenSize; j++) {
            for (int i = 0; i < hiddenSize; i++) {
                valueHidden[j] += sharedHidden[i] * valueW1[i][j];
            }
            valueHidden[j] += valueB1[j];
        }
        valueHidden = activation.forward(valueHidden);
        
        double value = valueB2[0];
        for (int i = 0; i < valueHiddenSize; i++) {
            value += valueHidden[i] * valueW2[i][0];
        }
        
        // 优势流
        double[] advantageHidden = new double[advantageHiddenSize];
        for (int j = 0; j < advantageHiddenSize; j++) {
            for (int i = 0; i < hiddenSize; i++) {
                advantageHidden[j] += sharedHidden[i] * advantageW1[i][j];
            }
            advantageHidden[j] += advantageB1[j];
        }
        advantageHidden = activation.forward(advantageHidden);
        
        double[] advantages = new double[outputSize];
        for (int j = 0; j < outputSize; j++) {
            advantages[j] = advantageB2[j];
            for (int i = 0; i < advantageHiddenSize; i++) {
                advantages[j] += advantageHidden[i] * advantageW2[i][j];
            }
        }
        
        // 组合价值和优势
        double[] qValues = new double[outputSize];
        double meanAdvantage = Arrays.stream(advantages).average().orElse(0);
        
        for (int i = 0; i < outputSize; i++) {
            qValues[i] = value + (advantages[i] - meanAdvantage);
        }
        
        return qValues;
    }
    
    /**
     * 获取指定动作的Q值
     */
    public double getQValue(double[] state, int action) {
        double[] qValues = forward(state);
        return qValues[action];
    }
    
    /**
     * 获取所有动作的Q值
     */
    public double[] getQValues(double[] state) {
        return forward(state);
    }
    
    /**
     * 获取最优动作
     */
    public int getOptimalAction(double[] state) {
        double[] qValues = forward(state);
        return argmax(qValues);
    }
    
    /**
     * 获取最大Q值
     */
    public double getMaxQValue(double[] state) {
        double[] qValues = forward(state);
        return Arrays.stream(qValues).max().orElse(0.0);
    }
    
    /**
     * 获取最大值索引
     */
    private int argmax(double[] array) {
        int maxIndex = 0;
        for (int i = 1; i < array.length; i++) {
            if (array[i] > array[maxIndex]) {
                maxIndex = i;
            }
        }
        return maxIndex;
    }
    
    /**
     * 获取网络参数
     */
    public DuelingNetworkParameters getParameters() {
        return new DuelingNetworkParameters(
            cloneMatrix(sharedW1), cloneArray(sharedB1),
            cloneMatrix(valueW1), cloneArray(valueB1),
            cloneMatrix(valueW2), cloneArray(valueB2),
            cloneMatrix(advantageW1), cloneArray(advantageB1),
            cloneMatrix(advantageW2), cloneArray(advantageB2)
        );
    }
    
    /**
     * 设置网络参数
     */
    public void setParameters(DuelingNetworkParameters params) {
        copyMatrix(params.getSharedW1(), sharedW1);
        copyArray(params.getSharedB1(), sharedB1);
        copyMatrix(params.getValueW1(), valueW1);
        copyArray(params.getValueB1(), valueB1);
        copyMatrix(params.getValueW2(), valueW2);
        copyArray(params.getValueB2(), valueB2);
        copyMatrix(params.getAdvantageW1(), advantageW1);
        copyArray(params.getAdvantageB1(), advantageB1);
        copyMatrix(params.getAdvantageW2(), advantageW2);
        copyArray(params.getAdvantageB2(), advantageB2);
    }
    
    /**
     * 克隆矩阵
     */
    private double[][] cloneMatrix(double[][] matrix) {
        double[][] clone = new double[matrix.length][];
        for (int i = 0; i < matrix.length; i++) {
            clone[i] = matrix[i].clone();
        }
        return clone;
    }
    
    /**
     * 克隆数组
     */
    private double[] cloneArray(double[] array) {
        return array.clone();
    }
    
    /**
     * 复制矩阵
     */
    private void copyMatrix(double[][] source, double[][] target) {
        for (int i = 0; i < source.length && i < target.length; i++) {
            System.arraycopy(source[i], 0, target[i], 0, 
                           Math.min(source[i].length, target[i].length));
        }
    }
    
    /**
     * 复制数组
     */
    private void copyArray(double[] source, double[] target) {
        System.arraycopy(source, 0, target, 0, Math.min(source.length, target.length));
    }
    
    // Getter方法
    public int getInputSize() { return inputSize; }
    public int getHiddenSize() { return hiddenSize; }
    public int getOutputSize() { return outputSize; }
}

/**
 * Dueling网络参数封装类
 */
class DuelingNetworkParameters {
    private double[][] sharedW1;
    private double[] sharedB1;
    private double[][] valueW1;
    private double[] valueB1;
    private double[][] valueW2;
    private double[] valueB2;
    private double[][] advantageW1;
    private double[] advantageB1;
    private double[][] advantageW2;
    private double[] advantageB2;
    
    public DuelingNetworkParameters(double[][] sharedW1, double[] sharedB1,
                                  double[][] valueW1, double[] valueB1,
                                  double[][] valueW2, double[] valueB2,
                                  double[][] advantageW1, double[] advantageB1,
                                  double[][] advantageW2, double[] advantageB2) {
        this.sharedW1 = sharedW1;
        this.sharedB1 = sharedB1;
        this.valueW1 = valueW1;
        this.valueB1 = valueB1;
        this.valueW2 = valueW2;
        this.valueB2 = valueB2;
        this.advantageW1 = advantageW1;
        this.advantageB1 = advantageB1;
        this.advantageW2 = advantageW2;
        this.advantageB2 = advantageB2;
    }
    
    // Getter方法
    public double[][] getSharedW1() { return sharedW1; }
    public double[] getSharedB1() { return sharedB1; }
    public double[][] getValueW1() { return valueW1; }
    public double[] getValueB1() { return valueB1; }
    public double[][] getValueW2() { return valueW2; }
    public double[] getValueB2() { return valueB2; }
    public double[][] getAdvantageW1() { return advantageW1; }
    public double[] getAdvantageB1() { return advantageB1; }
    public double[][] getAdvantageW2() { return advantageW2; }
    public double[] getAdvantageB2() { return advantageB2; }
}
```

## 10.5.3 Prioritized Experience Replay

Prioritized Experience Replay通过优先回放重要的经验来提高样本效率。

### 核心思想

不是均匀地采样经验，而是根据经验的重要性（TD误差）来优先采样。

### 实现

```java
/**
 * 优先经验回放缓冲区
 */
public class PrioritizedReplayBuffer {
    private int capacity;
    private List<Experience> buffer;
    private List<Double> priorities; // 每个经验的优先级
    private double alpha; // 优先级指数
    private double beta;  // 重要性采样指数
    private double epsilon; // 小常数，避免优先级为0
    private Random random;
    private int position;
    
    /**
     * 构造函数
     */
    public PrioritizedReplayBuffer(int capacity, double alpha, double beta, double epsilon) {
        this.capacity = capacity;
        this.buffer = new ArrayList<>(capacity);
        this.priorities = new ArrayList<>(capacity);
        this.alpha = alpha;
        this.beta = beta;
        this.epsilon = epsilon;
        this.random = new Random(42);
        this.position = 0;
    }
    
    /**
     * 添加经验
     */
    public void add(double[] state, int action, double reward, double[] nextState, boolean done) {
        Experience experience = new Experience(state, action, reward, nextState, done);
        double maxPriority = priorities.isEmpty() ? 1.0 : 
                           priorities.stream().mapToDouble(Double::doubleValue).max().orElse(1.0);
        
        if (buffer.size() < capacity) {
            buffer.add(experience);
            priorities.add(maxPriority);
        } else {
            buffer.set(position, experience);
            priorities.set(position, maxPriority);
        }
        
        position = (position + 1) % capacity;
    }
    
    /**
     * 更新经验优先级
     */
    public void updatePriorities(int[] indices, double[] tdErrors) {
        for (int i = 0; i < indices.length && i < tdErrors.length; i++) {
            if (indices[i] >= 0 && indices[i] < priorities.size()) {
                priorities.set(indices[i], tdErrors[i] + epsilon);
            }
        }
    }
    
    /**
     * 优先采样一批经验
     */
    public PrioritizedBatch sample(int batchSize) {
        if (buffer.size() < batchSize) {
            // 如果缓冲区不够，返回所有经验
            int[] indices = IntStream.range(0, buffer.size()).toArray();
            List<Experience> batch = new ArrayList<>(buffer);
            double[] weights = new double[buffer.size()];
            Arrays.fill(weights, 1.0);
            return new PrioritizedBatch(batch, indices, weights);
        }
        
        // 计算优先级
        double[] prioritiesArray = priorities.stream().mapToDouble(Double::doubleValue).toArray();
        double[] probs = new double[prioritiesArray.length];
        double sum = 0;
        
        for (int i = 0; i < prioritiesArray.length; i++) {
            probs[i] = Math.pow(prioritiesArray[i], alpha);
            sum += probs[i];
        }
        
        // 归一化概率
        for (int i = 0; i < probs.length; i++) {
            probs[i] /= sum;
        }
        
        // 采样
        int[] indices = new int[batchSize];
        List<Experience> batch = new ArrayList<>(batchSize);
        double[] weights = new double[batchSize];
        
        for (int i = 0; i < batchSize; i++) {
            // 轮盘赌选择
            double rand = random.nextDouble();
            double cumulative = 0;
            for (int j = 0; j < probs.length; j++) {
                cumulative += probs[j];
                if (rand <= cumulative) {
                    indices[i] = j;
                    batch.add(buffer.get(j));
                    // 计算重要性采样权重
                    weights[i] = Math.pow(probs[j] * buffer.size(), -beta);
                    break;
                }
            }
        }
        
        // 归一化权重
        double maxWeight = Arrays.stream(weights).max().orElse(1.0);
        for (int i = 0; i < weights.length; i++) {
            weights[i] /= maxWeight;
        }
        
        return new PrioritizedBatch(batch, indices, weights);
    }
    
    /**
     * 检查是否可以采样
     */
    public boolean canSample(int batchSize) {
        return buffer.size() >= batchSize;
    }
    
    /**
     * 获取缓冲区大小
     */
    public int size() {
        return buffer.size();
    }
    
    /**
     * 获取缓冲区容量
     */
    public int getCapacity() {
        return capacity;
    }
    
    /**
     * 清空缓冲区
     */
    public void clear() {
        buffer.clear();
        priorities.clear();
        position = 0;
    }
    
    /**
     * 设置beta值（通常在训练过程中逐渐增加到1）
     */
    public void setBeta(double beta) {
        this.beta = beta;
    }
}

/**
 * 优先采样批次
 */
class PrioritizedBatch {
    private List<Experience> experiences;
    private int[] indices;
    private double[] weights;
    
    public PrioritizedBatch(List<Experience> experiences, int[] indices, double[] weights) {
        this.experiences = experiences;
        this.indices = indices;
        this.weights = weights;
    }
    
    // Getter方法
    public List<Experience> getExperiences() { return experiences; }
    public int[] getIndices() { return indices; }
    public double[] getWeights() { return weights; }
}
```

## 10.5.4 Multi-step Learning

Multi-step Learning通过考虑多步回报来改善策略评估。

### 核心思想

标准的单步TD学习只考虑一步回报，而Multi-step Learning考虑n步回报：

$$G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n})$$

```java
/**
 * 多步学习经验
 */
public class MultiStepExperience {
    private List<Experience> steps; // n步经验序列
    private int n; // 步数
    
    /**
     * 构造函数
     */
    public MultiStepExperience(int n) {
        this.n = n;
        this.steps = new ArrayList<>(n);
    }
    
    /**
     * 添加一步经验
     */
    public void addStep(Experience experience) {
        if (steps.size() >= n) {
            steps.remove(0); // 移除最旧的经验
        }
        steps.add(experience);
    }
    
    /**
     * 计算n步回报
     */
    public double computeNStepReturn(double gamma, SimpleQNetwork targetNetwork) {
        if (steps.isEmpty()) {
            return 0.0;
        }
        
        double returnSum = 0.0;
        double discount = 1.0;
        
        // 计算前n-1步的回报
        for (int i = 0; i < steps.size() - 1; i++) {
            returnSum += discount * steps.get(i).getReward();
            discount *= gamma;
        }
        
        // 添加最后一步的目标Q值
        Experience lastStep = steps.get(steps.size() - 1);
        if (!lastStep.isDone()) {
            double targetQ = targetNetwork.getQValue(lastStep.getNextState(), 
                                                   targetNetwork.getOptimalAction(lastStep.getNextState()));
            returnSum += discount * targetQ;
        } else {
            returnSum += discount * lastStep.getReward();
        }
        
        return returnSum;
    }
    
    /**
     * 获取起始状态
     */
    public double[] getStartState() {
        return steps.isEmpty() ? new double[0] : steps.get(0).getState();
    }
    
    /**
     * 获取起始动作
     */
    public int getStartAction() {
        return steps.isEmpty() ? 0 : steps.get(0).getAction();
    }
    
    /**
     * 检查是否完整
     */
    public boolean isComplete() {
        return steps.size() == n;
    }
    
    /**
     * 获取经验序列
     */
    public List<Experience> getSteps() {
        return new ArrayList<>(steps);
    }
}

/**
 * Multi-step DQN
 */
public class MultiStepDQN extends DQN {
    private int nStep; // n步
    private Deque<Experience> recentExperiences; // 最近的经验
    
    /**
     * 构造函数
     */
    public MultiStepDQN(int stateSize, int actionSize, int hiddenSize,
                       double learningRate, double discountFactor,
                       int bufferSize, int batchSize, int targetUpdateFrequency,
                       double initialEpsilon, double epsilonDecay, double minEpsilon,
                       int nStep) {
        super(stateSize, actionSize, hiddenSize, learningRate, discountFactor,
              bufferSize, batchSize, targetUpdateFrequency,
              initialEpsilon, epsilonDecay, minEpsilon);
        
        this.nStep = nStep;
        this.recentExperiences = new ArrayDeque<>(nStep);
    }
    
    /**
     * 重写存储经验方法
     */
    @Override
    public void storeExperience(double[] state, int action, double reward, 
                              double[] nextState, boolean done) {
        Experience experience = new Experience(state, action, reward, nextState, done);
        recentExperiences.addLast(experience);
        
        // 如果队列满了，移除最旧的经验
        if (recentExperiences.size() > nStep) {
            recentExperiences.removeFirst();
        }
        
        // 如果收集了足够的经验，计算n步回报
        if (recentExperiences.size() == nStep) {
            storeNStepExperience();
        }
    }
    
    /**
     * 存储n步经验
     */
    private void storeNStepExperience() {
        // 创建n步经验
        MultiStepExperience multiStepExp = new MultiStepExperience(nStep);
        for (Experience exp : recentExperiences) {
            multiStepExp.addStep(exp);
        }
        
        // 计算n步回报
        double nStepReturn = multiStepExp.computeNStepReturn(getDiscountFactor(), 
                                                           getTargetManager().getTargetNetwork());
        
        // 存储起始状态和动作的经验
        Experience startExp = recentExperiences.getFirst();
        super.storeExperience(startExp.getState(), startExp.getAction(), nStepReturn,
                            recentExperiences.getLast().getNextState(), 
                            recentExperiences.getLast().isDone());
    }
}
```

## 10.5.5 Noisy Networks

Noisy Networks通过在网络权重中添加噪声来替代ε-贪心探索。

### 核心思想

在权重中添加参数化噪声，使网络能够自适应地探索环境。

```java
/**
 * 噪声线性层
 */
public class NoisyLinear {
    private int inputSize;
    private int outputSize;
    private double[][] weightMu;    // 权重均值
    private double[][] weightSigma; // 权重标准差
    private double[] biasMu;        // 偏置均值
    private double[] biasSigma;     // 偏置标准差
    private double[][] weight;      // 实际权重（包含噪声）
    private double[] bias;          // 实际偏置（包含噪声）
    private Random random;
    
    /**
     * 构造函数
     */
    public NoisyLinear(int inputSize, int outputSize) {
        this.inputSize = inputSize;
        this.outputSize = outputSize;
        this.random = new Random(42);
        
        // 初始化参数
        initializeParameters();
        
        // 生成初始噪声
        sampleNoise();
    }
    
    /**
     * 初始化参数
     */
    private void initializeParameters() {
        // 权重参数
        weightMu = new double[inputSize][outputSize];
        weightSigma = new double[inputSize][outputSize];
        
        double stddev = 0.1 / Math.sqrt(inputSize);
        for (int i = 0; i < inputSize; i++) {
            for (int j = 0; j < outputSize; j++) {
                weightMu[i][j] = random.nextGaussian() * stddev;
                weightSigma[i][j] = stddev;
            }
        }
        
        // 偏置参数
        biasMu = new double[outputSize];
        biasSigma = new double[outputSize];
        
        for (int i = 0; i < outputSize; i++) {
            biasMu[i] = random.nextGaussian() * stddev;
            biasSigma[i] = stddev;
        }
        
        // 实际权重和偏置
        weight = new double[inputSize][outputSize];
        bias = new double[outputSize];
    }
    
    /**
     * 采样噪声
     */
    public void sampleNoise() {
        // 生成噪声向量
        double[] epsilonIn = new double[inputSize];
        double[] epsilonOut = new double[outputSize];
        
        for (int i = 0; i < inputSize; i++) {
            epsilonIn[i] = random.nextGaussian();
        }
        for (int i = 0; i < outputSize; i++) {
            epsilonOut[i] = random.nextGaussian();
        }
        
        // 计算权重噪声
        for (int i = 0; i < inputSize; i++) {
            for (int j = 0; j < outputSize; j++) {
                weight[i][j] = weightMu[i][j] + weightSigma[i][j] * epsilonIn[i] * epsilonOut[j];
            }
        }
        
        // 计算偏置噪声
        for (int i = 0; i < outputSize; i++) {
            bias[i] = biasMu[i] + biasSigma[i] * epsilonOut[i];
        }
    }
    
    /**
     * 前向传播
     */
    public double[] forward(double[] input) {
        double[] output = new double[outputSize];
        
        // 线性变换
        for (int j = 0; j < outputSize; j++) {
            output[j] = bias[j];
            for (int i = 0; i < inputSize; i++) {
                output[j] += input[i] * weight[i][j];
            }
        }
        
        return output;
    }
    
    /**
     * 重置噪声
     */
    public void resetNoise() {
        sampleNoise();
    }
    
    // Getter和Setter方法
    public double[][] getWeightMu() { return weightMu; }
    public double[][] getWeightSigma() { return weightSigma; }
    public double[] getBiasMu() { return biasMu; }
    public double[] getBiasSigma() { return biasSigma; }
}

/**
 * Noisy DQN网络
 */
public class NoisyDQN extends SimpleQNetwork {
    private NoisyLinear noisyLayer1;
    private NoisyLinear noisyLayer2;
    
    /**
     * 构造函数
     */
    public NoisyDQN(int inputSize, int hiddenSize, int outputSize) {
        super(inputSize, hiddenSize, outputSize);
        this.noisyLayer1 = new NoisyLinear(hiddenSize, hiddenSize);
        this.noisyLayer2 = new NoisyLinear(hiddenSize, outputSize);
    }
    
    /**
     * 重写前向传播，使用噪声层
     */
    @Override
    public double[] forward(double[] state) {
        // 输入到隐藏层（使用原始网络）
        double[] hidden = super.forward(state); // 简化实现
        
        // 通过噪声层
        hidden = noisyLayer1.forward(hidden);
        hidden = new ReLUActivation().forward(hidden);
        
        // 输出层（使用噪声层）
        double[] output = noisyLayer2.forward(hidden);
        
        return output;
    }
    
    /**
     * 采样新的噪声
     */
    public void sampleNoise() {
        noisyLayer1.sampleNoise();
        noisyLayer2.sampleNoise();
    }
    
    /**
     * 重置噪声
     */
    public void resetNoise() {
        noisyLayer1.resetNoise();
        noisyLayer2.resetNoise();
    }
}
```

## 10.5.6 Rainbow DQN

Rainbow DQN结合了多种DQN改进技术，形成一个强大的强化学习算法。

### 核心组件

Rainbow DQN整合了以下技术：
1. Double DQN
2. Dueling DQN
3. Prioritized Experience Replay
4. Multi-step Learning
5. Noisy Networks
6. Distributional RL

```java
/**
 * Rainbow DQN实现
 */
public class RainbowDQN {
    private DuelingQNetwork mainNetwork;
    private DuelingQNetwork targetNetwork;
    private PrioritizedReplayBuffer replayBuffer;
    private NoisyDQN noisyNetwork;
    private int nStep;
    private double gamma;
    private double alpha;
    private double beta;
    private int targetUpdateFrequency;
    private int updateCounter;
    
    /**
     * 构造函数
     */
    public RainbowDQN(int stateSize, int actionSize, int hiddenSize,
                     int bufferSize, int batchSize, double gamma,
                     int nStep, double alpha, double beta,
                     int targetUpdateFrequency) {
        
        this.mainNetwork = new DuelingQNetwork(stateSize, hiddenSize, 64, 64, actionSize);
        this.targetNetwork = new DuelingQNetwork(stateSize, hiddenSize, 64, 64, actionSize);
        this.replayBuffer = new PrioritizedReplayBuffer(bufferSize, alpha, beta, 1e-6);
        this.noisyNetwork = new NoisyDQN(stateSize, hiddenSize, actionSize);
        this.nStep = nStep;
        this.gamma = gamma;
        this.alpha = alpha;
        this.beta = beta;
        this.targetUpdateFrequency = targetUpdateFrequency;
        this.updateCounter = 0;
        
        // 同步目标网络
        syncTargetNetwork();
    }
    
    /**
     * 同步目标网络
     */
    private void syncTargetNetwork() {
        targetNetwork.setParameters(mainNetwork.getParameters());
    }
    
    /**
     * 选择动作（使用噪声网络进行探索）
     */
    public int selectAction(double[] state) {
        // 使用噪声网络进行探索
        double[] qValues = noisyNetwork.forward(state);
        return argmax(qValues);
    }
    
    /**
     * 训练网络
     */
    public void train() {
        if (!replayBuffer.canSample(32)) { // 假设批次大小为32
            return;
        }
        
        // 采样优先经验
        PrioritizedBatch batch = replayBuffer.sample(32);
        
        // 计算目标Q值（使用Double DQN和Dueling DQN）
        double[][] targetQValues = computeRainbowTargets(batch);
        
        // 更新优先级
        double[] tdErrors = computeTDErrors(batch, targetQValues);
        replayBuffer.updatePriorities(batch.getIndices(), tdErrors);
        
        // 更新网络参数
        updateNetworkParameters(batch, targetQValues);
        
        // 更新目标网络
        updateCounter++;
        if (updateCounter >= targetUpdateFrequency) {
            syncTargetNetwork();
            updateCounter = 0;
        }
        
        // 重置噪声
        noisyNetwork.resetNoise();
    }
    
    /**
     * 计算Rainbow目标Q值
     */
    private double[][] computeRainbowTargets(PrioritizedBatch batch) {
        List<Experience> experiences = batch.getExperiences();
        double[][] targetQValues = new double[experiences.size()][getActionSize()];
        
        for (int i = 0; i < experiences.size(); i++) {
            Experience exp = experiences.get(i);
            
            // 获取当前Q值
            double[] currentQValues = mainNetwork.getQValues(exp.getState());
            System.arraycopy(currentQValues, 0, targetQValues[i], 0, getActionSize());
            
            // 计算目标（Double DQN + Dueling DQN）
            double target = exp.getReward();
            if (!exp.isDone()) {
                // 使用主网络选择动作
                int bestAction = mainNetwork.getOptimalAction(exp.getNextState());
                // 使用目标网络评估动作价值
                double nextQValue = targetNetwork.getQValue(exp.getNextState(), bestAction);
                target += Math.pow(gamma, nStep) * nextQValue;
            }
            
            // 更新目标Q值
            targetQValues[i][exp.getAction()] = target;
        }
        
        return targetQValues;
    }
    
    /**
     * 计算TD误差
     */
    private double[] computeTDErrors(PrioritizedBatch batch, double[][] targetQValues) {
        List<Experience> experiences = batch.getExperiences();
        double[] tdErrors = new double[experiences.size()];
        
        for (int i = 0; i < experiences.size(); i++) {
            Experience exp = experiences.get(i);
            double currentQ = mainNetwork.getQValue(exp.getState(), exp.getAction());
            double targetQ = targetQValues[i][exp.getAction()];
            tdErrors[i] = Math.abs(targetQ - currentQ);
        }
        
        return tdErrors;
    }
    
    /**
     * 更新网络参数
     */
    private void updateNetworkParameters(PrioritizedBatch batch, double[][] targetQValues) {
        // 简化实现，实际应该计算损失并使用梯度下降
        System.out.printf("使用优先批次更新网络参数，批次大小: %d%n", batch.getExperiences().size());
    }
    
    /**
     * 获取最大值索引
     */
    private int argmax(double[] array) {
        int maxIndex = 0;
        for (int i = 1; i < array.length; i++) {
            if (array[i] > array[maxIndex]) {
                maxIndex = i;
            }
        }
        return maxIndex;
    }
    
    /**
     * 获取动作数量
     */
    private int getActionSize() {
        return 2; // 假设为CartPole环境
    }
    
    // Getter方法
    public DuelingQNetwork getMainNetwork() { return mainNetwork; }
    public PrioritizedReplayBuffer getReplayBuffer() { return replayBuffer; }
    public NoisyDQN getNoisyNetwork() { return noisyNetwork; }
}

/**
 * Rainbow DQN训练器
 */
public class RainbowDQNTrainer {
    private RainbowDQN rainbowDQN;
    private Environment environment;
    private TrainingConfig config;
    
    /**
     * 构造函数
     */
    public RainbowDQNTrainer(RainbowDQN rainbowDQN, Environment environment, TrainingConfig config) {
        this.rainbowDQN = rainbowDQN;
        this.environment = environment;
        this.config = config;
    }
    
    /**
     * 训练Rainbow DQN
     */
    public void train() {
        System.out.println("=== Rainbow DQN训练开始 ===");
        
        for (int episode = 0; episode < config.getNumEpisodes(); episode++) {
            environment.reset();
            double[] state = ((VectorEnvironment) environment).getCurrentStateVector();
            double totalReward = 0;
            int steps = 0;
            
            while (!environment.isTerminal() && steps < config.getMaxStepsPerEpisode()) {
                // 选择动作
                int action = rainbowDQN.selectAction(state);
                
                // 执行动作
                environment.step(action);
                double[] nextState = ((VectorEnvironment) environment).getCurrentStateVector();
                double reward = environment.getReward();
                boolean isTerminal = environment.isTerminal();
                
                // 存储经验
                // 这里需要实现经验存储逻辑
                
                // 训练网络
                rainbowDQN.train();
                
                totalReward += reward;
                steps++;
                state = nextState;
            }
            
            // 打印进度
            if (episode % config.getLoggingFrequency() == 0) {
                System.out.printf("回合 %d: 奖励 = %.2f, 步数 = %d%n", episode, totalReward, steps);
            }
        }
        
        System.out.println("=== Rainbow DQN训练完成 ===");
    }
}
```

## 10.5.7 完整示例：比较不同DQN变体

让我们通过一个完整的示例来比较不同DQN变体的性能：

```java
/**
 * DQN变体性能比较示例
 */
public class DQNVariantComparison {
    public static void main(String[] args) {
        System.out.println("=== DQN变体性能比较 ===");
        
        // 创建环境
        CartPoleEnvironment environment = new CartPoleEnvironment();
        int stateSize = environment.getStateDimension();
        int actionSize = 2;
        
        // 标准DQN
        System.out.println("1. 训练标准DQN...");
        DQN standardDQN = createStandardDQN(stateSize, actionSize);
        DQNTrainer standardTrainer = new DQNTrainer(standardDQN, environment, 
                                                   new TrainingConfig(500, 200, 32, 50));
        DQNTrainingResult standardResult = standardTrainer.train();
        
        // Double DQN
        System.out.println("\n2. 训练Double DQN...");
        DoubleDQN doubleDQN = createDoubleDQN(stateSize, actionSize);
        DoubleDQNTrainer doubleTrainer = new DoubleDQNTrainer(doubleDQN, environment,
                                                            new TrainingConfig(500, 200, 32, 50));
        // 比较性能
        doubleTrainer.compareWithDQN(standardDQN);
        
        // Dueling DQN
        System.out.println("\n3. 训练Dueling DQN...");
        DuelingDQN duelingDQN = createDuelingDQN(stateSize, actionSize);
        trainDuelingDQN(duelingDQN, environment, 500);
        
        // 评估所有变体
        System.out.println("\n=== 最终性能评估 ===");
        evaluateAllVariants(standardDQN, doubleDQN, duelingDQN, environment);
    }
    
    /**
     * 创建标准DQN
     */
    private static DQN createStandardDQN(int stateSize, int actionSize) {
        return new DQN(stateSize, actionSize, 64, 0.001, 0.99,
                      10000, 32, 100, 1.0, 0.995, 0.01);
    }
    
    /**
     * 创建Double DQN
     */
    private static DoubleDQN createDoubleDQN(int stateSize, int actionSize) {
        return new DoubleDQN(stateSize, actionSize, 64, 0.001, 0.99,
                           10000, 32, 100, 1.0, 0.995, 0.01);
    }
    
    /**
     * 创建Dueling DQN
     */
    private static DuelingDQN createDuelingDQN(int stateSize, int actionSize) {
        return new DuelingDQN(stateSize, actionSize, 64, 0.001, 0.99,
                            10000, 32, 100, 1.0, 0.995, 0.01);
    }
    
    /**
     * 训练Dueling DQN
     */
    private static void trainDuelingDQN(DuelingDQN duelingDQN, 
                                      CartPoleEnvironment environment, 
                                      int numEpisodes) {
        for (int episode = 0; episode < numEpisodes; episode++) {
            environment.reset();
            double[] state = environment.getNormalizedState();
            double totalReward = 0;
            int steps = 0;
            
            while (!environment.isTerminal() && steps < 200) {
                // 选择动作
                int action = duelingDQN.selectAction(state);
                
                // 执行动作
                environment.step(action);
                double[] nextState = environment.getNormalizedState();
                double reward = environment.getReward();
                boolean isTerminal = environment.isTerminal();
                
                // 简化的训练过程
                totalReward += reward;
                steps++;
                state = nextState;
            }
            
            if (episode % 100 == 0) {
                System.out.printf("Dueling DQN 回合 %d: 奖励 = %.2f%n", episode, totalReward);
            }
        }
    }
    
    /**
     * 评估所有变体
     */
    private static void evaluateAllVariants(DQN standardDQN, DoubleDQN doubleDQN, 
                                          DuelingDQN duelingDQN, 
                                          CartPoleEnvironment environment) {
        int numEpisodes = 100;
        
        // 评估标准DQN
        double standardAvgReward = evaluateDQN(standardDQN, environment, numEpisodes);
        System.out.printf("标准DQN 平均奖励: %.2f%n", standardAvgReward);
        
        // 评估Double DQN
        double doubleAvgReward = evaluateDQN(doubleDQN, environment, numEpisodes);
        System.out.printf("Double DQN 平均奖励: %.2f%n", doubleAvgReward);
        
        // 评估Dueling DQN
        double duelingAvgReward = evaluateDuelingDQN(duelingDQN, environment, numEpisodes);
        System.out.printf("Dueling DQN 平均奖励: %.2f%n", duelingAvgReward);
        
        // 性能比较
        System.out.println("\n性能提升:");
        System.out.printf("Double DQN 相比标准DQN 提升: %.2f%n", doubleAvgReward - standardAvgReward);
        System.out.printf("Dueling DQN 相比标准DQN 提升: %.2f%n", duelingAvgReward - standardAvgReward);
    }
    
    /**
     * 评估DQN
     */
    private static double evaluateDQN(DQN dqn, CartPoleEnvironment environment, int numEpisodes) {
        double totalReward = 0;
        
        // 临时禁用探索
        double originalEpsilon = dqn.getPolicy().getEpsilon();
        dqn.getPolicy().setEpsilon(0.0);
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            environment.reset();
            double[] state = environment.getNormalizedState();
            double episodeReward = 0;
            int steps = 0;
            
            while (!environment.isTerminal() && steps < 200) {
                int action = dqn.selectAction(state);
                environment.step(action);
                state = environment.getNormalizedState();
                episodeReward += environment.getReward();
                steps++;
            }
            
            totalReward += episodeReward;
        }
        
        // 恢复原始ε值
        dqn.getPolicy().setEpsilon(originalEpsilon);
        
        return totalReward / numEpisodes;
    }
    
    /**
     * 评估Dueling DQN
     */
    private static double evaluateDuelingDQN(DuelingDQN duelingDQN, 
                                           CartPoleEnvironment environment, 
                                           int numEpisodes) {
        double totalReward = 0;
        
        for (int episode = 0; episode < numEpisodes; episode++) {
            environment.reset();
            double[] state = environment.getNormalizedState();
            double episodeReward = 0;
            int steps = 0;
            
            while (!environment.isTerminal() && steps < 200) {
                int action = duelingDQN.selectAction(state);
                environment.step(action);
                state = environment.getNormalizedState();
                episodeReward += environment.getReward();
                steps++;
            }
            
            totalReward += episodeReward;
        }
        
        return totalReward / numEpisodes;
    }
}

/**
 * Dueling DQN实现
 */
class DuelingDQN {
    private DuelingQNetwork network;
    private EpsilonGreedyPolicy policy;
    
    /**
     * 构造函数
     */
    public DuelingDQN(int stateSize, int actionSize, int hiddenSize,
                     double learningRate, double discountFactor,
                     int bufferSize, int batchSize, int targetUpdateFrequency,
                     double initialEpsilon, double epsilonDecay, double minEpsilon) {
        
        this.network = new DuelingQNetwork(stateSize, hiddenSize, 64, 64, actionSize);
        this.policy = new EpsilonGreedyPolicy(null, initialEpsilon, epsilonDecay, minEpsilon);
    }
    
    /**
     * 选择动作
     */
    public int selectAction(double[] state) {
        if (Math.random() < policy.getEpsilon()) {
            return (int) (Math.random() * 2); // 假设动作空间大小为2
        }
        return network.getOptimalAction(state);
    }
    
    /**
     * 获取策略
     */
    public EpsilonGreedyPolicy getPolicy() {
        return policy;
    }
}
```

## 本节小结

在本节中，我们深入学习了DQN的多种改进变体：

1. **Double DQN**：通过解耦动作选择和价值评估解决Q值过高估计问题
2. **Dueling DQN**：将Q值分解为状态价值和优势函数，提高学习效率
3. **Prioritized Experience Replay**：根据经验重要性优先回放，提高样本效率
4. **Multi-step Learning**：考虑多步回报，改善策略评估
5. **Noisy Networks**：在网络权重中添加噪声，替代ε-贪心探索
6. **Rainbow DQN**：整合多种改进技术的综合算法

通过Java代码实现，我们不仅掌握了理论知识，还获得了实际的编程经验。这些改进技术显著提升了DQN的性能，在实际应用中具有重要价值。

## 下一步计划

在下一节中，我们将通过一个综合项目来实践所学知识，构建完整的CartPole游戏智能体系统。