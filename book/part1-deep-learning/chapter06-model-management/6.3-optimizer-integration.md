# 6.3 优化器集成：梯度下降算法族

优化器是深度学习训练过程的核心组件，负责根据梯度信息更新模型参数。本节将深入探讨各种优化算法的设计原理，并提供完整的Java实现方案。

## 6.3.1 优化器抽象设计

优化器的核心职责包括：
- **参数更新**：根据梯度更新模型参数
- **学习率调度**：动态调整学习率
- **动量管理**：维护历史梯度信息
- **正则化**：应用权重衰减等正则化技术

### 优化器基类设计

```java
package com.tinyai.optimizers;

import com.tinyai.core.Variable;
import java.util.HashMap;
import java.util.Map;

/**
 * 优化器抽象基类
 */
public abstract class Optimizer {
    protected Map<String, Variable> parameters;
    protected double learningRate;
    protected int step = 0;
    
    // 优化器状态
    protected Map<String, Object> state = new HashMap<>();

    public Optimizer(double learningRate) {
        this.learningRate = learningRate;
        this.parameters = new HashMap<>();
    }

    /**
     * 设置需要优化的参数
     */
    public void setParameters(Map<String, Variable> parameters) {
        this.parameters = parameters;
        initializeOptimizerState();
    }

    /**
     * 执行优化步骤
     */
    public void step() {
        step++;
        
        for (Map.Entry<String, Variable> entry : parameters.entrySet()) {
            String name = entry.getKey();
            Variable param = entry.getValue();
            
            if (param.requiresGrad() && param.getGrad() != null) {
                updateParameter(name, param);
            }
        }
    }

    /**
     * 清零梯度
     */
    public void zeroGrad() {
        for (Variable param : parameters.values()) {
            if (param.requiresGrad()) {
                param.clearGrad();
            }
        }
    }

    /**
     * 设置学习率
     */
    public void setLearningRate(double learningRate) {
        this.learningRate = learningRate;
    }

    /**
     * 获取学习率
     */
    public double getLearningRate() {
        return learningRate;
    }

    /**
     * 获取当前步数
     */
    public int getStep() {
        return step;
    }

    /**
     * 获取优化器状态
     */
    public Map<String, Object> getState() {
        return new HashMap<>(state);
    }

    /**
     * 抽象方法：初始化优化器状态
     */
    protected abstract void initializeOptimizerState();

    /**
     * 抽象方法：更新参数
     */
    protected abstract void updateParameter(String name, Variable param);

    /**
     * 应用权重衰减
     */
    protected Variable applyWeightDecay(Variable param, double weightDecay) {
        if (weightDecay > 0) {
            Variable decay = param.multiply(weightDecay);
            return param.getGrad().add(decay);
        }
        return param.getGrad();
    }
}
```

## 6.3.2 SGD优化器实现

```java
/**
 * 随机梯度下降优化器
 */
public class SGD extends Optimizer {
    private final double momentum;
    private final double weightDecay;
    private final boolean nesterov;
    
    // 动量缓存
    private Map<String, Variable> momentumBuffer;

    public SGD(double learningRate, double momentum, double weightDecay, boolean nesterov) {
        super(learningRate);
        this.momentum = momentum;
        this.weightDecay = weightDecay;
        this.nesterov = nesterov;
        this.momentumBuffer = new HashMap<>();
    }

    public SGD(double learningRate) {
        this(learningRate, 0.0, 0.0, false);
    }

    @Override
    protected void initializeOptimizerState() {
        momentumBuffer.clear();
        
        for (Map.Entry<String, Variable> entry : parameters.entrySet()) {
            String name = entry.getKey();
            Variable param = entry.getValue();
            
            if (param.requiresGrad() && momentum > 0) {
                // 初始化动量缓存为零
                momentumBuffer.put(name, Variable.zeros(param.getShape()));
            }
        }
    }

    @Override
    protected void updateParameter(String name, Variable param) {
        Variable grad = applyWeightDecay(param, weightDecay);
        
        if (momentum > 0) {
            Variable buffer = momentumBuffer.get(name);
            if (buffer == null) {
                buffer = Variable.zeros(param.getShape());
                momentumBuffer.put(name, buffer);
            }
            
            // 更新动量: v = momentum * v + grad
            buffer.mulInPlace(momentum).addInPlace(grad);
            
            if (nesterov) {
                // Nesterov动量: param = param - lr * (momentum * v + grad)
                Variable update = buffer.multiply(momentum).add(grad).multiply(learningRate);
                param.subInPlace(update);
            } else {
                // 标准动量: param = param - lr * v
                Variable update = buffer.multiply(learningRate);
                param.subInPlace(update);
            }
        } else {
            // 无动量: param = param - lr * grad
            Variable update = grad.multiply(learningRate);
            param.subInPlace(update);
        }
    }

    @Override
    public String toString() {
        return String.format("SGD(lr=%.4f, momentum=%.2f, weight_decay=%.6f, nesterov=%b)", 
                           learningRate, momentum, weightDecay, nesterov);
    }
}
```

## 6.3.3 Adam优化器实现

```java
/**
 * Adam优化器实现
 */
public class Adam extends Optimizer {
    private final double beta1;
    private final double beta2;
    private final double epsilon;
    private final double weightDecay;
    private final boolean amsgrad;
    
    // Adam状态
    private Map<String, Variable> expAvg;      // 一阶矩估计
    private Map<String, Variable> expAvgSq;    // 二阶矩估计
    private Map<String, Variable> maxExpAvgSq; // AMSGrad的最大二阶矩

    public Adam(double learningRate, double beta1, double beta2, double epsilon, 
                double weightDecay, boolean amsgrad) {
        super(learningRate);
        this.beta1 = beta1;
        this.beta2 = beta2;
        this.epsilon = epsilon;
        this.weightDecay = weightDecay;
        this.amsgrad = amsgrad;
        
        this.expAvg = new HashMap<>();
        this.expAvgSq = new HashMap<>();
        this.maxExpAvgSq = new HashMap<>();
    }

    public Adam(double learningRate) {
        this(learningRate, 0.9, 0.999, 1e-8, 0.0, false);
    }

    @Override
    protected void initializeOptimizerState() {
        expAvg.clear();
        expAvgSq.clear();
        maxExpAvgSq.clear();
        
        for (Map.Entry<String, Variable> entry : parameters.entrySet()) {
            String name = entry.getKey();
            Variable param = entry.getValue();
            
            if (param.requiresGrad()) {
                expAvg.put(name, Variable.zeros(param.getShape()));
                expAvgSq.put(name, Variable.zeros(param.getShape()));
                
                if (amsgrad) {
                    maxExpAvgSq.put(name, Variable.zeros(param.getShape()));
                }
            }
        }
    }

    @Override
    protected void updateParameter(String name, Variable param) {
        Variable grad = applyWeightDecay(param, weightDecay);
        
        Variable expAvgParam = expAvg.get(name);
        Variable expAvgSqParam = expAvgSq.get(name);
        
        if (expAvgParam == null || expAvgSqParam == null) {
            expAvgParam = Variable.zeros(param.getShape());
            expAvgSqParam = Variable.zeros(param.getShape());
            expAvg.put(name, expAvgParam);
            expAvgSq.put(name, expAvgSqParam);
        }
        
        // 更新一阶矩估计: m_t = beta1 * m_{t-1} + (1 - beta1) * grad
        expAvgParam.mulInPlace(beta1).addInPlace(grad.multiply(1 - beta1));
        
        // 更新二阶矩估计: v_t = beta2 * v_{t-1} + (1 - beta2) * grad^2
        Variable gradSquared = grad.square();
        expAvgSqParam.mulInPlace(beta2).addInPlace(gradSquared.multiply(1 - beta2));
        
        // 偏差修正
        double biasCorrection1 = 1.0 - Math.pow(beta1, step);
        double biasCorrection2 = 1.0 - Math.pow(beta2, step);
        
        Variable correctedExpAvg = expAvgParam.divide(biasCorrection1);
        Variable correctedExpAvgSq = expAvgSqParam.divide(biasCorrection2);
        
        Variable denominator;
        if (amsgrad) {
            Variable maxExpAvgSqParam = maxExpAvgSq.get(name);
            if (maxExpAvgSqParam == null) {
                maxExpAvgSqParam = Variable.zeros(param.getShape());
                maxExpAvgSq.put(name, maxExpAvgSqParam);
            }
            
            // 保持最大的二阶矩估计
            maxExpAvgSqParam = Variable.maximum(maxExpAvgSqParam, correctedExpAvgSq);
            maxExpAvgSq.put(name, maxExpAvgSqParam);
            denominator = maxExpAvgSqParam.sqrt().add(epsilon);
        } else {
            denominator = correctedExpAvgSq.sqrt().add(epsilon);
        }
        
        // 参数更新: param = param - lr * m_hat / (sqrt(v_hat) + epsilon)
        Variable update = correctedExpAvg.divide(denominator).multiply(learningRate);
        param.subInPlace(update);
    }

    @Override
    public String toString() {
        return String.format("Adam(lr=%.4f, beta1=%.3f, beta2=%.3f, eps=%.2e, weight_decay=%.6f, amsgrad=%b)", 
                           learningRate, beta1, beta2, epsilon, weightDecay, amsgrad);
    }
}
```

## 6.3.4 AdamW优化器实现

```java
/**
 * AdamW优化器实现（解耦权重衰减）
 */
public class AdamW extends Optimizer {
    private final double beta1;
    private final double beta2;
    private final double epsilon;
    private final double weightDecay;
    
    private Map<String, Variable> expAvg;
    private Map<String, Variable> expAvgSq;

    public AdamW(double learningRate, double beta1, double beta2, double epsilon, double weightDecay) {
        super(learningRate);
        this.beta1 = beta1;
        this.beta2 = beta2;
        this.epsilon = epsilon;
        this.weightDecay = weightDecay;
        
        this.expAvg = new HashMap<>();
        this.expAvgSq = new HashMap<>();
    }

    public AdamW(double learningRate) {
        this(learningRate, 0.9, 0.999, 1e-8, 0.01);
    }

    @Override
    protected void initializeOptimizerState() {
        expAvg.clear();
        expAvgSq.clear();
        
        for (Map.Entry<String, Variable> entry : parameters.entrySet()) {
            String name = entry.getKey();
            Variable param = entry.getValue();
            
            if (param.requiresGrad()) {
                expAvg.put(name, Variable.zeros(param.getShape()));
                expAvgSq.put(name, Variable.zeros(param.getShape()));
            }
        }
    }

    @Override
    protected void updateParameter(String name, Variable param) {
        Variable grad = param.getGrad(); // AdamW不在梯度上应用权重衰减
        
        Variable expAvgParam = expAvg.get(name);
        Variable expAvgSqParam = expAvgSq.get(name);
        
        // 更新一阶和二阶矩估计
        expAvgParam.mulInPlace(beta1).addInPlace(grad.multiply(1 - beta1));
        expAvgSqParam.mulInPlace(beta2).addInPlace(grad.square().multiply(1 - beta2));
        
        // 偏差修正
        double biasCorrection1 = 1.0 - Math.pow(beta1, step);
        double biasCorrection2 = 1.0 - Math.pow(beta2, step);
        
        Variable correctedExpAvg = expAvgParam.divide(biasCorrection1);
        Variable correctedExpAvgSq = expAvgSqParam.divide(biasCorrection2);
        
        // Adam更新
        Variable denominator = correctedExpAvgSq.sqrt().add(epsilon);
        Variable adamUpdate = correctedExpAvg.divide(denominator).multiply(learningRate);
        
        // 权重衰减（解耦）
        Variable weightDecayUpdate = param.multiply(weightDecay * learningRate);
        
        // 参数更新: param = param - lr * m_hat / (sqrt(v_hat) + eps) - lr * weight_decay * param
        param.subInPlace(adamUpdate).subInPlace(weightDecayUpdate);
    }
}
```

## 6.3.5 学习率调度器

```java
/**
 * 学习率调度器接口
 */
public interface LearningRateScheduler {
    double getLearningRate(int step);
    void step();
}

/**
 * 指数衰减学习率调度器
 */
public class ExponentialLR implements LearningRateScheduler {
    private final double initialLR;
    private final double gamma;
    private int step = 0;

    public ExponentialLR(double initialLR, double gamma) {
        this.initialLR = initialLR;
        this.gamma = gamma;
    }

    @Override
    public double getLearningRate(int step) {
        return initialLR * Math.pow(gamma, step);
    }

    @Override
    public void step() {
        step++;
    }
}

/**
 * 余弦退火学习率调度器
 */
public class CosineAnnealingLR implements LearningRateScheduler {
    private final double initialLR;
    private final double minLR;
    private final int totalSteps;
    private int step = 0;

    public CosineAnnealingLR(double initialLR, double minLR, int totalSteps) {
        this.initialLR = initialLR;
        this.minLR = minLR;
        this.totalSteps = totalSteps;
    }

    @Override
    public double getLearningRate(int step) {
        if (step >= totalSteps) {
            return minLR;
        }
        
        double cosine = Math.cos(Math.PI * step / totalSteps);
        return minLR + (initialLR - minLR) * (1 + cosine) / 2;
    }

    @Override
    public void step() {
        step++;
    }
}

/**
 * 带学习率调度的优化器包装器
 */
public class ScheduledOptimizer extends Optimizer {
    private final Optimizer baseOptimizer;
    private final LearningRateScheduler scheduler;

    public ScheduledOptimizer(Optimizer baseOptimizer, LearningRateScheduler scheduler) {
        super(baseOptimizer.getLearningRate());
        this.baseOptimizer = baseOptimizer;
        this.scheduler = scheduler;
    }

    @Override
    public void step() {
        scheduler.step();
        double newLR = scheduler.getLearningRate(getStep());
        baseOptimizer.setLearningRate(newLR);
        baseOptimizer.step();
        step++;
    }

    @Override
    protected void initializeOptimizerState() {
        // 由基础优化器处理
    }

    @Override
    protected void updateParameter(String name, Variable param) {
        // 由基础优化器处理
    }

    @Override
    public void setParameters(Map<String, Variable> parameters) {
        baseOptimizer.setParameters(parameters);
    }
}
```

## 6.3.6 优化器工厂

```java
/**
 * 优化器工厂
 */
public class OptimizerFactory {
    
    public static Optimizer createSGD(double learningRate, double momentum, double weightDecay) {
        return new SGD(learningRate, momentum, weightDecay, false);
    }
    
    public static Optimizer createAdam(double learningRate, double beta1, double beta2) {
        return new Adam(learningRate, beta1, beta2, 1e-8, 0.0, false);
    }
    
    public static Optimizer createAdamW(double learningRate, double weightDecay) {
        return new AdamW(learningRate, 0.9, 0.999, 1e-8, weightDecay);
    }
    
    public static Optimizer fromConfig(OptimizerConfig config) {
        switch (config.type.toLowerCase()) {
            case "sgd":
                return new SGD(config.learningRate, config.momentum, 
                              config.weightDecay, config.nesterov);
            case "adam":
                return new Adam(config.learningRate, config.beta1, config.beta2, 
                               config.epsilon, config.weightDecay, config.amsgrad);
            case "adamw":
                return new AdamW(config.learningRate, config.beta1, config.beta2, 
                                config.epsilon, config.weightDecay);
            default:
                throw new IllegalArgumentException("Unknown optimizer type: " + config.type);
        }
    }
    
    /**
     * 优化器配置类
     */
    public static class OptimizerConfig {
        public String type = "adam";
        public double learningRate = 0.001;
        public double momentum = 0.9;
        public double beta1 = 0.9;
        public double beta2 = 0.999;
        public double epsilon = 1e-8;
        public double weightDecay = 0.0;
        public boolean nesterov = false;
        public boolean amsgrad = false;
    }
}
```

## 6.3.7 使用示例

```java
/**
 * 优化器使用示例
 */
public class OptimizerExample {
    
    public static void demonstrateOptimizers() {
        // 创建模型
        SequentialModel model = new SequentialModel("OptimizationDemo");
        model.add(new Dense(784, 128))
             .add(new ReLU())
             .add(new Dense(128, 10));
        
        model.initializeParameters(new int[]{1, 784});
        
        // 创建不同的优化器
        Optimizer sgd = new SGD(0.01, 0.9, 1e-4, true);
        Optimizer adam = new Adam(0.001, 0.9, 0.999, 1e-8, 0.0, false);
        Optimizer adamw = new AdamW(0.001, 0.01);
        
        // 设置优化器
        model.setOptimizer(adam);
        
        // 创建带调度器的优化器
        LearningRateScheduler scheduler = new CosineAnnealingLR(0.001, 1e-6, 1000);
        Optimizer scheduledOptimizer = new ScheduledOptimizer(adam, scheduler);
        
        model.setOptimizer(scheduledOptimizer);
        
        System.out.println("Optimizers configured successfully!");
    }
    
    public static void demonstrateTrainingLoop() {
        SequentialModel model = createModel();
        Optimizer optimizer = new Adam(0.001);
        model.setOptimizer(optimizer);
        
        // 模拟训练循环
        for (int epoch = 0; epoch < 10; epoch++) {
            // 模拟前向传播和损失计算
            Variable input = Variable.randn(new int[]{32, 784});
            Variable target = Variable.randint(0, 10, new int[]{32});
            
            // 前向传播
            Variable output = model.forward(input);
            
            // 计算损失（简化）
            Variable loss = computeLoss(output, target);
            
            // 反向传播
            model.zeroGrad();
            loss.backward();
            
            // 优化步骤
            optimizer.step();
            
            System.out.printf("Epoch %d, Loss: %.4f, LR: %.6f%n", 
                            epoch, loss.getData(), optimizer.getLearningRate());
        }
    }
    
    private static SequentialModel createModel() {
        SequentialModel model = new SequentialModel("TrainingModel");
        model.add(new Dense(784, 128))
             .add(new ReLU())
             .add(new Dense(128, 10));
        model.initializeParameters(new int[]{1, 784});
        return model;
    }
    
    private static Variable computeLoss(Variable output, Variable target) {
        // 简化的交叉熵损失实现
        return Variable.scalar(Math.random()); // 占位符
    }
}
```

优化器作为深度学习训练的核心组件，提供了各种梯度下降算法的实现。通过合理的抽象设计和完整的实现，我们构建了一个灵活且高效的优化器体系，支持从基础的SGD到先进的Adam等多种优化算法。

下一节我们将探讨训练循环设计，学习如何将优化器集成到完整的训练流程中。